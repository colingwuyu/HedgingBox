{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "qtable_bot.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMvKaUvJkTtJZxGGU1/HEmD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/colingwuyu/hedgingbox/blob/develop/examples/qtable/qtable_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "cellView": "both",
        "colab_type": "code",
        "id": "KH3O0zcXUeun",
        "colab": {}
      },
      "source": [
        "#@title Install necessary dependencies.\n",
        "!pip install dm-acme\n",
        "!pip install dm-acme[reverb]\n",
        "!pip install dm-acme[tf]\n",
        "!pip install dm-acme[envs]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VQMfM_ntzBrW",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "765c822d-48c1-434e-fb85-45ba4b3f9ef7"
      },
      "source": [
        "#@title Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FrwdDuNVRghT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Update HedgingBox\n",
        "import os\n",
        "project_folder = '/content/Projects'\n",
        "hb_source = os.path.join(project_folder, 'HedgingBox')\n",
        "%mkdir \"$project_folder\"\n",
        "%cd \"$project_folder\"\n",
        "!pwd\n",
        "!mkdir -p 'ACME Models/QTable'\n",
        "!git clone https://github.com/colingwuyu/HedgingBox.git\n",
        "%cd HedgingBox\n",
        "!git fetch --all\n",
        "!git checkout -b develop origin/develop \n",
        "!git pull\n",
        "import sys\n",
        "sys.path.append(hb_source)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-9qlJqw-1gzz",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Import Modules\n",
        "import acme\n",
        "import dm_env\n",
        "from acme import specs\n",
        "from acme import datasets\n",
        "from acme import wrappers\n",
        "from acme.utils.loggers.csv import CSVLogger\n",
        "from acme.adders import reverb as adders\n",
        "\n",
        "import reverb\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "import hb\n",
        "from hb.market_env import hedging_market_env\n",
        "from hb.market_env.pathgenerators import gbm_pathgenerator\n",
        "from hb.market_env.rewardrules.pnl_reward import PnLReward\n",
        "from hb.market_env.rewardrules.pnl_sqrpenalty_reward import PnLSquarePenaltyReward\n",
        "from hb.market_env import market_specs\n",
        "from hb.bots.qtablebot.bot import QTableBot\n",
        "from hb.bots.qtablebot.actor import QTableActor\n",
        "from hb.bots.qtablebot.learning import QTableLearner\n",
        "from hb.bots.qtablebot.qtable import QTable\n",
        "from hb.bots.deltabot.bot import DeltaHedgeBot"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "STPDJf3MsvDu",
        "colab_type": "text"
      },
      "source": [
        "Build Environment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wGpxgmw21fEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Default title text\n",
        "trading_cost_pct =  0.05#@param {type:\"number\"}\n",
        "kappa = 0.08 #@param {type:\"number\"}\n",
        "initial_price =  50#@param {type:\"number\"}\n",
        "drift = 0.0 #@param {type:\"number\"}\n",
        "sigma = 0.15 #@param {type:\"number\"}\n",
        "number_steps =  3#@param {type:\"integer\"}\n",
        "stock_price_lower_bound = 1 #@param {type:\"integer\"}\n",
        "stock_price_upper_bound = 10000 #@param {type:\"integer\"}\n",
        "lot_size = 1 #@param {type:\"integer\"}\n",
        "stock_ticker_size = 1 #@param {type:\"integer\"}\n",
        "option_holding =  -10#@param {type:\"integer\"}\n",
        "initial_stock_holding =  2#@param {type:\"integer\"}\n",
        "buy_sell_lots_bound =  4#@param {type:\"integer\"}\n",
        "gbm = gbm_pathgenerator.GBMGenerator(\n",
        "            initial_price=initial_price, drift=drift,\n",
        "            div=0.0, sigma=sigma, num_step=number_steps, \n",
        "            step_size=30./360., seed=1234\n",
        "        )\n",
        "gbm_pred = gbm_pathgenerator.GBMGenerator(\n",
        "            initial_price=initial_price, drift=drift,\n",
        "            div=0.0, sigma=sigma, num_step=number_steps, \n",
        "            step_size=30./360., seed=4321\n",
        "        )\n",
        "pnl_penalty_reward = PnLSquarePenaltyReward(scale_k=kappa)\n",
        "pnl_reward = PnLReward()\n",
        "market_param = market_specs.MarketEnvParam(\n",
        "    stock_ticker_size=stock_ticker_size,\n",
        "    stock_price_lower_bound=stock_price_lower_bound,\n",
        "    stock_price_upper_bound=stock_price_upper_bound,\n",
        "    lot_size=lot_size,\n",
        "    buy_sell_lots_bound=buy_sell_lots_bound,\n",
        "    holding_lots_bound=np.infty)\n",
        "environment = wrappers.SinglePrecisionWrapper(hedging_market_env.HedgingMarketEnv(\n",
        "            stock_generator=gbm,\n",
        "            reward_rule=pnl_penalty_reward,\n",
        "            market_param=market_param,\n",
        "            trading_cost_pct=trading_cost_pct,\n",
        "            risk_free_rate=0.,\n",
        "            discount_rate=0.,\n",
        "            option_maturity=450./360.,\n",
        "            option_strike=initial_price,\n",
        "            option_holding=option_holding,\n",
        "            initial_stock_holding=initial_stock_holding,\n",
        "        ))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jR_nofE8E8ZZ",
        "colab_type": "code",
        "cellView": "both",
        "colab": {}
      },
      "source": [
        "#@title Create QTable Hedging Bot\n",
        "#@markdown ###Replay Buffer Param\n",
        "batch_size =  1#@param {type:\"integer\"}\n",
        "samples_per_insert =  1#@param {type:\"integer\"}\n",
        "min_replay_size =  1#@param {type:\"integer\"}\n",
        "max_replay_size =  3#@param {type:\"integer\"}\n",
        "#@markdown ###Prediction Param\n",
        "num_prediction_episodes = 1000 #@param {type:\"integer\"}\n",
        "train_episodes_per_pred = 5000 #@param {type:\"integer\"}\n",
        "model_name = \"5pcttradingcost_kappa8_init2holding\" #@param {type:\"string\"}\n",
        "model_path = f\"/content/gdrive/My Drive/Projects/ACME Models/QTable/{model_name}/\"\n",
        "if not os.path.exists(model_path):\n",
        "  os.makedirs(model_path)\n",
        "  \n",
        "qtable_bot_env_attr = ['remaining_time', 'stock_holding', 'stock_price']\n",
        "environment.set_obs_attr(qtable_bot_env_attr)\n",
        "spec = specs.make_environment_spec(environment)\n",
        "pred_logger = CSVLogger(f'qtable_pred/{model_name}',\n",
        "                        label='qtable_pred')\n",
        "qtable_bot = QTableBot(environment_spec=spec,\n",
        "                       epsilon=0.8,\n",
        "                       batch_size=batch_size,\n",
        "                       samples_per_insert=samples_per_insert, \n",
        "                       min_replay_size=min_replay_size,\n",
        "                       max_replay_size=max_replay_size,\n",
        "                       pred_episode = num_prediction_episodes,\n",
        "                       pred_logger = pred_logger,\n",
        "                       observation_per_pred = train_episodes_per_pred)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KwI7PPCA7RV5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "61dc4a53-6e8e-4dc7-d035-1511e218118f"
      },
      "source": [
        " #@title Train QTable\n",
        "qtable_bot._actor._actor._adder.reset()\n",
        "environment.set_obs_attr(qtable_bot_env_attr)\n",
        "num_episodes =  train_episodes_per_pred + num_prediction_episodes\n",
        "epsilon = 0.8 #@param {type:\"number\"}\n",
        "num_check_points =  60#@param {type:\"integer\"}\n",
        "learning_rate = 1e-3 #@param {type:\"number\"}\n",
        "repeat_path =  None#@param {type:\"integer\"}\n",
        "start_new_model = False #@param {type:\"boolean\"}\n",
        "environment.set_repeat_path(repeat_path)\n",
        "model_location = f'{model_path}qtable.pickle'\n",
        "# model_location = f'/content/gdrive/My\\ Drive/Projects/ACME\\ Models/QTable/{model_name}_qtable.pickle'\n",
        "if os.path.exists(model_location) and (not start_new_model):\n",
        "  qtable_bot.restore(model_location)\n",
        "qtable_bot._actor._epsilon = epsilon\n",
        "qtable_bot._learner._learning_rate = learning_rate\n",
        "# Try running the environment loop. We have no assertions here because all\n",
        "# we care about is that the agent runs without raising any errors.\n",
        "if num_episodes > 0:\n",
        "  for i in range(num_check_points):\n",
        "    print(f\"Check Point {i}\")\n",
        "    # train\n",
        "    environment.set_stock_generator(gbm)\n",
        "    loop = acme.EnvironmentLoop(environment, qtable_bot)\n",
        "    loop.run(num_episodes=train_episodes_per_pred)\n",
        "    # prediction\n",
        "    gbm_pred.restart()\n",
        "    environment.set_stock_generator(gbm_pred)\n",
        "    loop.run(num_episodes=num_prediction_episodes)\n",
        "    # save model\n",
        "    qtable_bot.save(model_location)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "[Learner] Action = -4.000 | Avg Td Error = -36.960 | Q = -0.455 | Reward = -37.403 | State = 420|1|52 | Steps = 39711 | Walltime = 114.539\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.276 | Episodes = 3245 | Steps = 9735 | Steps Per Second = 378.581\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.788 | Q = -2.170 | Reward = 4.762 | State = 450|2|50 | Steps = 40081 | Walltime = 115.540\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.473 | Episodes = 3367 | Steps = 10101 | Steps Per Second = 193.010\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.964 | Q = -2.120 | Reward = 3.958 | State = 450|2|50 | Steps = 40433 | Walltime = 116.541\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.451 | Episodes = 3487 | Steps = 10461 | Steps Per Second = 381.925\n",
            "[Learner] Action = 0.000 | Avg Td Error = -47.747 | Q = -0.059 | Reward = -47.806 | State = 390|0|50 | Steps = 40801 | Walltime = 117.543\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.564 | Episodes = 3610 | Steps = 10830 | Steps Per Second = 362.140\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.106 | Q = -0.003 | Reward = 2.039 | State = 420|4|52 | Steps = 41148 | Walltime = 118.546\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.303 | Episodes = 3726 | Steps = 11178 | Steps Per Second = 232.153\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.202 | Q = -2.418 | Reward = 4.944 | State = 450|2|50 | Steps = 41517 | Walltime = 119.548\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -152.118 | Episodes = 3850 | Steps = 11550 | Steps Per Second = 227.988\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.550 | Q = -0.555 | Reward = 4.024 | State = 420|2|52 | Steps = 41878 | Walltime = 120.550\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.862 | Episodes = 3971 | Steps = 11913 | Steps Per Second = 321.066\n",
            "[Learner] Action = 4.000 | Avg Td Error = -20.740 | Q = -0.062 | Reward = -20.803 | State = 390|4|54 | Steps = 42252 | Walltime = 121.551\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.155 | Episodes = 4096 | Steps = 12288 | Steps Per Second = 375.665\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.841 | Q = -0.004 | Reward = -4.849 | State = 420|5|52 | Steps = 42624 | Walltime = 122.552\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.615 | Episodes = 4220 | Steps = 12660 | Steps Per Second = 383.286\n",
            "[Learner] Action = 4.000 | Avg Td Error = 3.270 | Q = -11.874 | Reward = -8.566 | State = 450|2|50 | Steps = 42985 | Walltime = 123.555\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -101.968 | Episodes = 4341 | Steps = 13023 | Steps Per Second = 393.844\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.049 | Q = -0.049 | Reward = 4.000 | State = 390|1|47 | Steps = 43360 | Walltime = 124.556\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.071 | Episodes = 4466 | Steps = 13398 | Steps Per Second = 381.092\n",
            "[Learner] Action = 4.000 | Avg Td Error = 6.716 | Q = -11.841 | Reward = -5.088 | State = 450|2|50 | Steps = 43693 | Walltime = 125.556\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.844 | Episodes = 4577 | Steps = 13731 | Steps Per Second = 378.377\n",
            "[Learner] Action = 1.000 | Avg Td Error = -13.802 | Q = -0.056 | Reward = -13.858 | State = 420|6|49 | Steps = 44058 | Walltime = 126.558\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.758 | Episodes = 4699 | Steps = 14097 | Steps Per Second = 366.934\n",
            "[Learner] Action = 4.000 | Avg Td Error = -2.852 | Q = -0.250 | Reward = -3.101 | State = 390|2|52 | Steps = 44393 | Walltime = 127.560\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.308 | Episodes = 4811 | Steps = 14433 | Steps Per Second = 337.534\n",
            "[Learner] Action = 1.000 | Avg Td Error = -11.787 | Q = -0.671 | Reward = -12.397 | State = 420|2|52 | Steps = 44720 | Walltime = 128.561\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.416 | Episodes = 4921 | Steps = 14763 | Steps Per Second = 376.373\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.440 | Episodes = 5230 | Steps = 15690 | Steps Per Second = 1866.901\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.258 | Episodes = 5855 | Steps = 17565 | Steps Per Second = 1893.306\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.267 | Q = -0.219 | Reward = 1.075 | State = 420|2|49 | Steps = 45000 | Walltime = 130.919\n",
            "Check Point 3\n",
            "[Learner] Action = 3.000 | Avg Td Error = -14.148 | Q = -0.137 | Reward = -14.285 | State = 390|5|46 | Steps = 45370 | Walltime = 131.921\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.085 | Episodes = 125 | Steps = 375 | Steps Per Second = 340.364\n",
            "[Learner] Action = -2.000 | Avg Td Error = 8.694 | Q = -5.909 | Reward = 2.795 | State = 450|2|50 | Steps = 45707 | Walltime = 132.921\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -112.716 | Episodes = 238 | Steps = 714 | Steps Per Second = 338.742\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.023 | Q = -2.601 | Reward = 2.511 | State = 450|2|50 | Steps = 46066 | Walltime = 133.922\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.761 | Episodes = 358 | Steps = 1074 | Steps Per Second = 353.681\n",
            "[Learner] Action = 2.000 | Avg Td Error = 9.419 | Q = -6.141 | Reward = 3.284 | State = 450|2|50 | Steps = 46417 | Walltime = 134.922\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -109.970 | Episodes = 475 | Steps = 1425 | Steps Per Second = 374.938\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.736 | Q = -0.081 | Reward = 1.655 | State = 390|5|51 | Steps = 46753 | Walltime = 135.930\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.055 | Episodes = 586 | Steps = 1758 | Steps Per Second = 306.803\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.690 | Q = 0.003 | Reward = 1.693 | State = 390|6|43 | Steps = 47085 | Walltime = 136.932\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -74.996 | Episodes = 697 | Steps = 2091 | Steps Per Second = 373.735\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.888 | Q = -2.658 | Reward = -6.021 | State = 450|2|50 | Steps = 47424 | Walltime = 137.932\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.427 | Episodes = 810 | Steps = 2430 | Steps Per Second = 368.514\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.094 | Q = -2.599 | Reward = 4.658 | State = 450|2|50 | Steps = 47778 | Walltime = 138.932\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.308 | Episodes = 928 | Steps = 2784 | Steps Per Second = 324.503\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.249 | Q = -3.557 | Reward = 3.733 | State = 450|2|50 | Steps = 48125 | Walltime = 139.934\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.147 | Episodes = 1044 | Steps = 3132 | Steps Per Second = 269.944\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.516 | Q = 0.000 | Reward = 2.559 | State = 420|1|56 | Steps = 48445 | Walltime = 140.937\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.463 | Episodes = 1152 | Steps = 3456 | Steps Per Second = 360.263\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.760 | Q = -0.145 | Reward = 4.614 | State = 420|2|49 | Steps = 48789 | Walltime = 141.937\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.185 | Episodes = 1267 | Steps = 3801 | Steps Per Second = 340.134\n",
            "[Learner] Action = -3.000 | Avg Td Error = -17.317 | Q = -0.228 | Reward = -17.510 | State = 420|5|51 | Steps = 49129 | Walltime = 142.940\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.663 | Episodes = 1381 | Steps = 4143 | Steps Per Second = 365.506\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.656 | Q = -6.140 | Reward = -9.781 | State = 450|2|50 | Steps = 49458 | Walltime = 143.941\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.605 | Episodes = 1488 | Steps = 4464 | Steps Per Second = 172.069\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.508 | Q = -0.027 | Reward = 0.549 | State = 420|4|50 | Steps = 49765 | Walltime = 144.941\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.887 | Episodes = 1593 | Steps = 4779 | Steps Per Second = 355.289\n",
            "[Learner] Action = -3.000 | Avg Td Error = -35.607 | Q = -10.120 | Reward = -45.709 | State = 450|2|50 | Steps = 50134 | Walltime = 145.943\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.662 | Episodes = 1716 | Steps = 5148 | Steps Per Second = 216.566\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.164 | Q = -0.251 | Reward = -5.415 | State = 390|-2|50 | Steps = 50499 | Walltime = 146.946\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.383 | Episodes = 1838 | Steps = 5514 | Steps Per Second = 252.385\n",
            "[Learner] Action = 3.000 | Avg Td Error = 8.353 | Q = -9.500 | Reward = -1.099 | State = 450|2|50 | Steps = 50871 | Walltime = 147.948\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.391 | Episodes = 1963 | Steps = 5889 | Steps Per Second = 366.785\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.526 | Q = -6.096 | Reward = -8.591 | State = 450|2|50 | Steps = 51236 | Walltime = 148.949\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.632 | Episodes = 2084 | Steps = 6252 | Steps Per Second = 325.915\n",
            "[Learner] Action = -4.000 | Avg Td Error = -50.994 | Q = -1.100 | Reward = -52.094 | State = 420|0|51 | Steps = 51607 | Walltime = 149.950\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.291 | Episodes = 2209 | Steps = 6627 | Steps Per Second = 375.497\n",
            "[Learner] Action = 3.000 | Avg Td Error = -11.767 | Q = -0.020 | Reward = -11.788 | State = 390|8|47 | Steps = 51976 | Walltime = 150.952\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.673 | Episodes = 2332 | Steps = 6996 | Steps Per Second = 380.298\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.018 | Q = -0.077 | Reward = -4.095 | State = 390|5|51 | Steps = 52348 | Walltime = 151.952\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.713 | Episodes = 2457 | Steps = 7371 | Steps Per Second = 373.402\n",
            "[Learner] Action = 2.000 | Avg Td Error = 1.078 | Q = -0.021 | Reward = 1.057 | State = 390|8|48 | Steps = 52707 | Walltime = 152.953\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.995 | Episodes = 2576 | Steps = 7728 | Steps Per Second = 333.800\n",
            "[Learner] Action = -4.000 | Avg Td Error = -102.549 | Q = -0.316 | Reward = -102.864 | State = 390|-4|48 | Steps = 53048 | Walltime = 153.953\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.330 | Episodes = 2691 | Steps = 8073 | Steps Per Second = 385.034\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.703 | Q = -0.035 | Reward = -2.738 | State = 390|5|48 | Steps = 53415 | Walltime = 154.954\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.222 | Episodes = 2814 | Steps = 8442 | Steps Per Second = 374.614\n",
            "[Learner] Action = 4.000 | Avg Td Error = -10.853 | Q = -0.213 | Reward = -11.063 | State = 420|4|48 | Steps = 53779 | Walltime = 155.956\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.900 | Episodes = 2936 | Steps = 8808 | Steps Per Second = 370.205\n",
            "[Learner] Action = 1.000 | Avg Td Error = -3.944 | Q = -0.056 | Reward = -3.996 | State = 420|5|49 | Steps = 54136 | Walltime = 156.958\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.410 | Episodes = 3054 | Steps = 9162 | Steps Per Second = 333.552\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.454 | Q = -3.683 | Reward = 3.812 | State = 450|2|50 | Steps = 54480 | Walltime = 157.958\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.550 | Episodes = 3170 | Steps = 9510 | Steps Per Second = 375.396\n",
            "[Learner] Action = 4.000 | Avg Td Error = -47.904 | Q = -0.601 | Reward = -48.330 | State = 420|-2|48 | Steps = 54845 | Walltime = 158.960\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.315 | Episodes = 3292 | Steps = 9876 | Steps Per Second = 372.022\n",
            "[Learner] Action = 3.000 | Avg Td Error = -31.219 | Q = -0.040 | Reward = -31.259 | State = 390|9|49 | Steps = 55216 | Walltime = 159.963\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.879 | Episodes = 3416 | Steps = 10248 | Steps Per Second = 377.593\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.495 | Q = -0.098 | Reward = -6.603 | State = 420|5|47 | Steps = 55589 | Walltime = 160.964\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.873 | Episodes = 3541 | Steps = 10623 | Steps Per Second = 395.192\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.978 | Q = -0.658 | Reward = -9.636 | State = 390|2|50 | Steps = 55962 | Walltime = 161.964\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -127.509 | Episodes = 3665 | Steps = 10995 | Steps Per Second = 384.904\n",
            "[Learner] Action = 0.000 | Avg Td Error = -19.117 | Q = -2.576 | Reward = -21.686 | State = 450|2|50 | Steps = 56324 | Walltime = 162.966\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.428 | Episodes = 3787 | Steps = 11361 | Steps Per Second = 383.649\n",
            "[Learner] Action = -2.000 | Avg Td Error = 6.214 | Q = 0.002 | Reward = 6.216 | State = 390|8|47 | Steps = 56688 | Walltime = 163.967\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.358 | Episodes = 3909 | Steps = 11727 | Steps Per Second = 380.080\n",
            "[Learner] Action = 4.000 | Avg Td Error = 11.612 | Q = -12.813 | Reward = -1.134 | State = 450|2|50 | Steps = 57028 | Walltime = 164.969\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -80.844 | Episodes = 4021 | Steps = 12063 | Steps Per Second = 347.077\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.918 | Q = -0.113 | Reward = -3.982 | State = 420|5|47 | Steps = 57376 | Walltime = 165.972\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.700 | Episodes = 4139 | Steps = 12417 | Steps Per Second = 332.644\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.967 | Q = -0.978 | Reward = 4.998 | State = 420|2|51 | Steps = 57736 | Walltime = 166.974\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.903 | Episodes = 4259 | Steps = 12777 | Steps Per Second = 382.471\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.578 | Q = -0.723 | Reward = 5.890 | State = 420|2|52 | Steps = 58105 | Walltime = 167.976\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -88.409 | Episodes = 4382 | Steps = 13146 | Steps Per Second = 384.141\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.038 | Q = -0.055 | Reward = -8.093 | State = 390|2|47 | Steps = 58467 | Walltime = 168.977\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.074 | Episodes = 4502 | Steps = 13506 | Steps Per Second = 336.441\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.644 | Q = -0.012 | Reward = 2.626 | State = 420|2|47 | Steps = 58803 | Walltime = 169.980\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.831 | Episodes = 4613 | Steps = 13839 | Steps Per Second = 338.232\n",
            "[Learner] Action = -4.000 | Avg Td Error = 4.177 | Q = -13.666 | Reward = -9.123 | State = 450|2|50 | Steps = 59157 | Walltime = 170.981\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.292 | Episodes = 4733 | Steps = 14199 | Steps Per Second = 394.795\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.856 | Q = 0.012 | Reward = 4.868 | State = 390|9|51 | Steps = 59533 | Walltime = 171.981\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.911 | Episodes = 4858 | Steps = 14574 | Steps Per Second = 385.270\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.178 | Q = -2.552 | Reward = -3.377 | State = 450|2|50 | Steps = 59904 | Walltime = 172.983\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -73.330 | Episodes = 4982 | Steps = 14946 | Steps Per Second = 370.195\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.806 | Episodes = 5548 | Steps = 16644 | Steps Per Second = 1947.216\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.511 | Q = -0.011 | Reward = 1.500 | State = 390|4|49 | Steps = 60000 | Walltime = 174.824\n",
            "Check Point 4\n",
            "[Learner] Action = -2.000 | Avg Td Error = -1.652 | Q = -0.154 | Reward = -1.805 | State = 390|4|50 | Steps = 60368 | Walltime = 175.825\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.744 | Episodes = 124 | Steps = 372 | Steps Per Second = 369.379\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.936 | Q = -4.120 | Reward = 3.858 | State = 450|2|50 | Steps = 60721 | Walltime = 176.827\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.985 | Episodes = 242 | Steps = 726 | Steps Per Second = 374.714\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.287 | Q = -0.016 | Reward = -6.119 | State = 420|2|47 | Steps = 61080 | Walltime = 177.829\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.159 | Episodes = 362 | Steps = 1086 | Steps Per Second = 375.464\n",
            "[Learner] Action = 4.000 | Avg Td Error = -13.665 | Q = -0.374 | Reward = -14.045 | State = 420|6|50 | Steps = 61448 | Walltime = 178.830\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.312 | Episodes = 485 | Steps = 1455 | Steps Per Second = 392.260\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.455 | Q = -2.412 | Reward = 1.553 | State = 450|2|50 | Steps = 61820 | Walltime = 179.832\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.755 | Episodes = 609 | Steps = 1827 | Steps Per Second = 354.249\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.253 | Q = -0.168 | Reward = 4.099 | State = 420|2|48 | Steps = 62183 | Walltime = 180.834\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.811 | Episodes = 731 | Steps = 2193 | Steps Per Second = 375.800\n",
            "[Learner] Action = -4.000 | Avg Td Error = -13.378 | Q = 0.000 | Reward = -13.378 | State = 390|2|60 | Steps = 62555 | Walltime = 181.836\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.779 | Episodes = 855 | Steps = 2565 | Steps Per Second = 375.665\n",
            "[Learner] Action = 0.000 | Avg Td Error = -11.883 | Q = -2.344 | Reward = -13.890 | State = 450|2|50 | Steps = 62907 | Walltime = 182.837\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.966 | Episodes = 973 | Steps = 2919 | Steps Per Second = 379.506\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.052 | Q = -0.219 | Reward = -5.264 | State = 420|3|48 | Steps = 63273 | Walltime = 183.839\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.633 | Episodes = 1095 | Steps = 3285 | Steps Per Second = 364.490\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.006 | Q = -0.082 | Reward = -5.089 | State = 390|7|47 | Steps = 63625 | Walltime = 184.842\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.809 | Episodes = 1213 | Steps = 3639 | Steps Per Second = 385.790\n",
            "[Learner] Action = -4.000 | Avg Td Error = -86.516 | Q = -0.652 | Reward = -87.165 | State = 420|1|48 | Steps = 63989 | Walltime = 185.844\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.295 | Episodes = 1334 | Steps = 4002 | Steps Per Second = 344.624\n",
            "[Learner] Action = 4.000 | Avg Td Error = -0.864 | Q = -0.353 | Reward = -1.218 | State = 390|-1|54 | Steps = 64358 | Walltime = 186.846\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.196 | Episodes = 1457 | Steps = 4371 | Steps Per Second = 343.298\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.217 | Q = 0.000 | Reward = -6.217 | State = 390|2|43 | Steps = 64685 | Walltime = 187.847\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.422 | Episodes = 1567 | Steps = 4701 | Steps Per Second = 385.896\n",
            "[Learner] Action = -3.000 | Avg Td Error = 6.646 | Q = -11.019 | Reward = -4.133 | State = 450|2|50 | Steps = 65052 | Walltime = 188.849\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.521 | Episodes = 1690 | Steps = 5070 | Steps Per Second = 375.430\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.136 | Q = -10.426 | Reward = -19.549 | State = 450|2|50 | Steps = 65403 | Walltime = 189.851\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.055 | Episodes = 1807 | Steps = 5421 | Steps Per Second = 328.570\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.641 | Q = -2.194 | Reward = -9.097 | State = 450|2|50 | Steps = 65762 | Walltime = 190.853\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.316 | Episodes = 1927 | Steps = 5781 | Steps Per Second = 330.052\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.050 | Q = -6.444 | Reward = -6.286 | State = 450|2|50 | Steps = 66100 | Walltime = 191.854\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.502 | Episodes = 2041 | Steps = 6123 | Steps Per Second = 378.855\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.298 | Q = -0.040 | Reward = 0.258 | State = 390|3|50 | Steps = 66449 | Walltime = 192.856\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.164 | Episodes = 2157 | Steps = 6471 | Steps Per Second = 391.284\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.680 | Q = -0.739 | Reward = 1.093 | State = 420|2|50 | Steps = 66819 | Walltime = 193.856\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.723 | Episodes = 2281 | Steps = 6843 | Steps Per Second = 384.622\n",
            "[Learner] Action = 4.000 | Avg Td Error = -12.399 | Q = -13.685 | Reward = -26.081 | State = 450|2|50 | Steps = 67180 | Walltime = 194.858\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.943 | Episodes = 2402 | Steps = 7206 | Steps Per Second = 367.760\n",
            "[Learner] Action = -2.000 | Avg Td Error = -12.601 | Q = -0.678 | Reward = -13.279 | State = 390|1|51 | Steps = 67549 | Walltime = 195.858\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -127.143 | Episodes = 2524 | Steps = 7572 | Steps Per Second = 308.269\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.595 | Q = -0.026 | Reward = 0.598 | State = 420|4|48 | Steps = 67887 | Walltime = 196.858\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.419 | Episodes = 2638 | Steps = 7914 | Steps Per Second = 374.815\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.343 | Q = -0.054 | Reward = 2.289 | State = 390|3|48 | Steps = 68249 | Walltime = 197.861\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.350 | Episodes = 2759 | Steps = 8277 | Steps Per Second = 361.184\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.286 | Q = -0.030 | Reward = -0.316 | State = 390|-2|55 | Steps = 68584 | Walltime = 198.863\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.261 | Episodes = 2871 | Steps = 8613 | Steps Per Second = 381.728\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.722 | Q = -0.068 | Reward = -7.790 | State = 390|8|51 | Steps = 68954 | Walltime = 199.864\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.362 | Episodes = 2995 | Steps = 8985 | Steps Per Second = 388.026\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.027 | Q = -0.106 | Reward = -5.086 | State = 420|1|48 | Steps = 69325 | Walltime = 200.865\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -64.285 | Episodes = 3118 | Steps = 9354 | Steps Per Second = 318.813\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.685 | Q = -0.033 | Reward = 3.652 | State = 390|-1|55 | Steps = 69663 | Walltime = 201.866\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.041 | Episodes = 3230 | Steps = 9690 | Steps Per Second = 330.061\n",
            "[Learner] Action = -3.000 | Avg Td Error = -9.600 | Q = -0.116 | Reward = -9.716 | State = 390|5|52 | Steps = 69993 | Walltime = 202.867\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.829 | Episodes = 3341 | Steps = 10023 | Steps Per Second = 357.540\n",
            "[Learner] Action = 0.000 | Avg Td Error = -17.719 | Q = -0.624 | Reward = -18.172 | State = 420|2|50 | Steps = 70355 | Walltime = 203.869\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.609 | Episodes = 3462 | Steps = 10386 | Steps Per Second = 371.671\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.333 | Q = -0.090 | Reward = 4.245 | State = 420|0|48 | Steps = 70705 | Walltime = 204.869\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.863 | Episodes = 3578 | Steps = 10734 | Steps Per Second = 336.073\n",
            "[Learner] Action = -2.000 | Avg Td Error = 3.538 | Q = -6.847 | Reward = -3.045 | State = 450|2|50 | Steps = 71045 | Walltime = 205.872\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -103.081 | Episodes = 3693 | Steps = 11079 | Steps Per Second = 384.834\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.433 | Q = -0.372 | Reward = -6.669 | State = 420|0|51 | Steps = 71414 | Walltime = 206.872\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.329 | Episodes = 3817 | Steps = 11451 | Steps Per Second = 387.668\n",
            "[Learner] Action = 2.000 | Avg Td Error = 11.320 | Q = -7.498 | Reward = 3.823 | State = 450|2|50 | Steps = 71774 | Walltime = 207.874\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.479 | Episodes = 3937 | Steps = 11811 | Steps Per Second = 355.530\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.327 | Q = -0.095 | Reward = 1.361 | State = 420|2|54 | Steps = 72142 | Walltime = 208.874\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.701 | Episodes = 4061 | Steps = 12183 | Steps Per Second = 384.799\n",
            "[Learner] Action = 3.000 | Avg Td Error = 6.521 | Q = -10.646 | Reward = -4.100 | State = 450|2|50 | Steps = 72493 | Walltime = 209.876\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.116 | Episodes = 4178 | Steps = 12534 | Steps Per Second = 381.231\n",
            "[Learner] Action = -1.000 | Avg Td Error = -6.363 | Q = -0.243 | Reward = -6.605 | State = 390|2|50 | Steps = 72862 | Walltime = 210.879\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.962 | Episodes = 4301 | Steps = 12903 | Steps Per Second = 377.956\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.918 | Q = -0.235 | Reward = 2.683 | State = 390|1|53 | Steps = 73229 | Walltime = 211.880\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.606 | Episodes = 4424 | Steps = 13272 | Steps Per Second = 362.808\n",
            "[Learner] Action = -4.000 | Avg Td Error = 0.159 | Q = 0.000 | Reward = 0.159 | State = 390|10|48 | Steps = 73595 | Walltime = 212.880\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.822 | Episodes = 4546 | Steps = 13638 | Steps Per Second = 398.926\n",
            "[Learner] Action = -4.000 | Avg Td Error = -33.136 | Q = -0.148 | Reward = -33.284 | State = 390|6|46 | Steps = 73961 | Walltime = 213.880\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.767 | Episodes = 4667 | Steps = 14001 | Steps Per Second = 324.620\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.713 | Q = -0.205 | Reward = 1.508 | State = 390|1|48 | Steps = 74306 | Walltime = 214.882\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.587 | Episodes = 4782 | Steps = 14346 | Steps Per Second = 338.069\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.737 | Q = -0.065 | Reward = 0.717 | State = 420|3|48 | Steps = 74650 | Walltime = 215.883\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -86.803 | Episodes = 4899 | Steps = 14697 | Steps Per Second = 386.501\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.763 | Episodes = 5125 | Steps = 15375 | Steps Per Second = 1944.508\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.229 | Episodes = 5729 | Steps = 17187 | Steps Per Second = 1947.819\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.468 | Q = -0.034 | Reward = 0.495 | State = 420|5|48 | Steps = 75000 | Walltime = 218.466\n",
            "Check Point 5\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.517 | Q = -2.045 | Reward = 5.515 | State = 450|2|50 | Steps = 75350 | Walltime = 219.468\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.316 | Episodes = 119 | Steps = 357 | Steps Per Second = 324.010\n",
            "[Learner] Action = 3.000 | Avg Td Error = -11.859 | Q = -0.845 | Reward = -12.710 | State = 420|2|49 | Steps = 75694 | Walltime = 220.469\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.827 | Episodes = 234 | Steps = 702 | Steps Per Second = 324.603\n",
            "[Learner] Action = 4.000 | Avg Td Error = -11.802 | Q = -14.546 | Reward = -26.345 | State = 450|2|50 | Steps = 76068 | Walltime = 221.472\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.226 | Episodes = 359 | Steps = 1077 | Steps Per Second = 317.750\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.387 | Q = 0.009 | Reward = 0.396 | State = 390|5|50 | Steps = 76398 | Walltime = 222.473\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.416 | Episodes = 469 | Steps = 1407 | Steps Per Second = 280.393\n",
            "[Learner] Action = -4.000 | Avg Td Error = -16.787 | Q = -0.463 | Reward = -17.251 | State = 390|0|51 | Steps = 76759 | Walltime = 223.483\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.545 | Episodes = 589 | Steps = 1767 | Steps Per Second = 313.062\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.708 | Q = -0.013 | Reward = 2.689 | State = 420|5|52 | Steps = 77119 | Walltime = 224.485\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.088 | Episodes = 709 | Steps = 2127 | Steps Per Second = 291.751\n",
            "[Learner] Action = 2.000 | Avg Td Error = 4.550 | Q = -7.775 | Reward = -3.187 | State = 450|2|50 | Steps = 77485 | Walltime = 225.485\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -58.337 | Episodes = 832 | Steps = 2496 | Steps Per Second = 335.625\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.788 | Q = -14.624 | Reward = -20.390 | State = 450|2|50 | Steps = 77824 | Walltime = 226.485\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.677 | Episodes = 946 | Steps = 2838 | Steps Per Second = 375.598\n",
            "[Learner] Action = 4.000 | Avg Td Error = -16.644 | Q = -0.224 | Reward = -16.869 | State = 390|-4|49 | Steps = 78191 | Walltime = 227.487\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.519 | Episodes = 1069 | Steps = 3207 | Steps Per Second = 371.824\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.896 | Q = -2.334 | Reward = -5.283 | State = 450|2|50 | Steps = 78540 | Walltime = 228.488\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.932 | Episodes = 1185 | Steps = 3555 | Steps Per Second = 369.087\n",
            "[Learner] Action = 0.000 | Avg Td Error = -20.724 | Q = -0.018 | Reward = -20.741 | State = 420|6|53 | Steps = 78894 | Walltime = 229.489\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.812 | Episodes = 1303 | Steps = 3909 | Steps Per Second = 301.063\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.657 | Q = -0.033 | Reward = 2.686 | State = 420|4|51 | Steps = 79222 | Walltime = 230.490\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -126.404 | Episodes = 1413 | Steps = 4239 | Steps Per Second = 388.842\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.802 | Q = -0.360 | Reward = 5.598 | State = 420|0|51 | Steps = 79588 | Walltime = 231.492\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.328 | Episodes = 1535 | Steps = 4605 | Steps Per Second = 334.937\n",
            "[Learner] Action = 3.000 | Avg Td Error = -3.402 | Q = -0.953 | Reward = -4.318 | State = 420|2|49 | Steps = 79942 | Walltime = 232.494\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.770 | Episodes = 1654 | Steps = 4962 | Steps Per Second = 380.332\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.012 | Q = -0.084 | Reward = -0.095 | State = 390|1|51 | Steps = 80310 | Walltime = 233.496\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.361 | Episodes = 1777 | Steps = 5331 | Steps Per Second = 387.131\n",
            "[Learner] Action = -4.000 | Avg Td Error = -3.540 | Q = -15.481 | Reward = -18.547 | State = 450|2|50 | Steps = 80690 | Walltime = 234.497\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.715 | Episodes = 1904 | Steps = 5712 | Steps Per Second = 373.436\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.363 | Q = -2.841 | Reward = 0.130 | State = 450|2|50 | Steps = 81061 | Walltime = 235.499\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.042 | Episodes = 2028 | Steps = 6084 | Steps Per Second = 373.713\n",
            "[Learner] Action = -2.000 | Avg Td Error = -2.432 | Q = 0.010 | Reward = -2.422 | State = 390|1|58 | Steps = 81414 | Walltime = 236.500\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -106.254 | Episodes = 2146 | Steps = 6438 | Steps Per Second = 375.329\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.516 | Q = -0.043 | Reward = 1.391 | State = 420|4|51 | Steps = 81783 | Walltime = 237.500\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.857 | Episodes = 2269 | Steps = 6807 | Steps Per Second = 296.921\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.196 | Q = -0.098 | Reward = 1.099 | State = 390|7|47 | Steps = 82140 | Walltime = 238.502\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.265 | Episodes = 2390 | Steps = 7170 | Steps Per Second = 380.183\n",
            "[Learner] Action = 0.000 | Avg Td Error = -17.340 | Q = -0.558 | Reward = -17.477 | State = 420|-2|49 | Steps = 82503 | Walltime = 239.503\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.645 | Episodes = 2511 | Steps = 7533 | Steps Per Second = 386.572\n",
            "[Learner] Action = -4.000 | Avg Td Error = -19.025 | Q = -0.556 | Reward = -19.580 | State = 390|0|50 | Steps = 82848 | Walltime = 240.506\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.226 | Episodes = 2625 | Steps = 7875 | Steps Per Second = 388.962\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.126 | Q = -0.077 | Reward = -3.203 | State = 390|6|47 | Steps = 83178 | Walltime = 241.508\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.910 | Episodes = 2736 | Steps = 8208 | Steps Per Second = 304.391\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.996 | Q = -0.740 | Reward = -6.771 | State = 420|2|48 | Steps = 83541 | Walltime = 242.510\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.479 | Episodes = 2858 | Steps = 8574 | Steps Per Second = 368.503\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.155 | Q = -4.618 | Reward = 2.644 | State = 450|2|50 | Steps = 83905 | Walltime = 243.512\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.477 | Episodes = 2980 | Steps = 8940 | Steps Per Second = 380.148\n",
            "[Learner] Action = 1.000 | Avg Td Error = -8.877 | Q = -0.407 | Reward = -9.171 | State = 420|2|49 | Steps = 84270 | Walltime = 244.515\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.784 | Episodes = 3102 | Steps = 9306 | Steps Per Second = 376.373\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.987 | Q = -0.749 | Reward = -10.733 | State = 420|4|49 | Steps = 84613 | Walltime = 245.517\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.299 | Episodes = 3217 | Steps = 9651 | Steps Per Second = 387.369\n",
            "[Learner] Action = -1.000 | Avg Td Error = -7.418 | Q = -0.585 | Reward = -7.823 | State = 420|2|53 | Steps = 84946 | Walltime = 246.519\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.863 | Episodes = 3328 | Steps = 9984 | Steps Per Second = 373.325\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.550 | Q = -0.358 | Reward = 4.247 | State = 420|2|49 | Steps = 85292 | Walltime = 247.519\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.799 | Episodes = 3443 | Steps = 10329 | Steps Per Second = 314.423\n",
            "[Learner] Action = -1.000 | Avg Td Error = -7.729 | Q = -0.207 | Reward = -7.799 | State = 420|1|53 | Steps = 85620 | Walltime = 248.522\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.750 | Episodes = 3551 | Steps = 10653 | Steps Per Second = 329.413\n",
            "[Learner] Action = -4.000 | Avg Td Error = -5.378 | Q = -0.287 | Reward = -5.665 | State = 390|-2|48 | Steps = 85888 | Walltime = 249.526\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.278 | Episodes = 3641 | Steps = 10923 | Steps Per Second = 337.244\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.163 | Q = -2.599 | Reward = -2.080 | State = 450|2|50 | Steps = 86256 | Walltime = 250.527\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.784 | Episodes = 3766 | Steps = 11298 | Steps Per Second = 361.132\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.439 | Q = -0.558 | Reward = 0.887 | State = 420|2|48 | Steps = 86625 | Walltime = 251.528\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -118.947 | Episodes = 3889 | Steps = 11667 | Steps Per Second = 382.355\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.693 | Q = -4.802 | Reward = 5.891 | State = 450|2|50 | Steps = 86966 | Walltime = 252.529\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.218 | Episodes = 4003 | Steps = 12009 | Steps Per Second = 385.199\n",
            "[Learner] Action = -3.000 | Avg Td Error = -9.003 | Q = -0.339 | Reward = -9.342 | State = 390|1|54 | Steps = 87306 | Walltime = 253.529\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.814 | Episodes = 4116 | Steps = 12348 | Steps Per Second = 353.086\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.551 | Q = -2.931 | Reward = 5.659 | State = 450|2|50 | Steps = 87680 | Walltime = 254.530\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.164 | Episodes = 4242 | Steps = 12726 | Steps Per Second = 378.388\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.641 | Q = -0.298 | Reward = -7.962 | State = 420|5|49 | Steps = 88052 | Walltime = 255.531\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.313 | Episodes = 4366 | Steps = 13098 | Steps Per Second = 377.231\n",
            "[Learner] Action = -2.000 | Avg Td Error = -9.915 | Q = -0.622 | Reward = -10.386 | State = 420|1|49 | Steps = 88416 | Walltime = 256.532\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.108 | Episodes = 4488 | Steps = 13464 | Steps Per Second = 349.778\n",
            "[Learner] Action = 2.000 | Avg Td Error = 12.629 | Q = -7.956 | Reward = 4.672 | State = 450|2|50 | Steps = 88794 | Walltime = 257.535\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.129 | Episodes = 4614 | Steps = 13842 | Steps Per Second = 305.455\n",
            "[Learner] Action = -4.000 | Avg Td Error = -1.751 | Q = -0.079 | Reward = -1.830 | State = 390|2|55 | Steps = 89121 | Walltime = 258.535\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.175 | Episodes = 4723 | Steps = 14169 | Steps Per Second = 339.482\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.787 | Q = -0.083 | Reward = -0.870 | State = 390|2|46 | Steps = 89491 | Walltime = 259.537\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.611 | Episodes = 4847 | Steps = 14541 | Steps Per Second = 378.502\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.964 | Q = -0.049 | Reward = -1.013 | State = 390|2|47 | Steps = 89859 | Walltime = 260.538\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.735 | Episodes = 4970 | Steps = 14910 | Steps Per Second = 358.927\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.044 | Episodes = 5472 | Steps = 16416 | Steps Per Second = 1957.516\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.605 | Q = -0.274 | Reward = -4.720 | State = 420|1|52 | Steps = 90000 | Walltime = 262.521\n",
            "Check Point 6\n",
            "[Learner] Action = -4.000 | Avg Td Error = -44.511 | Q = -0.167 | Reward = -44.684 | State = 420|2|55 | Steps = 90366 | Walltime = 263.524\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.997 | Episodes = 123 | Steps = 369 | Steps Per Second = 339.656\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.464 | Q = -1.940 | Reward = -14.068 | State = 420|2|49 | Steps = 90735 | Walltime = 264.526\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.377 | Episodes = 246 | Steps = 738 | Steps Per Second = 333.296\n",
            "[Learner] Action = -4.000 | Avg Td Error = 14.854 | Q = -15.858 | Reward = -0.816 | State = 450|2|50 | Steps = 91103 | Walltime = 265.527\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.041 | Episodes = 369 | Steps = 1107 | Steps Per Second = 381.682\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.731 | Q = -0.229 | Reward = 2.661 | State = 420|-1|53 | Steps = 91455 | Walltime = 266.529\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.637 | Episodes = 487 | Steps = 1461 | Steps Per Second = 381.694\n",
            "[Learner] Action = -1.000 | Avg Td Error = -5.991 | Q = -0.117 | Reward = -6.107 | State = 390|3|55 | Steps = 91796 | Walltime = 267.535\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.560 | Episodes = 600 | Steps = 1800 | Steps Per Second = 282.153\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.380 | Q = -2.969 | Reward = -5.281 | State = 450|2|50 | Steps = 92143 | Walltime = 268.538\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.707 | Episodes = 716 | Steps = 2148 | Steps Per Second = 350.070\n",
            "[Learner] Action = 1.000 | Avg Td Error = -9.449 | Q = -4.489 | Reward = -13.810 | State = 450|2|50 | Steps = 92480 | Walltime = 269.539\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.902 | Episodes = 828 | Steps = 2484 | Steps Per Second = 207.184\n",
            "[Learner] Action = 2.000 | Avg Td Error = -54.835 | Q = -8.226 | Reward = -63.059 | State = 450|2|50 | Steps = 92825 | Walltime = 270.540\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.210 | Episodes = 944 | Steps = 2832 | Steps Per Second = 244.395\n",
            "[Learner] Action = 3.000 | Avg Td Error = 6.673 | Q = -0.625 | Reward = 6.052 | State = 420|0|50 | Steps = 93151 | Walltime = 271.541\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -354.874 | Episodes = 1053 | Steps = 3159 | Steps Per Second = 382.634\n",
            "[Learner] Action = -4.000 | Avg Td Error = -15.188 | Q = -0.203 | Reward = -15.323 | State = 420|1|46 | Steps = 93522 | Walltime = 272.541\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -131.131 | Episodes = 1177 | Steps = 3531 | Steps Per Second = 380.034\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.744 | Q = -0.156 | Reward = 1.588 | State = 390|1|48 | Steps = 93889 | Walltime = 273.543\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.891 | Episodes = 1300 | Steps = 3900 | Steps Per Second = 373.868\n",
            "[Learner] Action = 3.000 | Avg Td Error = 0.511 | Q = -0.775 | Reward = -0.265 | State = 420|-1|48 | Steps = 94216 | Walltime = 274.545\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.867 | Episodes = 1409 | Steps = 4227 | Steps Per Second = 393.918\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.898 | Q = -8.089 | Reward = -1.072 | State = 450|2|50 | Steps = 94584 | Walltime = 275.547\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.704 | Episodes = 1532 | Steps = 4596 | Steps Per Second = 382.739\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.314 | Q = -0.032 | Reward = 0.282 | State = 390|6|49 | Steps = 94923 | Walltime = 276.549\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.664 | Episodes = 1645 | Steps = 4935 | Steps Per Second = 361.921\n",
            "[Learner] Action = -1.000 | Avg Td Error = -125.160 | Q = -0.060 | Reward = -125.221 | State = 420|-1|55 | Steps = 95255 | Walltime = 277.551\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.076 | Episodes = 1756 | Steps = 5268 | Steps Per Second = 365.336\n",
            "[Learner] Action = -4.000 | Avg Td Error = 4.332 | Q = -15.964 | Reward = -10.993 | State = 450|2|50 | Steps = 95589 | Walltime = 278.551\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.005 | Episodes = 1868 | Steps = 5604 | Steps Per Second = 381.485\n",
            "[Learner] Action = -2.000 | Avg Td Error = 12.601 | Q = -8.221 | Reward = 4.368 | State = 450|2|50 | Steps = 95947 | Walltime = 279.553\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.727 | Episodes = 1988 | Steps = 5964 | Steps Per Second = 374.102\n",
            "[Learner] Action = 3.000 | Avg Td Error = 6.818 | Q = -10.711 | Reward = -3.879 | State = 450|2|50 | Steps = 96300 | Walltime = 280.555\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.154 | Episodes = 2106 | Steps = 6318 | Steps Per Second = 369.184\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.788 | Q = -0.277 | Reward = -5.860 | State = 420|5|51 | Steps = 96672 | Walltime = 281.558\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.527 | Episodes = 2230 | Steps = 6690 | Steps Per Second = 381.937\n",
            "[Learner] Action = 2.000 | Avg Td Error = 0.304 | Q = -0.324 | Reward = -0.028 | State = 420|3|49 | Steps = 97049 | Walltime = 282.559\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.307 | Episodes = 2356 | Steps = 7068 | Steps Per Second = 351.213\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.049 | Q = -2.801 | Reward = 6.248 | State = 450|2|50 | Steps = 97422 | Walltime = 283.562\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.319 | Episodes = 2480 | Steps = 7440 | Steps Per Second = 372.000\n",
            "[Learner] Action = -4.000 | Avg Td Error = 13.823 | Q = -16.024 | Reward = -1.988 | State = 450|2|50 | Steps = 97772 | Walltime = 284.563\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.139 | Episodes = 2597 | Steps = 7791 | Steps Per Second = 391.784\n",
            "[Learner] Action = -3.000 | Avg Td Error = 1.884 | Q = -11.978 | Reward = -9.554 | State = 450|2|50 | Steps = 98150 | Walltime = 285.563\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.208 | Episodes = 2723 | Steps = 8169 | Steps Per Second = 380.332\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.788 | Q = -0.462 | Reward = -9.181 | State = 420|6|49 | Steps = 98526 | Walltime = 286.565\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -184.068 | Episodes = 2849 | Steps = 8547 | Steps Per Second = 330.182\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.743 | Q = -0.027 | Reward = -0.770 | State = 390|2|45 | Steps = 98899 | Walltime = 287.567\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.868 | Episodes = 2974 | Steps = 8922 | Steps Per Second = 353.195\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.191 | Q = -2.959 | Reward = 6.241 | State = 450|2|50 | Steps = 99240 | Walltime = 288.568\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.654 | Episodes = 3087 | Steps = 9261 | Steps Per Second = 337.262\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.670 | Q = -12.135 | Reward = -13.023 | State = 450|2|50 | Steps = 99598 | Walltime = 289.570\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -77.207 | Episodes = 3208 | Steps = 9624 | Steps Per Second = 366.134\n",
            "[Learner] Action = 4.000 | Avg Td Error = -32.890 | Q = -0.153 | Reward = -33.052 | State = 420|1|46 | Steps = 99966 | Walltime = 290.571\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.109 | Episodes = 3331 | Steps = 9993 | Steps Per Second = 396.700\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.321 | Q = -0.194 | Reward = -2.514 | State = 390|2|54 | Steps = 100339 | Walltime = 291.571\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.286 | Episodes = 3456 | Steps = 10368 | Steps Per Second = 373.447\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.017 | Q = -0.063 | Reward = 1.950 | State = 420|3|48 | Steps = 100704 | Walltime = 292.573\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.550 | Episodes = 3578 | Steps = 10734 | Steps Per Second = 373.037\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.811 | Q = -0.248 | Reward = -10.066 | State = 420|1|47 | Steps = 101046 | Walltime = 293.575\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.890 | Episodes = 3692 | Steps = 11076 | Steps Per Second = 381.949\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.076 | Q = -2.814 | Reward = 1.905 | State = 450|2|50 | Steps = 101420 | Walltime = 294.577\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.377 | Episodes = 3817 | Steps = 11451 | Steps Per Second = 377.073\n",
            "[Learner] Action = 0.000 | Avg Td Error = -24.195 | Q = -2.728 | Reward = -26.801 | State = 450|2|50 | Steps = 101788 | Walltime = 295.578\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.751 | Episodes = 3940 | Steps = 11820 | Steps Per Second = 359.389\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.854 | Q = -4.454 | Reward = 1.539 | State = 450|2|50 | Steps = 102133 | Walltime = 296.580\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.744 | Episodes = 4055 | Steps = 12165 | Steps Per Second = 363.259\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.375 | Q = -0.255 | Reward = 5.120 | State = 390|3|52 | Steps = 102487 | Walltime = 297.581\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -149.832 | Episodes = 4174 | Steps = 12522 | Steps Per Second = 375.083\n",
            "[Learner] Action = 3.000 | Avg Td Error = 6.391 | Q = -11.309 | Reward = -4.904 | State = 450|2|50 | Steps = 102851 | Walltime = 298.581\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.412 | Episodes = 4296 | Steps = 12888 | Steps Per Second = 374.826\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.871 | Q = -0.005 | Reward = -0.876 | State = 390|3|41 | Steps = 103216 | Walltime = 299.583\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.439 | Episodes = 4417 | Steps = 13251 | Steps Per Second = 371.649\n",
            "[Learner] Action = 4.000 | Avg Td Error = -169.801 | Q = -1.334 | Reward = -171.135 | State = 390|-2|50 | Steps = 103579 | Walltime = 300.583\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.709 | Episodes = 4540 | Steps = 13620 | Steps Per Second = 373.303\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.328 | Q = -2.636 | Reward = 0.272 | State = 450|2|50 | Steps = 103951 | Walltime = 301.585\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.532 | Episodes = 4664 | Steps = 13992 | Steps Per Second = 373.546\n",
            "[Learner] Action = 1.000 | Avg Td Error = -22.623 | Q = -0.139 | Reward = -22.762 | State = 390|0|53 | Steps = 104315 | Walltime = 302.585\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.429 | Episodes = 4786 | Steps = 14358 | Steps Per Second = 378.866\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.089 | Q = -0.419 | Reward = -7.398 | State = 420|1|50 | Steps = 104687 | Walltime = 303.587\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.008 | Episodes = 4910 | Steps = 14730 | Steps Per Second = 382.657\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.621 | Episodes = 5170 | Steps = 15510 | Steps Per Second = 1991.597\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.983 | Episodes = 5797 | Steps = 17391 | Steps Per Second = 1954.172\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.625 | Q = -0.222 | Reward = -5.810 | State = 420|6|49 | Steps = 105000 | Walltime = 306.045\n",
            "Check Point 7\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.833 | Q = 0.002 | Reward = 2.876 | State = 420|5|55 | Steps = 105359 | Walltime = 307.046\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.621 | Episodes = 121 | Steps = 363 | Steps Per Second = 379.735\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.453 | Q = -8.191 | Reward = -11.137 | State = 450|2|50 | Steps = 105711 | Walltime = 308.046\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.720 | Episodes = 238 | Steps = 714 | Steps Per Second = 259.978\n",
            "[Learner] Action = 3.000 | Avg Td Error = 13.410 | Q = -11.271 | Reward = 2.162 | State = 450|2|50 | Steps = 106050 | Walltime = 309.049\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.851 | Episodes = 351 | Steps = 1053 | Steps Per Second = 203.042\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.956 | Q = -0.144 | Reward = -7.100 | State = 390|7|51 | Steps = 106406 | Walltime = 310.050\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.092 | Episodes = 471 | Steps = 1413 | Steps Per Second = 201.885\n",
            "[Learner] Action = 2.000 | Avg Td Error = 2.037 | Q = -0.087 | Reward = 1.951 | State = 390|-2|45 | Steps = 106774 | Walltime = 311.054\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.058 | Episodes = 594 | Steps = 1782 | Steps Per Second = 202.304\n",
            "[Learner] Action = -3.000 | Avg Td Error = 14.633 | Q = -12.323 | Reward = 2.370 | State = 450|2|50 | Steps = 107119 | Walltime = 312.055\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.994 | Episodes = 711 | Steps = 2133 | Steps Per Second = 236.161\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.178 | Q = -1.394 | Reward = 5.886 | State = 420|2|52 | Steps = 107459 | Walltime = 313.056\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.320 | Episodes = 825 | Steps = 2475 | Steps Per Second = 247.959\n",
            "[Learner] Action = 1.000 | Avg Td Error = -3.713 | Q = -0.020 | Reward = -3.733 | State = 390|9|54 | Steps = 107783 | Walltime = 314.058\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.767 | Episodes = 933 | Steps = 2799 | Steps Per Second = 312.363\n",
            "[Learner] Action = 2.000 | Avg Td Error = -11.715 | Q = -0.127 | Reward = -11.825 | State = 420|5|53 | Steps = 108132 | Walltime = 315.058\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.911 | Episodes = 1051 | Steps = 3153 | Steps Per Second = 385.058\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.501 | Q = -0.359 | Reward = -5.861 | State = 390|0|46 | Steps = 108497 | Walltime = 316.060\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.913 | Episodes = 1173 | Steps = 3519 | Steps Per Second = 379.552\n",
            "[Learner] Action = 4.000 | Avg Td Error = -24.868 | Q = -0.744 | Reward = -25.608 | State = 420|4|52 | Steps = 108873 | Walltime = 317.062\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.471 | Episodes = 1299 | Steps = 3897 | Steps Per Second = 386.809\n",
            "[Learner] Action = 4.000 | Avg Td Error = -11.394 | Q = -0.356 | Reward = -11.750 | State = 390|6|50 | Steps = 109241 | Walltime = 318.063\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.415 | Episodes = 1422 | Steps = 4266 | Steps Per Second = 363.416\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.917 | Q = -0.638 | Reward = 5.332 | State = 420|0|50 | Steps = 109605 | Walltime = 319.064\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -100.530 | Episodes = 1543 | Steps = 4629 | Steps Per Second = 349.380\n",
            "[Learner] Action = 2.000 | Avg Td Error = -20.641 | Q = -8.263 | Reward = -28.817 | State = 450|2|50 | Steps = 109963 | Walltime = 320.065\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.584 | Episodes = 1663 | Steps = 4989 | Steps Per Second = 375.352\n",
            "[Learner] Action = 0.000 | Avg Td Error = -9.900 | Q = -1.069 | Reward = -10.558 | State = 420|-1|50 | Steps = 110314 | Walltime = 321.067\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.626 | Episodes = 1779 | Steps = 5337 | Steps Per Second = 341.296\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.897 | Q = -0.145 | Reward = 2.883 | State = 420|1|48 | Steps = 110635 | Walltime = 322.070\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.435 | Episodes = 1886 | Steps = 5658 | Steps Per Second = 278.512\n",
            "[Learner] Action = -3.000 | Avg Td Error = 3.272 | Q = -0.219 | Reward = 3.053 | State = 390|-1|45 | Steps = 110963 | Walltime = 323.071\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.644 | Episodes = 1997 | Steps = 5991 | Steps Per Second = 361.859\n",
            "[Learner] Action = -2.000 | Avg Td Error = 5.314 | Q = -1.648 | Reward = 3.895 | State = 420|-1|51 | Steps = 111322 | Walltime = 324.073\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.726 | Episodes = 2116 | Steps = 6348 | Steps Per Second = 360.119\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.242 | Q = -0.364 | Reward = -3.606 | State = 390|1|50 | Steps = 111678 | Walltime = 325.076\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.848 | Episodes = 2235 | Steps = 6705 | Steps Per Second = 336.082\n",
            "[Learner] Action = 1.000 | Avg Td Error = -7.976 | Q = -4.872 | Reward = -12.699 | State = 450|2|50 | Steps = 112011 | Walltime = 326.078\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.667 | Episodes = 2346 | Steps = 7038 | Steps Per Second = 331.714\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.243 | Q = -0.608 | Reward = -6.851 | State = 390|2|49 | Steps = 112363 | Walltime = 327.079\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.157 | Episodes = 2465 | Steps = 7395 | Steps Per Second = 398.635\n",
            "[Learner] Action = 4.000 | Avg Td Error = 11.433 | Q = -15.069 | Reward = -3.579 | State = 450|2|50 | Steps = 112728 | Walltime = 328.081\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.881 | Episodes = 2587 | Steps = 7761 | Steps Per Second = 375.699\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.998 | Q = -2.404 | Reward = -9.075 | State = 450|2|50 | Steps = 113092 | Walltime = 329.082\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.629 | Episodes = 2709 | Steps = 8127 | Steps Per Second = 378.764\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.663 | Q = -0.069 | Reward = -7.597 | State = 420|4|51 | Steps = 113452 | Walltime = 330.082\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.437 | Episodes = 2829 | Steps = 8487 | Steps Per Second = 353.393\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.999 | Q = -0.078 | Reward = 1.920 | State = 390|6|48 | Steps = 113821 | Walltime = 331.084\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.827 | Episodes = 2952 | Steps = 8856 | Steps Per Second = 374.815\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.861 | Q = -11.569 | Reward = -19.395 | State = 450|2|50 | Steps = 114182 | Walltime = 332.087\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -74.227 | Episodes = 3071 | Steps = 9213 | Steps Per Second = 337.452\n",
            "[Learner] Action = 3.000 | Avg Td Error = 13.750 | Q = -11.494 | Reward = 2.285 | State = 450|2|50 | Steps = 114534 | Walltime = 333.087\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.693 | Episodes = 3190 | Steps = 9570 | Steps Per Second = 373.668\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.214 | Q = -0.076 | Reward = 3.137 | State = 390|6|48 | Steps = 114876 | Walltime = 334.089\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.438 | Episodes = 3303 | Steps = 9909 | Steps Per Second = 336.361\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.777 | Q = -12.250 | Reward = -5.791 | State = 450|2|50 | Steps = 115231 | Walltime = 335.090\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.420 | Episodes = 3423 | Steps = 10269 | Steps Per Second = 391.954\n",
            "[Learner] Action = 2.000 | Avg Td Error = -19.190 | Q = -0.431 | Reward = -19.622 | State = 390|0|51 | Steps = 115588 | Walltime = 336.093\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.761 | Episodes = 3541 | Steps = 10623 | Steps Per Second = 342.290\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.196 | Q = -2.405 | Reward = -3.496 | State = 450|2|50 | Steps = 115942 | Walltime = 337.094\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -66.100 | Episodes = 3661 | Steps = 10983 | Steps Per Second = 387.703\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.232 | Q = -0.380 | Reward = 4.852 | State = 390|2|53 | Steps = 116297 | Walltime = 338.095\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.593 | Episodes = 3778 | Steps = 11334 | Steps Per Second = 329.232\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.445 | Q = -0.266 | Reward = -4.711 | State = 390|6|52 | Steps = 116648 | Walltime = 339.096\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.321 | Episodes = 3897 | Steps = 11691 | Steps Per Second = 377.888\n",
            "[Learner] Action = 0.000 | Avg Td Error = -13.445 | Q = -0.405 | Reward = -13.487 | State = 420|2|49 | Steps = 116994 | Walltime = 340.099\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.669 | Episodes = 4011 | Steps = 12033 | Steps Per Second = 345.400\n",
            "[Learner] Action = -2.000 | Avg Td Error = 5.355 | Q = -8.064 | Reward = -2.294 | State = 450|2|50 | Steps = 117349 | Walltime = 341.099\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.487 | Episodes = 4131 | Steps = 12393 | Steps Per Second = 374.625\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.333 | Q = -0.427 | Reward = 4.922 | State = 420|2|49 | Steps = 117709 | Walltime = 342.102\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -115.182 | Episodes = 4251 | Steps = 12753 | Steps Per Second = 386.311\n",
            "[Learner] Action = -2.000 | Avg Td Error = 2.653 | Q = -0.714 | Reward = 2.110 | State = 420|0|49 | Steps = 118067 | Walltime = 343.102\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.851 | Episodes = 4370 | Steps = 13110 | Steps Per Second = 378.547\n",
            "[Learner] Action = -3.000 | Avg Td Error = -74.323 | Q = -0.806 | Reward = -75.129 | State = 390|-1|54 | Steps = 118432 | Walltime = 344.103\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.260 | Episodes = 4492 | Steps = 13476 | Steps Per Second = 374.068\n",
            "[Learner] Action = 1.000 | Avg Td Error = -8.696 | Q = -1.184 | Reward = -9.635 | State = 420|2|51 | Steps = 118798 | Walltime = 345.105\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.806 | Episodes = 4615 | Steps = 13845 | Steps Per Second = 376.790\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.366 | Q = -0.020 | Reward = 1.367 | State = 420|6|50 | Steps = 119171 | Walltime = 346.107\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -101.106 | Episodes = 4740 | Steps = 14220 | Steps Per Second = 361.225\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.157 | Q = -0.033 | Reward = 1.124 | State = 390|4|48 | Steps = 119509 | Walltime = 347.109\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.003 | Episodes = 4849 | Steps = 14547 | Steps Per Second = 323.086\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.473 | Q = -0.234 | Reward = 5.239 | State = 390|3|52 | Steps = 119815 | Walltime = 348.109\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.193 | Episodes = 4953 | Steps = 14859 | Steps Per Second = 319.842\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.508 | Episodes = 5352 | Steps = 16056 | Steps Per Second = 1951.747\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.239 | Episodes = 5954 | Steps = 17862 | Steps Per Second = 1869.397\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.248 | Q = -0.283 | Reward = -3.173 | State = 420|2|48 | Steps = 120000 | Walltime = 350.329\n",
            "Check Point 8\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.145 | Q = -0.381 | Reward = -0.236 | State = 390|1|48 | Steps = 120330 | Walltime = 351.331\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.277 | Episodes = 111 | Steps = 333 | Steps Per Second = 322.862\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.349 | Q = -0.038 | Reward = -6.387 | State = 390|4|44 | Steps = 120663 | Walltime = 352.332\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.619 | Episodes = 222 | Steps = 666 | Steps Per Second = 287.912\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.998 | Q = -0.183 | Reward = -6.181 | State = 420|2|45 | Steps = 121018 | Walltime = 353.332\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.286 | Episodes = 341 | Steps = 1023 | Steps Per Second = 400.794\n",
            "[Learner] Action = -1.000 | Avg Td Error = -5.584 | Q = -1.276 | Reward = -6.587 | State = 420|-2|50 | Steps = 121356 | Walltime = 354.335\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.153 | Episodes = 454 | Steps = 1362 | Steps Per Second = 335.938\n",
            "[Learner] Action = 0.000 | Avg Td Error = -23.458 | Q = -0.183 | Reward = -23.630 | State = 420|0|54 | Steps = 121719 | Walltime = 355.339\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.996 | Episodes = 575 | Steps = 1725 | Steps Per Second = 374.180\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.291 | Q = -0.092 | Reward = -0.202 | State = 420|2|47 | Steps = 122079 | Walltime = 356.340\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.014 | Episodes = 696 | Steps = 2088 | Steps Per Second = 375.677\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.141 | Q = -0.112 | Reward = 1.027 | State = 420|4|49 | Steps = 122441 | Walltime = 357.343\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.914 | Episodes = 817 | Steps = 2451 | Steps Per Second = 386.512\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.334 | Q = -0.638 | Reward = -6.852 | State = 420|0|49 | Steps = 122803 | Walltime = 358.345\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.151 | Episodes = 938 | Steps = 2814 | Steps Per Second = 374.202\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.996 | Q = -2.280 | Reward = 5.811 | State = 450|2|50 | Steps = 123179 | Walltime = 359.346\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -93.356 | Episodes = 1064 | Steps = 3192 | Steps Per Second = 380.401\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.029 | Q = -7.586 | Reward = -12.537 | State = 450|2|50 | Steps = 123539 | Walltime = 360.348\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.287 | Episodes = 1184 | Steps = 3552 | Steps Per Second = 376.272\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.336 | Q = -2.459 | Reward = 3.230 | State = 450|2|50 | Steps = 123902 | Walltime = 361.348\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.820 | Episodes = 1305 | Steps = 3915 | Steps Per Second = 346.436\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.457 | Q = 0.003 | Reward = 0.460 | State = 390|3|44 | Steps = 124263 | Walltime = 362.350\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.113 | Episodes = 1426 | Steps = 4278 | Steps Per Second = 380.057\n",
            "[Learner] Action = 4.000 | Avg Td Error = -14.284 | Q = -1.464 | Reward = -15.566 | State = 420|-1|51 | Steps = 124636 | Walltime = 363.352\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.863 | Episodes = 1551 | Steps = 4653 | Steps Per Second = 388.278\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.117 | Q = -0.660 | Reward = -5.667 | State = 420|1|49 | Steps = 124995 | Walltime = 364.354\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.953 | Episodes = 1670 | Steps = 5010 | Steps Per Second = 332.494\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.177 | Q = 0.000 | Reward = 2.177 | State = 390|-1|40 | Steps = 125361 | Walltime = 365.357\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -68.025 | Episodes = 1793 | Steps = 5379 | Steps Per Second = 386.192\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.544 | Q = -0.491 | Reward = 6.139 | State = 420|0|49 | Steps = 125715 | Walltime = 366.359\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -73.833 | Episodes = 1911 | Steps = 5733 | Steps Per Second = 394.412\n",
            "[Learner] Action = 3.000 | Avg Td Error = 0.942 | Q = -11.580 | Reward = -10.607 | State = 450|2|50 | Steps = 126058 | Walltime = 367.362\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -161.446 | Episodes = 2025 | Steps = 6075 | Steps Per Second = 336.820\n",
            "[Learner] Action = 2.000 | Avg Td Error = -17.101 | Q = -0.402 | Reward = -17.511 | State = 420|4|50 | Steps = 126422 | Walltime = 368.363\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.020 | Episodes = 2147 | Steps = 6441 | Steps Per Second = 379.506\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.739 | Q = -0.805 | Reward = 1.934 | State = 390|0|48 | Steps = 126773 | Walltime = 369.364\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.024 | Episodes = 2264 | Steps = 6792 | Steps Per Second = 334.021\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.882 | Q = -0.083 | Reward = 0.800 | State = 390|0|47 | Steps = 127136 | Walltime = 370.366\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.130 | Episodes = 2384 | Steps = 7152 | Steps Per Second = 386.026\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.214 | Q = -0.321 | Reward = -5.459 | State = 420|6|51 | Steps = 127489 | Walltime = 371.368\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -140.715 | Episodes = 2504 | Steps = 7512 | Steps Per Second = 375.195\n",
            "[Learner] Action = 2.000 | Avg Td Error = -7.055 | Q = -0.357 | Reward = -7.412 | State = 390|2|47 | Steps = 127841 | Walltime = 372.370\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.264 | Episodes = 2621 | Steps = 7863 | Steps Per Second = 353.651\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.165 | Q = -0.220 | Reward = -6.386 | State = 390|5|47 | Steps = 128209 | Walltime = 373.371\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.926 | Episodes = 2744 | Steps = 8232 | Steps Per Second = 322.135\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.785 | Q = -0.010 | Reward = 0.776 | State = 390|5|50 | Steps = 128547 | Walltime = 374.371\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.738 | Episodes = 2856 | Steps = 8568 | Steps Per Second = 331.907\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.438 | Q = -0.257 | Reward = -5.694 | State = 390|7|49 | Steps = 128918 | Walltime = 375.371\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.371 | Episodes = 2981 | Steps = 8943 | Steps Per Second = 390.749\n",
            "[Learner] Action = 3.000 | Avg Td Error = -38.148 | Q = -0.546 | Reward = -38.695 | State = 390|2|48 | Steps = 129286 | Walltime = 376.373\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -80.109 | Episodes = 3104 | Steps = 9312 | Steps Per Second = 381.613\n",
            "[Learner] Action = -4.000 | Avg Td Error = -134.860 | Q = -1.079 | Reward = -135.939 | State = 420|-2|52 | Steps = 129653 | Walltime = 377.374\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.091 | Episodes = 3226 | Steps = 9678 | Steps Per Second = 338.169\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.157 | Q = -1.035 | Reward = 0.122 | State = 390|2|52 | Steps = 130025 | Walltime = 378.374\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -76.215 | Episodes = 3351 | Steps = 10053 | Steps Per Second = 388.218\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.127 | Q = -5.018 | Reward = 1.354 | State = 450|2|50 | Steps = 130394 | Walltime = 379.375\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.732 | Episodes = 3474 | Steps = 10422 | Steps Per Second = 353.274\n",
            "[Learner] Action = -4.000 | Avg Td Error = -11.390 | Q = -16.245 | Reward = -27.037 | State = 450|2|50 | Steps = 130764 | Walltime = 380.376\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.527 | Episodes = 3598 | Steps = 10794 | Steps Per Second = 378.388\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.057 | Q = -0.295 | Reward = -1.248 | State = 420|4|49 | Steps = 131094 | Walltime = 381.378\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.446 | Episodes = 3708 | Steps = 11124 | Steps Per Second = 377.729\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.819 | Q = -0.284 | Reward = -7.103 | State = 390|4|50 | Steps = 131469 | Walltime = 382.378\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -133.826 | Episodes = 3833 | Steps = 11499 | Steps Per Second = 395.502\n",
            "[Learner] Action = 3.000 | Avg Td Error = -20.076 | Q = -0.743 | Reward = -20.645 | State = 420|0|50 | Steps = 131843 | Walltime = 383.379\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.441 | Episodes = 3958 | Steps = 11874 | Steps Per Second = 371.627\n",
            "[Learner] Action = 2.000 | Avg Td Error = 3.343 | Q = -8.402 | Reward = -4.994 | State = 450|2|50 | Steps = 132217 | Walltime = 384.381\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.304 | Episodes = 4083 | Steps = 12249 | Steps Per Second = 396.637\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.059 | Q = -0.851 | Reward = 4.207 | State = 390|-2|51 | Steps = 132570 | Walltime = 385.382\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -88.353 | Episodes = 4200 | Steps = 12600 | Steps Per Second = 331.225\n",
            "[Learner] Action = -2.000 | Avg Td Error = 10.216 | Q = -8.653 | Reward = 1.711 | State = 450|2|50 | Steps = 132942 | Walltime = 386.384\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.323 | Episodes = 4325 | Steps = 12975 | Steps Per Second = 381.520\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.663 | Q = -0.062 | Reward = 1.608 | State = 420|5|51 | Steps = 133289 | Walltime = 387.386\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.398 | Episodes = 4440 | Steps = 13320 | Steps Per Second = 323.884\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.956 | Q = -0.068 | Reward = -2.939 | State = 420|6|48 | Steps = 133652 | Walltime = 388.387\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.254 | Episodes = 4563 | Steps = 13689 | Steps Per Second = 389.576\n",
            "[Learner] Action = -1.000 | Avg Td Error = -12.505 | Q = -0.167 | Reward = -12.672 | State = 390|1|47 | Steps = 134026 | Walltime = 389.388\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.363 | Episodes = 4688 | Steps = 14064 | Steps Per Second = 371.024\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.391 | Q = -0.267 | Reward = -3.404 | State = 420|1|52 | Steps = 134399 | Walltime = 390.390\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.637 | Episodes = 4813 | Steps = 14439 | Steps Per Second = 353.800\n",
            "[Learner] Action = -4.000 | Avg Td Error = -11.551 | Q = -0.992 | Reward = -12.336 | State = 420|2|47 | Steps = 134749 | Walltime = 391.391\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.996 | Episodes = 4930 | Steps = 14790 | Steps Per Second = 367.760\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.821 | Episodes = 5289 | Steps = 15867 | Steps Per Second = 1935.535\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.169 | Episodes = 5915 | Steps = 17745 | Steps Per Second = 1849.884\n",
            "[Learner] Action = 3.000 | Avg Td Error = 3.819 | Q = -11.828 | Reward = -7.982 | State = 450|2|50 | Steps = 135000 | Walltime = 393.639\n",
            "Check Point 9\n",
            "[Learner] Action = -4.000 | Avg Td Error = 17.244 | Q = -16.415 | Reward = 0.893 | State = 450|2|50 | Steps = 135364 | Walltime = 394.643\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.277 | Episodes = 123 | Steps = 369 | Steps Per Second = 333.146\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.679 | Q = -0.020 | Reward = 1.659 | State = 390|2|46 | Steps = 135736 | Walltime = 395.645\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.105 | Episodes = 248 | Steps = 744 | Steps Per Second = 364.437\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.868 | Q = -0.895 | Reward = 5.988 | State = 420|2|50 | Steps = 136070 | Walltime = 396.646\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.168 | Episodes = 360 | Steps = 1080 | Steps Per Second = 383.918\n",
            "[Learner] Action = -4.000 | Avg Td Error = 13.162 | Q = -16.360 | Reward = -2.923 | State = 450|2|50 | Steps = 136413 | Walltime = 397.647\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.791 | Episodes = 474 | Steps = 1422 | Steps Per Second = 358.457\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.618 | Q = -0.227 | Reward = -2.845 | State = 390|1|54 | Steps = 136772 | Walltime = 398.652\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.639 | Episodes = 594 | Steps = 1782 | Steps Per Second = 372.683\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.628 | Q = -0.526 | Reward = -3.154 | State = 390|2|50 | Steps = 137145 | Walltime = 399.652\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.797 | Episodes = 719 | Steps = 2157 | Steps Per Second = 385.671\n",
            "[Learner] Action = -3.000 | Avg Td Error = -30.066 | Q = -1.824 | Reward = -31.743 | State = 420|2|51 | Steps = 137487 | Walltime = 400.652\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.399 | Episodes = 833 | Steps = 2499 | Steps Per Second = 381.023\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.770 | Q = -0.136 | Reward = -0.851 | State = 420|6|50 | Steps = 137828 | Walltime = 401.653\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.851 | Episodes = 947 | Steps = 2841 | Steps Per Second = 384.422\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.582 | Q = -0.326 | Reward = -7.496 | State = 420|2|48 | Steps = 138194 | Walltime = 402.654\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.160 | Episodes = 1069 | Steps = 3207 | Steps Per Second = 382.960\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.778 | Q = -0.578 | Reward = 6.200 | State = 390|-1|51 | Steps = 138521 | Walltime = 403.656\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.470 | Episodes = 1178 | Steps = 3534 | Steps Per Second = 360.738\n",
            "[Learner] Action = -2.000 | Avg Td Error = -2.280 | Q = -0.034 | Reward = -2.314 | State = 390|4|44 | Steps = 138886 | Walltime = 404.657\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.681 | Episodes = 1300 | Steps = 3900 | Steps Per Second = 347.940\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.345 | Q = -0.593 | Reward = 0.753 | State = 390|-2|52 | Steps = 139238 | Walltime = 405.658\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.828 | Episodes = 1418 | Steps = 4254 | Steps Per Second = 384.575\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.135 | Q = -0.573 | Reward = -8.601 | State = 420|6|51 | Steps = 139611 | Walltime = 406.661\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.408 | Episodes = 1542 | Steps = 4626 | Steps Per Second = 380.827\n",
            "[Learner] Action = -3.000 | Avg Td Error = -3.058 | Q = -0.895 | Reward = -3.953 | State = 390|2|48 | Steps = 139984 | Walltime = 407.663\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.782 | Episodes = 1667 | Steps = 5001 | Steps Per Second = 384.317\n",
            "[Learner] Action = 3.000 | Avg Td Error = -12.392 | Q = -0.825 | Reward = -13.082 | State = 420|0|50 | Steps = 140355 | Walltime = 408.665\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.658 | Episodes = 1791 | Steps = 5373 | Steps Per Second = 377.220\n",
            "[Learner] Action = 0.000 | Avg Td Error = -20.393 | Q = -0.389 | Reward = -20.386 | State = 420|0|52 | Steps = 140707 | Walltime = 409.667\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.199 | Episodes = 1908 | Steps = 5724 | Steps Per Second = 337.371\n",
            "[Learner] Action = -3.000 | Avg Td Error = 1.618 | Q = -2.702 | Reward = -0.951 | State = 420|2|50 | Steps = 141040 | Walltime = 410.667\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.262 | Episodes = 2020 | Steps = 6060 | Steps Per Second = 367.921\n",
            "[Learner] Action = 1.000 | Avg Td Error = -5.467 | Q = -0.175 | Reward = -5.614 | State = 420|5|50 | Steps = 141370 | Walltime = 411.669\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.273 | Episodes = 2131 | Steps = 6393 | Steps Per Second = 391.893\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.210 | Q = -0.881 | Reward = 0.746 | State = 420|2|50 | Steps = 141746 | Walltime = 412.671\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.318 | Episodes = 2256 | Steps = 6768 | Steps Per Second = 363.489\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.687 | Q = -0.693 | Reward = 4.065 | State = 420|2|49 | Steps = 142109 | Walltime = 413.672\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -77.104 | Episodes = 2378 | Steps = 7134 | Steps Per Second = 385.872\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.183 | Q = -0.406 | Reward = 1.160 | State = 420|2|49 | Steps = 142445 | Walltime = 414.672\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.880 | Episodes = 2490 | Steps = 7470 | Steps Per Second = 371.320\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.759 | Q = -0.125 | Reward = 4.634 | State = 390|4|52 | Steps = 142809 | Walltime = 415.674\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -92.518 | Episodes = 2612 | Steps = 7836 | Steps Per Second = 384.305\n",
            "[Learner] Action = 4.000 | Avg Td Error = 6.478 | Q = -0.250 | Reward = 6.228 | State = 390|-4|51 | Steps = 143181 | Walltime = 416.676\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.638 | Episodes = 2736 | Steps = 8208 | Steps Per Second = 343.551\n",
            "[Learner] Action = 1.000 | Avg Td Error = -9.162 | Q = -0.244 | Reward = -9.364 | State = 420|4|52 | Steps = 143548 | Walltime = 417.677\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -75.737 | Episodes = 2858 | Steps = 8574 | Steps Per Second = 311.551\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.013 | Q = -2.991 | Reward = 3.409 | State = 450|2|50 | Steps = 143881 | Walltime = 418.680\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.318 | Episodes = 2971 | Steps = 8913 | Steps Per Second = 381.139\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.631 | Q = -0.225 | Reward = -5.856 | State = 390|6|47 | Steps = 144253 | Walltime = 419.682\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.854 | Episodes = 3094 | Steps = 9282 | Steps Per Second = 323.036\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.605 | Q = -0.696 | Reward = 4.910 | State = 390|2|51 | Steps = 144588 | Walltime = 420.682\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.452 | Episodes = 3208 | Steps = 9624 | Steps Per Second = 373.458\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.044 | Q = -0.331 | Reward = 4.714 | State = 390|0|48 | Steps = 144943 | Walltime = 421.682\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.850 | Episodes = 3327 | Steps = 9981 | Steps Per Second = 384.328\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.189 | Q = -1.895 | Reward = 5.720 | State = 420|2|52 | Steps = 145311 | Walltime = 422.683\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.558 | Episodes = 3450 | Steps = 10350 | Steps Per Second = 367.696\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.737 | Q = -2.702 | Reward = 0.922 | State = 450|2|50 | Steps = 145672 | Walltime = 423.685\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.960 | Episodes = 3571 | Steps = 10713 | Steps Per Second = 401.228\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.372 | Q = -0.954 | Reward = -13.222 | State = 420|5|49 | Steps = 146041 | Walltime = 424.688\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -99.166 | Episodes = 3694 | Steps = 11082 | Steps Per Second = 382.448\n",
            "[Learner] Action = 4.000 | Avg Td Error = 8.667 | Q = -16.069 | Reward = -7.365 | State = 450|2|50 | Steps = 146406 | Walltime = 425.690\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.888 | Episodes = 3816 | Steps = 11448 | Steps Per Second = 379.300\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.911 | Q = -2.637 | Reward = 4.607 | State = 450|2|50 | Steps = 146775 | Walltime = 426.691\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.493 | Episodes = 3938 | Steps = 11814 | Steps Per Second = 335.276\n",
            "[Learner] Action = 1.000 | Avg Td Error = -13.400 | Q = -0.171 | Reward = -13.571 | State = 420|4|48 | Steps = 147105 | Walltime = 427.692\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.263 | Episodes = 4050 | Steps = 12150 | Steps Per Second = 356.507\n",
            "[Learner] Action = -4.000 | Avg Td Error = -3.209 | Q = -0.185 | Reward = -3.393 | State = 390|-1|45 | Steps = 147470 | Walltime = 428.692\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -55.667 | Episodes = 4171 | Steps = 12513 | Steps Per Second = 344.464\n",
            "[Learner] Action = 4.000 | Avg Td Error = -15.453 | Q = -0.931 | Reward = -16.386 | State = 420|4|52 | Steps = 147828 | Walltime = 429.693\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.896 | Episodes = 4292 | Steps = 12876 | Steps Per Second = 334.652\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.508 | Q = -4.801 | Reward = 1.070 | State = 450|2|50 | Steps = 148187 | Walltime = 430.694\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.067 | Episodes = 4412 | Steps = 13236 | Steps Per Second = 362.934\n",
            "[Learner] Action = 0.000 | Avg Td Error = -10.056 | Q = -2.709 | Reward = -11.978 | State = 450|2|50 | Steps = 148523 | Walltime = 431.696\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.805 | Episodes = 4524 | Steps = 13572 | Steps Per Second = 344.898\n",
            "[Learner] Action = 2.000 | Avg Td Error = -49.422 | Q = -0.517 | Reward = -49.939 | State = 390|3|53 | Steps = 148876 | Walltime = 432.697\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.688 | Episodes = 4640 | Steps = 13920 | Steps Per Second = 307.013\n",
            "[Learner] Action = -1.000 | Avg Td Error = -6.101 | Q = -4.796 | Reward = -10.658 | State = 450|2|50 | Steps = 149191 | Walltime = 433.699\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.825 | Episodes = 4748 | Steps = 14244 | Steps Per Second = 378.013\n",
            "[Learner] Action = 0.000 | Avg Td Error = -10.042 | Q = -2.767 | Reward = -10.943 | State = 450|2|50 | Steps = 149551 | Walltime = 434.700\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.955 | Episodes = 4867 | Steps = 14601 | Steps Per Second = 385.754\n",
            "[Learner] Action = 1.000 | Avg Td Error = -28.112 | Q = -0.525 | Reward = -28.637 | State = 390|-1|51 | Steps = 149913 | Walltime = 435.701\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.715 | Episodes = 4989 | Steps = 14967 | Steps Per Second = 376.531\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.020 | Episodes = 5579 | Steps = 16737 | Steps Per Second = 1951.747\n",
            "[Learner] Action = -2.000 | Avg Td Error = -1.258 | Q = -0.970 | Reward = -2.141 | State = 420|2|48 | Steps = 150000 | Walltime = 437.516\n",
            "Check Point 10\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.801 | Q = -2.723 | Reward = 2.433 | State = 450|2|50 | Steps = 150369 | Walltime = 438.518\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -89.781 | Episodes = 125 | Steps = 375 | Steps Per Second = 367.706\n",
            "[Learner] Action = 2.000 | Avg Td Error = 4.923 | Q = -0.670 | Reward = 4.250 | State = 420|-2|48 | Steps = 150701 | Walltime = 439.519\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.044 | Episodes = 236 | Steps = 708 | Steps Per Second = 321.075\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.362 | Q = -0.142 | Reward = 0.220 | State = 390|2|46 | Steps = 151068 | Walltime = 440.519\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.904 | Episodes = 359 | Steps = 1077 | Steps Per Second = 369.564\n",
            "[Learner] Action = -3.000 | Avg Td Error = 14.174 | Q = -13.368 | Reward = 0.984 | State = 450|2|50 | Steps = 151427 | Walltime = 441.521\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.038 | Episodes = 478 | Steps = 1434 | Steps Per Second = 166.635\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.986 | Q = 0.009 | Reward = 0.995 | State = 390|6|51 | Steps = 151761 | Walltime = 442.528\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.938 | Episodes = 591 | Steps = 1773 | Steps Per Second = 380.413\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.088 | Q = -0.760 | Reward = 5.429 | State = 420|0|49 | Steps = 152127 | Walltime = 443.529\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.292 | Episodes = 714 | Steps = 2142 | Steps Per Second = 367.739\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.248 | Q = -2.554 | Reward = 5.785 | State = 450|2|50 | Steps = 152470 | Walltime = 444.530\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.439 | Episodes = 829 | Steps = 2487 | Steps Per Second = 384.235\n",
            "[Learner] Action = 2.000 | Avg Td Error = 3.941 | Q = -0.281 | Reward = 3.659 | State = 390|-4|47 | Steps = 152814 | Walltime = 445.532\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.748 | Episodes = 943 | Steps = 2829 | Steps Per Second = 359.543\n",
            "[Learner] Action = -1.000 | Avg Td Error = -12.151 | Q = -0.719 | Reward = -12.140 | State = 420|2|49 | Steps = 153150 | Walltime = 446.534\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.811 | Episodes = 1055 | Steps = 3165 | Steps Per Second = 358.457\n",
            "[Learner] Action = 3.000 | Avg Td Error = -33.121 | Q = -0.950 | Reward = -34.071 | State = 390|2|50 | Steps = 153498 | Walltime = 447.534\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.605 | Episodes = 1171 | Steps = 3513 | Steps Per Second = 334.785\n",
            "[Learner] Action = 4.000 | Avg Td Error = -2.433 | Q = -1.112 | Reward = -3.544 | State = 390|2|52 | Steps = 153821 | Walltime = 448.536\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.559 | Episodes = 1279 | Steps = 3837 | Steps Per Second = 332.793\n",
            "[Learner] Action = -3.000 | Avg Td Error = -10.602 | Q = -13.667 | Reward = -23.592 | State = 450|2|50 | Steps = 154150 | Walltime = 449.537\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -64.015 | Episodes = 1390 | Steps = 4170 | Steps Per Second = 368.223\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.223 | Q = -4.894 | Reward = 5.327 | State = 450|2|50 | Steps = 154473 | Walltime = 450.539\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.856 | Episodes = 1498 | Steps = 4494 | Steps Per Second = 354.728\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.185 | Q = -0.048 | Reward = 1.137 | State = 390|4|47 | Steps = 154823 | Walltime = 451.541\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.187 | Episodes = 1614 | Steps = 4842 | Steps Per Second = 302.089\n",
            "[Learner] Action = 0.000 | Avg Td Error = -20.655 | Q = -1.212 | Reward = -21.628 | State = 420|2|51 | Steps = 155162 | Walltime = 452.543\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.998 | Episodes = 1728 | Steps = 5184 | Steps Per Second = 370.227\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.776 | Q = -2.544 | Reward = -8.485 | State = 450|2|50 | Steps = 155504 | Walltime = 453.544\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.960 | Episodes = 1843 | Steps = 5529 | Steps Per Second = 367.223\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.466 | Q = -0.645 | Reward = -4.111 | State = 390|2|49 | Steps = 155852 | Walltime = 454.546\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.929 | Episodes = 1959 | Steps = 5877 | Steps Per Second = 385.695\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.890 | Q = -0.173 | Reward = 5.717 | State = 390|-4|46 | Steps = 156214 | Walltime = 455.548\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -74.271 | Episodes = 2080 | Steps = 6240 | Steps Per Second = 378.616\n",
            "[Learner] Action = -2.000 | Avg Td Error = 7.100 | Q = -8.869 | Reward = -1.024 | State = 450|2|50 | Steps = 156585 | Walltime = 456.549\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.689 | Episodes = 2204 | Steps = 6612 | Steps Per Second = 316.663\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.039 | Q = -0.052 | Reward = 1.986 | State = 390|7|51 | Steps = 156952 | Walltime = 457.550\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.063 | Episodes = 2327 | Steps = 6981 | Steps Per Second = 229.093\n",
            "[Learner] Action = 1.000 | Avg Td Error = -12.723 | Q = -4.922 | Reward = -17.669 | State = 450|2|50 | Steps = 157288 | Walltime = 458.552\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.829 | Episodes = 2440 | Steps = 7320 | Steps Per Second = 376.486\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.350 | Q = -0.209 | Reward = 0.141 | State = 390|-1|48 | Steps = 157657 | Walltime = 459.553\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.084 | Episodes = 2564 | Steps = 7692 | Steps Per Second = 382.006\n",
            "[Learner] Action = -4.000 | Avg Td Error = 7.736 | Q = -2.213 | Reward = 5.654 | State = 420|0|51 | Steps = 158033 | Walltime = 460.555\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.651 | Episodes = 2689 | Steps = 8067 | Steps Per Second = 384.411\n",
            "[Learner] Action = 3.000 | Avg Td Error = 7.059 | Q = -1.380 | Reward = 6.149 | State = 420|-1|51 | Steps = 158408 | Walltime = 461.557\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.601 | Episodes = 2814 | Steps = 8442 | Steps Per Second = 372.309\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.702 | Q = -0.738 | Reward = 6.040 | State = 420|0|49 | Steps = 158736 | Walltime = 462.558\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.550 | Episodes = 2924 | Steps = 8772 | Steps Per Second = 366.774\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.528 | Q = -0.069 | Reward = 2.459 | State = 390|4|53 | Steps = 159112 | Walltime = 463.560\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -74.491 | Episodes = 3050 | Steps = 9150 | Steps Per Second = 357.063\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.504 | Q = -17.470 | Reward = -29.139 | State = 450|2|50 | Steps = 159485 | Walltime = 464.561\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.513 | Episodes = 3175 | Steps = 9525 | Steps Per Second = 378.138\n",
            "[Learner] Action = 3.000 | Avg Td Error = 9.212 | Q = -12.337 | Reward = -3.108 | State = 450|2|50 | Steps = 159828 | Walltime = 465.561\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.018 | Episodes = 3290 | Steps = 9870 | Steps Per Second = 383.707\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.403 | Q = -0.101 | Reward = 4.302 | State = 390|7|49 | Steps = 160181 | Walltime = 466.562\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -89.161 | Episodes = 3408 | Steps = 10224 | Steps Per Second = 377.888\n",
            "[Learner] Action = -3.000 | Avg Td Error = -13.578 | Q = -0.098 | Reward = -13.677 | State = 390|9|51 | Steps = 160553 | Walltime = 467.564\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -99.688 | Episodes = 3533 | Steps = 10599 | Steps Per Second = 398.989\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.033 | Q = -2.684 | Reward = -1.409 | State = 450|2|50 | Steps = 160896 | Walltime = 468.565\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -73.261 | Episodes = 3647 | Steps = 10941 | Steps Per Second = 376.486\n",
            "[Learner] Action = 1.000 | Avg Td Error = 8.439 | Q = -5.054 | Reward = 3.452 | State = 450|2|50 | Steps = 161266 | Walltime = 469.565\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.748 | Episodes = 3771 | Steps = 11313 | Steps Per Second = 394.041\n",
            "[Learner] Action = -2.000 | Avg Td Error = 12.620 | Q = -9.076 | Reward = 3.541 | State = 450|2|50 | Steps = 161640 | Walltime = 470.565\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -128.410 | Episodes = 3895 | Steps = 11685 | Steps Per Second = 387.918\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.501 | Q = -4.959 | Reward = 5.539 | State = 450|2|50 | Steps = 162002 | Walltime = 471.568\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.696 | Episodes = 4017 | Steps = 12051 | Steps Per Second = 394.808\n",
            "[Learner] Action = -4.000 | Avg Td Error = -13.412 | Q = -0.593 | Reward = -14.005 | State = 390|1|55 | Steps = 162338 | Walltime = 472.568\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.121 | Episodes = 4130 | Steps = 12390 | Steps Per Second = 372.904\n",
            "[Learner] Action = 1.000 | Avg Td Error = 2.730 | Q = -4.981 | Reward = -2.098 | State = 450|2|50 | Steps = 162715 | Walltime = 473.569\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.261 | Episodes = 4256 | Steps = 12768 | Steps Per Second = 380.643\n",
            "[Learner] Action = -1.000 | Avg Td Error = -16.700 | Q = -4.919 | Reward = -21.317 | State = 450|2|50 | Steps = 163091 | Walltime = 474.569\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.533 | Episodes = 4381 | Steps = 13143 | Steps Per Second = 379.129\n",
            "[Learner] Action = 2.000 | Avg Td Error = -23.813 | Q = -0.966 | Reward = -24.646 | State = 420|1|51 | Steps = 163471 | Walltime = 475.571\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.999 | Episodes = 4508 | Steps = 13524 | Steps Per Second = 379.884\n",
            "[Learner] Action = -4.000 | Avg Td Error = -16.058 | Q = -17.477 | Reward = -32.707 | State = 450|2|50 | Steps = 163835 | Walltime = 476.573\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -68.793 | Episodes = 4628 | Steps = 13884 | Steps Per Second = 373.402\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.432 | Q = -5.160 | Reward = 5.269 | State = 450|2|50 | Steps = 164206 | Walltime = 477.573\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.215 | Episodes = 4753 | Steps = 14259 | Steps Per Second = 392.382\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.862 | Q = -0.056 | Reward = 5.805 | State = 390|10|50 | Steps = 164577 | Walltime = 478.575\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.051 | Episodes = 4877 | Steps = 14631 | Steps Per Second = 388.625\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.890 | Q = -5.011 | Reward = -0.354 | State = 450|2|50 | Steps = 164944 | Walltime = 479.577\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.649 | Episodes = 5000 | Steps = 15000 | Steps Per Second = 497.250\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.694 | Episodes = 5636 | Steps = 16908 | Steps Per Second = 1986.880\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.906 | Q = -5.175 | Reward = -7.647 | State = 450|2|50 | Steps = 165000 | Walltime = 481.307\n",
            "Check Point 11\n",
            "[Learner] Action = -3.000 | Avg Td Error = -0.066 | Q = -13.145 | Reward = -12.012 | State = 450|2|50 | Steps = 165346 | Walltime = 482.310\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.437 | Episodes = 117 | Steps = 351 | Steps Per Second = 382.785\n",
            "[Learner] Action = 3.000 | Avg Td Error = 2.261 | Q = -1.227 | Reward = 1.084 | State = 420|-1|50 | Steps = 165702 | Walltime = 483.312\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.591 | Episodes = 236 | Steps = 708 | Steps Per Second = 375.173\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.171 | Q = -1.093 | Reward = -0.449 | State = 420|2|50 | Steps = 166073 | Walltime = 484.312\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.412 | Episodes = 360 | Steps = 1080 | Steps Per Second = 326.473\n",
            "[Learner] Action = -4.000 | Avg Td Error = 1.249 | Q = -17.411 | Reward = -14.853 | State = 450|2|50 | Steps = 166440 | Walltime = 485.314\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.636 | Episodes = 482 | Steps = 1446 | Steps Per Second = 307.313\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.123 | Q = -2.845 | Reward = -9.799 | State = 420|2|48 | Steps = 166793 | Walltime = 486.314\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -134.482 | Episodes = 600 | Steps = 1800 | Steps Per Second = 207.673\n",
            "[Learner] Action = 1.000 | Avg Td Error = 2.045 | Q = -0.776 | Reward = 1.269 | State = 390|0|50 | Steps = 167158 | Walltime = 487.316\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.729 | Episodes = 723 | Steps = 2169 | Steps Per Second = 198.810\n",
            "[Learner] Action = 2.000 | Avg Td Error = -8.338 | Q = -8.898 | Reward = -17.058 | State = 450|2|50 | Steps = 167518 | Walltime = 488.318\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.261 | Episodes = 844 | Steps = 2532 | Steps Per Second = 372.816\n",
            "[Learner] Action = -3.000 | Avg Td Error = -20.687 | Q = -13.349 | Reward = -33.597 | State = 450|2|50 | Steps = 167889 | Walltime = 489.320\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.755 | Episodes = 968 | Steps = 2904 | Steps Per Second = 386.596\n",
            "[Learner] Action = -1.000 | Avg Td Error = -46.651 | Q = -0.117 | Reward = -46.768 | State = 390|-3|48 | Steps = 168264 | Walltime = 490.323\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.600 | Episodes = 1093 | Steps = 3279 | Steps Per Second = 400.462\n",
            "[Learner] Action = -4.000 | Avg Td Error = -28.388 | Q = -17.252 | Reward = -45.201 | State = 450|2|50 | Steps = 168634 | Walltime = 491.323\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -80.458 | Episodes = 1217 | Steps = 3651 | Steps Per Second = 384.975\n",
            "[Learner] Action = -3.000 | Avg Td Error = -11.543 | Q = -0.217 | Reward = -11.760 | State = 390|6|45 | Steps = 168984 | Walltime = 492.327\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.854 | Episodes = 1333 | Steps = 3999 | Steps Per Second = 342.467\n",
            "[Learner] Action = 2.000 | Avg Td Error = 11.785 | Q = -8.640 | Reward = 3.137 | State = 450|2|50 | Steps = 169343 | Walltime = 493.328\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.353 | Episodes = 1453 | Steps = 4359 | Steps Per Second = 359.132\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.015 | Q = -15.974 | Reward = -23.973 | State = 450|2|50 | Steps = 169703 | Walltime = 494.330\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.291 | Episodes = 1573 | Steps = 4719 | Steps Per Second = 342.048\n",
            "[Learner] Action = -2.000 | Avg Td Error = 3.061 | Q = -0.624 | Reward = 2.599 | State = 420|-2|48 | Steps = 170042 | Walltime = 495.330\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.406 | Episodes = 1686 | Steps = 5058 | Steps Per Second = 328.484\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.525 | Q = -0.748 | Reward = 5.776 | State = 390|2|51 | Steps = 170402 | Walltime = 496.332\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.535 | Episodes = 1807 | Steps = 5421 | Steps Per Second = 392.468\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.850 | Q = -0.538 | Reward = 5.388 | State = 420|1|49 | Steps = 170760 | Walltime = 497.333\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -88.453 | Episodes = 1926 | Steps = 5778 | Steps Per Second = 332.723\n",
            "[Learner] Action = 2.000 | Avg Td Error = -11.011 | Q = -0.224 | Reward = -11.231 | State = 420|6|48 | Steps = 171125 | Walltime = 498.335\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.292 | Episodes = 2049 | Steps = 6147 | Steps Per Second = 386.417\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.313 | Q = -3.094 | Reward = 3.838 | State = 450|2|50 | Steps = 171467 | Walltime = 499.337\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.970 | Episodes = 2162 | Steps = 6486 | Steps Per Second = 333.543\n",
            "[Learner] Action = 0.000 | Avg Td Error = -12.724 | Q = -0.221 | Reward = -12.945 | State = 390|-2|48 | Steps = 171804 | Walltime = 500.337\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.008 | Episodes = 2275 | Steps = 6825 | Steps Per Second = 334.830\n",
            "[Learner] Action = 4.000 | Avg Td Error = -19.461 | Q = -16.066 | Reward = -35.466 | State = 450|2|50 | Steps = 172170 | Walltime = 501.340\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.365 | Episodes = 2398 | Steps = 7194 | Steps Per Second = 371.781\n",
            "[Learner] Action = 4.000 | Avg Td Error = -2.504 | Q = -0.143 | Reward = -2.647 | State = 390|7|47 | Steps = 172534 | Walltime = 502.341\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.428 | Episodes = 2520 | Steps = 7560 | Steps Per Second = 373.402\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.954 | Q = -0.597 | Reward = -7.551 | State = 390|2|47 | Steps = 172864 | Walltime = 503.343\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.649 | Episodes = 2627 | Steps = 7881 | Steps Per Second = 249.334\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.382 | Q = -0.830 | Reward = -6.213 | State = 390|0|48 | Steps = 173193 | Walltime = 504.344\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.088 | Episodes = 2739 | Steps = 8217 | Steps Per Second = 339.144\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.992 | Q = -0.043 | Reward = 0.949 | State = 390|5|51 | Steps = 173550 | Walltime = 505.346\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.445 | Episodes = 2859 | Steps = 8577 | Steps Per Second = 391.893\n",
            "[Learner] Action = 3.000 | Avg Td Error = 7.260 | Q = -12.591 | Reward = -5.316 | State = 450|2|50 | Steps = 173904 | Walltime = 506.347\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.108 | Episodes = 2977 | Steps = 8931 | Steps Per Second = 381.058\n",
            "[Learner] Action = 2.000 | Avg Td Error = 7.437 | Q = -1.747 | Reward = 6.177 | State = 420|-1|51 | Steps = 174262 | Walltime = 507.350\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.877 | Episodes = 3096 | Steps = 9288 | Steps Per Second = 374.057\n",
            "[Learner] Action = -4.000 | Avg Td Error = -25.852 | Q = -1.269 | Reward = -27.122 | State = 390|2|51 | Steps = 174630 | Walltime = 508.351\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.550 | Episodes = 3219 | Steps = 9657 | Steps Per Second = 382.773\n",
            "[Learner] Action = 3.000 | Avg Td Error = -11.898 | Q = -1.098 | Reward = -12.949 | State = 420|3|51 | Steps = 174991 | Walltime = 509.354\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.067 | Episodes = 3338 | Steps = 10014 | Steps Per Second = 330.938\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.627 | Q = -1.403 | Reward = -7.972 | State = 420|2|49 | Steps = 175355 | Walltime = 510.354\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.481 | Episodes = 3461 | Steps = 10383 | Steps Per Second = 378.502\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.737 | Q = -0.365 | Reward = -7.146 | State = 420|3|48 | Steps = 175716 | Walltime = 511.356\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.514 | Episodes = 3582 | Steps = 10746 | Steps Per Second = 380.966\n",
            "[Learner] Action = -3.000 | Avg Td Error = -22.302 | Q = -0.808 | Reward = -23.110 | State = 390|2|49 | Steps = 176070 | Walltime = 512.358\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.506 | Episodes = 3700 | Steps = 11100 | Steps Per Second = 379.655\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.648 | Q = -13.125 | Reward = -9.439 | State = 450|2|50 | Steps = 176437 | Walltime = 513.360\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -81.147 | Episodes = 3822 | Steps = 11466 | Steps Per Second = 367.288\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.754 | Q = -0.441 | Reward = -5.195 | State = 390|3|48 | Steps = 176796 | Walltime = 514.363\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -66.160 | Episodes = 3942 | Steps = 11826 | Steps Per Second = 365.082\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.337 | Q = -0.278 | Reward = -3.615 | State = 390|6|49 | Steps = 177166 | Walltime = 515.364\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.098 | Episodes = 4066 | Steps = 12198 | Steps Per Second = 377.808\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.524 | Q = -3.240 | Reward = 4.678 | State = 450|2|50 | Steps = 177533 | Walltime = 516.365\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.294 | Episodes = 4188 | Steps = 12564 | Steps Per Second = 373.491\n",
            "[Learner] Action = -1.000 | Avg Td Error = -28.473 | Q = -0.108 | Reward = -28.581 | State = 390|-2|55 | Steps = 177896 | Walltime = 517.366\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.049 | Episodes = 4309 | Steps = 12927 | Steps Per Second = 377.979\n",
            "[Learner] Action = 3.000 | Avg Td Error = -115.610 | Q = -1.730 | Reward = -117.301 | State = 420|-2|52 | Steps = 178254 | Walltime = 518.366\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.420 | Episodes = 4429 | Steps = 13287 | Steps Per Second = 351.871\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.673 | Q = -0.474 | Reward = 0.199 | State = 390|1|53 | Steps = 178624 | Walltime = 519.366\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -58.317 | Episodes = 4553 | Steps = 13659 | Steps Per Second = 388.385\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.164 | Q = -9.330 | Reward = -13.832 | State = 450|2|50 | Steps = 178995 | Walltime = 520.367\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.402 | Episodes = 4677 | Steps = 14031 | Steps Per Second = 370.347\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.612 | Q = -1.385 | Reward = -5.190 | State = 420|0|51 | Steps = 179362 | Walltime = 521.369\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.984 | Episodes = 4800 | Steps = 14400 | Steps Per Second = 376.205\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.126 | Q = -13.117 | Reward = -6.957 | State = 450|2|50 | Steps = 179719 | Walltime = 522.369\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.368 | Episodes = 4918 | Steps = 14754 | Steps Per Second = 333.986\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.326 | Episodes = 5155 | Steps = 15465 | Steps Per Second = 1775.242\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.813 | Episodes = 5763 | Steps = 17289 | Steps Per Second = 1775.743\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.900 | Q = -0.592 | Reward = -3.492 | State = 390|2|52 | Steps = 180000 | Walltime = 524.894\n",
            "Check Point 12\n",
            "[Learner] Action = -1.000 | Avg Td Error = -9.166 | Q = -0.384 | Reward = -8.956 | State = 420|3|50 | Steps = 180337 | Walltime = 525.895\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.419 | Episodes = 114 | Steps = 342 | Steps Per Second = 377.956\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.098 | Q = -0.010 | Reward = 1.101 | State = 420|5|49 | Steps = 180695 | Walltime = 526.897\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.366 | Episodes = 234 | Steps = 702 | Steps Per Second = 379.220\n",
            "[Learner] Action = -4.000 | Avg Td Error = -2.481 | Q = -1.016 | Reward = -3.497 | State = 390|3|50 | Steps = 181068 | Walltime = 527.898\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.003 | Episodes = 359 | Steps = 1077 | Steps Per Second = 395.726\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.680 | Q = -2.044 | Reward = 3.221 | State = 420|2|52 | Steps = 181419 | Walltime = 528.900\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.096 | Episodes = 476 | Steps = 1428 | Steps Per Second = 378.036\n",
            "[Learner] Action = 1.000 | Avg Td Error = -41.592 | Q = -5.741 | Reward = -47.315 | State = 450|2|50 | Steps = 181764 | Walltime = 529.906\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.722 | Episodes = 591 | Steps = 1773 | Steps Per Second = 386.548\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.545 | Q = -0.501 | Reward = -7.046 | State = 390|0|48 | Steps = 182108 | Walltime = 530.906\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.382 | Episodes = 706 | Steps = 2118 | Steps Per Second = 334.083\n",
            "[Learner] Action = 2.000 | Avg Td Error = 4.920 | Q = -0.079 | Reward = 4.841 | State = 390|8|46 | Steps = 182459 | Walltime = 531.907\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.426 | Episodes = 823 | Steps = 2469 | Steps Per Second = 391.906\n",
            "[Learner] Action = 4.000 | Avg Td Error = -15.295 | Q = -0.216 | Reward = -15.511 | State = 390|1|45 | Steps = 182832 | Walltime = 532.908\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -52.026 | Episodes = 947 | Steps = 2841 | Steps Per Second = 387.930\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.716 | Q = -0.534 | Reward = 6.182 | State = 390|2|53 | Steps = 183199 | Walltime = 533.910\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.403 | Episodes = 1070 | Steps = 3210 | Steps Per Second = 326.143\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.469 | Q = -3.574 | Reward = 2.488 | State = 450|2|50 | Steps = 183566 | Walltime = 534.911\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.473 | Episodes = 1193 | Steps = 3579 | Steps Per Second = 207.461\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.333 | Q = -3.745 | Reward = 5.704 | State = 450|2|50 | Steps = 183913 | Walltime = 535.914\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.636 | Episodes = 1310 | Steps = 3930 | Steps Per Second = 179.341\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.285 | Q = -0.102 | Reward = 0.183 | State = 390|5|47 | Steps = 184262 | Walltime = 536.915\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.948 | Episodes = 1428 | Steps = 4284 | Steps Per Second = 239.917\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.065 | Q = -0.208 | Reward = -3.273 | State = 390|-3|48 | Steps = 184617 | Walltime = 537.916\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.600 | Episodes = 1547 | Steps = 4641 | Steps Per Second = 388.962\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.104 | Q = -5.596 | Reward = 3.589 | State = 450|2|50 | Steps = 184989 | Walltime = 538.918\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.288 | Episodes = 1670 | Steps = 5010 | Steps Per Second = 341.955\n",
            "[Learner] Action = -4.000 | Avg Td Error = -15.006 | Q = -1.305 | Reward = -16.311 | State = 390|3|51 | Steps = 185342 | Walltime = 539.918\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.737 | Episodes = 1788 | Steps = 5364 | Steps Per Second = 333.543\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.400 | Q = -8.267 | Reward = 2.128 | State = 450|2|50 | Steps = 185706 | Walltime = 540.918\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.305 | Episodes = 1911 | Steps = 5733 | Steps Per Second = 386.334\n",
            "[Learner] Action = 2.000 | Avg Td Error = 8.572 | Q = -8.316 | Reward = 0.385 | State = 450|2|50 | Steps = 186079 | Walltime = 541.919\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.166 | Episodes = 2036 | Steps = 6108 | Steps Per Second = 380.574\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.127 | Q = -0.122 | Reward = 4.462 | State = 420|-1|56 | Steps = 186451 | Walltime = 542.920\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.327 | Episodes = 2160 | Steps = 6480 | Steps Per Second = 382.099\n",
            "[Learner] Action = -4.000 | Avg Td Error = -3.894 | Q = -4.554 | Reward = -7.659 | State = 420|2|51 | Steps = 186802 | Walltime = 543.922\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.562 | Episodes = 2276 | Steps = 6828 | Steps Per Second = 233.883\n",
            "[Learner] Action = 3.000 | Avg Td Error = -13.778 | Q = -0.472 | Reward = -13.968 | State = 420|-2|47 | Steps = 187159 | Walltime = 544.924\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.841 | Episodes = 2397 | Steps = 7191 | Steps Per Second = 346.742\n",
            "[Learner] Action = -4.000 | Avg Td Error = 6.259 | Q = -16.226 | Reward = -8.881 | State = 450|2|50 | Steps = 187519 | Walltime = 545.926\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.105 | Episodes = 2516 | Steps = 7548 | Steps Per Second = 339.491\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.361 | Q = -0.375 | Reward = -5.736 | State = 390|7|49 | Steps = 187865 | Walltime = 546.927\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.143 | Episodes = 2633 | Steps = 7899 | Steps Per Second = 395.963\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.283 | Q = -1.172 | Reward = -9.264 | State = 420|-2|48 | Steps = 188234 | Walltime = 547.929\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.136 | Episodes = 2756 | Steps = 8268 | Steps Per Second = 383.041\n",
            "[Learner] Action = 1.000 | Avg Td Error = -85.467 | Q = -1.085 | Reward = -86.501 | State = 420|-2|52 | Steps = 188596 | Walltime = 548.931\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -55.499 | Episodes = 2876 | Steps = 8628 | Steps Per Second = 377.945\n",
            "[Learner] Action = -1.000 | Avg Td Error = 11.039 | Q = -5.515 | Reward = 5.588 | State = 450|2|50 | Steps = 188945 | Walltime = 549.932\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.613 | Episodes = 2992 | Steps = 8976 | Steps Per Second = 337.244\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.378 | Q = -1.056 | Reward = 4.687 | State = 420|2|53 | Steps = 189305 | Walltime = 550.933\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.715 | Episodes = 3112 | Steps = 9336 | Steps Per Second = 336.163\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.664 | Q = -0.009 | Reward = 0.655 | State = 390|6|51 | Steps = 189658 | Walltime = 551.934\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.928 | Episodes = 3231 | Steps = 9693 | Steps Per Second = 376.599\n",
            "[Learner] Action = -2.000 | Avg Td Error = -26.788 | Q = -0.093 | Reward = -26.881 | State = 390|5|46 | Steps = 190024 | Walltime = 552.935\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.073 | Episodes = 3353 | Steps = 10059 | Steps Per Second = 338.150\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.316 | Q = -0.296 | Reward = -8.122 | State = 420|1|52 | Steps = 190367 | Walltime = 553.937\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.827 | Episodes = 3468 | Steps = 10404 | Steps Per Second = 365.326\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.109 | Q = -0.342 | Reward = -3.406 | State = 420|1|48 | Steps = 190722 | Walltime = 554.939\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.549 | Episodes = 3587 | Steps = 10761 | Steps Per Second = 395.478\n",
            "[Learner] Action = -2.000 | Avg Td Error = -18.133 | Q = -1.068 | Reward = -18.601 | State = 420|1|51 | Steps = 191095 | Walltime = 555.940\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.302 | Episodes = 3711 | Steps = 11133 | Steps Per Second = 378.604\n",
            "[Learner] Action = 0.000 | Avg Td Error = -11.373 | Q = -0.495 | Reward = -11.221 | State = 420|2|48 | Steps = 191466 | Walltime = 556.940\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.678 | Episodes = 3836 | Steps = 11508 | Steps Per Second = 385.896\n",
            "[Learner] Action = -4.000 | Avg Td Error = 9.421 | Q = -16.350 | Reward = -5.814 | State = 450|2|50 | Steps = 191829 | Walltime = 557.942\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -75.650 | Episodes = 3955 | Steps = 11865 | Steps Per Second = 313.476\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.993 | Q = -0.150 | Reward = 2.843 | State = 390|6|48 | Steps = 192171 | Walltime = 558.943\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -88.050 | Episodes = 4071 | Steps = 12213 | Steps Per Second = 398.799\n",
            "[Learner] Action = -2.000 | Avg Td Error = -7.359 | Q = -9.135 | Reward = -15.772 | State = 450|2|50 | Steps = 192541 | Walltime = 559.945\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.472 | Episodes = 4195 | Steps = 12585 | Steps Per Second = 394.523\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.318 | Q = -0.677 | Reward = -7.699 | State = 420|4|48 | Steps = 192913 | Walltime = 560.947\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.174 | Episodes = 4319 | Steps = 12957 | Steps Per Second = 360.201\n",
            "[Learner] Action = 0.000 | Avg Td Error = -82.511 | Q = -1.147 | Reward = -83.494 | State = 420|-2|51 | Steps = 193266 | Walltime = 561.949\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.266 | Episodes = 4437 | Steps = 13311 | Steps Per Second = 378.342\n",
            "[Learner] Action = -3.000 | Avg Td Error = -27.667 | Q = -0.246 | Reward = -27.844 | State = 420|3|54 | Steps = 193605 | Walltime = 562.951\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.890 | Episodes = 4551 | Steps = 13653 | Steps Per Second = 381.243\n",
            "[Learner] Action = 2.000 | Avg Td Error = -31.057 | Q = -8.634 | Reward = -39.590 | State = 450|2|50 | Steps = 193973 | Walltime = 563.953\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.847 | Episodes = 4673 | Steps = 14019 | Steps Per Second = 383.146\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.932 | Q = -0.497 | Reward = -7.429 | State = 390|-4|51 | Steps = 194334 | Walltime = 564.955\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.407 | Episodes = 4794 | Steps = 14382 | Steps Per Second = 378.923\n",
            "[Learner] Action = 4.000 | Avg Td Error = -3.862 | Q = -16.596 | Reward = -20.420 | State = 450|2|50 | Steps = 194702 | Walltime = 565.956\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.068 | Episodes = 4917 | Steps = 14751 | Steps Per Second = 395.627\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.809 | Episodes = 5067 | Steps = 15201 | Steps Per Second = 1254.403\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.454 | Episodes = 5476 | Steps = 16428 | Steps Per Second = 1280.182\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.389 | Episodes = 5879 | Steps = 17637 | Steps Per Second = 1041.546\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.356 | Q = -3.699 | Reward = -2.565 | State = 450|2|50 | Steps = 195000 | Walltime = 569.472\n",
            "Check Point 13\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.597 | Q = -0.246 | Reward = 4.333 | State = 420|3|50 | Steps = 195296 | Walltime = 570.473\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.308 | Episodes = 100 | Steps = 300 | Steps Per Second = 290.196\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.821 | Q = -3.866 | Reward = 6.072 | State = 450|2|50 | Steps = 195663 | Walltime = 571.475\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.585 | Episodes = 222 | Steps = 666 | Steps Per Second = 193.384\n",
            "[Learner] Action = -4.000 | Avg Td Error = -164.660 | Q = -16.502 | Reward = -181.162 | State = 450|2|50 | Steps = 196037 | Walltime = 572.476\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.325 | Episodes = 348 | Steps = 1044 | Steps Per Second = 193.081\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.892 | Q = -3.800 | Reward = -7.609 | State = 450|2|50 | Steps = 196403 | Walltime = 573.477\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.635 | Episodes = 471 | Steps = 1413 | Steps Per Second = 207.639\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.693 | Q = -3.731 | Reward = -0.484 | State = 450|2|50 | Steps = 196766 | Walltime = 574.479\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -110.931 | Episodes = 593 | Steps = 1779 | Steps Per Second = 383.508\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.572 | Q = -0.346 | Reward = -0.918 | State = 420|6|50 | Steps = 197140 | Walltime = 575.480\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.712 | Episodes = 718 | Steps = 2154 | Steps Per Second = 369.716\n",
            "[Learner] Action = 0.000 | Avg Td Error = -10.170 | Q = -1.501 | Reward = -11.067 | State = 420|2|51 | Steps = 197505 | Walltime = 576.482\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.055 | Episodes = 840 | Steps = 2520 | Steps Per Second = 376.960\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.931 | Q = -1.503 | Reward = 1.123 | State = 420|2|51 | Steps = 197876 | Walltime = 577.484\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.631 | Episodes = 964 | Steps = 2892 | Steps Per Second = 326.803\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.242 | Q = 0.004 | Reward = 6.246 | State = 390|-1|45 | Steps = 198237 | Walltime = 578.485\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.580 | Episodes = 1085 | Steps = 3255 | Steps Per Second = 371.649\n",
            "[Learner] Action = -4.000 | Avg Td Error = 5.241 | Q = -16.943 | Reward = -10.245 | State = 450|2|50 | Steps = 198580 | Walltime = 579.488\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.920 | Episodes = 1199 | Steps = 3597 | Steps Per Second = 331.365\n",
            "[Learner] Action = 0.000 | Avg Td Error = -16.234 | Q = -3.598 | Reward = -18.815 | State = 450|2|50 | Steps = 198941 | Walltime = 580.488\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.943 | Episodes = 1321 | Steps = 3963 | Steps Per Second = 380.378\n",
            "[Learner] Action = -3.000 | Avg Td Error = -0.172 | Q = -12.494 | Reward = -11.288 | State = 450|2|50 | Steps = 199300 | Walltime = 581.489\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.723 | Episodes = 1441 | Steps = 4323 | Steps Per Second = 376.813\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.298 | Q = -0.000 | Reward = 0.414 | State = 420|0|47 | Steps = 199639 | Walltime = 582.492\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.689 | Episodes = 1554 | Steps = 4662 | Steps Per Second = 375.789\n",
            "[Learner] Action = 4.000 | Avg Td Error = -1.648 | Q = -0.978 | Reward = -2.626 | State = 390|1|50 | Steps = 200011 | Walltime = 583.494\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.978 | Episodes = 1678 | Steps = 5034 | Steps Per Second = 372.529\n",
            "[Learner] Action = 3.000 | Avg Td Error = 14.657 | Q = -12.323 | Reward = 2.383 | State = 450|2|50 | Steps = 200383 | Walltime = 584.495\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.207 | Episodes = 1802 | Steps = 5406 | Steps Per Second = 365.804\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.433 | Q = -2.340 | Reward = -8.524 | State = 420|2|51 | Steps = 200755 | Walltime = 585.498\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.652 | Episodes = 1926 | Steps = 5778 | Steps Per Second = 383.298\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.797 | Q = -1.580 | Reward = -10.287 | State = 420|4|49 | Steps = 201122 | Walltime = 586.498\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -138.886 | Episodes = 2049 | Steps = 6147 | Steps Per Second = 390.156\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.261 | Q = -0.942 | Reward = 4.306 | State = 420|0|50 | Steps = 201468 | Walltime = 587.499\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.670 | Episodes = 2165 | Steps = 6495 | Steps Per Second = 373.968\n",
            "[Learner] Action = -4.000 | Avg Td Error = 2.531 | Q = -17.239 | Reward = -13.259 | State = 450|2|50 | Steps = 201838 | Walltime = 588.499\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -64.930 | Episodes = 2288 | Steps = 6864 | Steps Per Second = 384.458\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.815 | Q = -5.156 | Reward = -5.429 | State = 450|2|50 | Steps = 202213 | Walltime = 589.500\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.545 | Episodes = 2414 | Steps = 7242 | Steps Per Second = 370.238\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.762 | Q = -0.462 | Reward = -6.224 | State = 390|6|45 | Steps = 202581 | Walltime = 590.500\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.828 | Episodes = 2537 | Steps = 7611 | Steps Per Second = 390.483\n",
            "[Learner] Action = -4.000 | Avg Td Error = 2.842 | Q = -1.262 | Reward = 1.786 | State = 420|0|53 | Steps = 202957 | Walltime = 591.502\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.007 | Episodes = 2663 | Steps = 7989 | Steps Per Second = 392.983\n",
            "[Learner] Action = 2.000 | Avg Td Error = -8.214 | Q = -1.315 | Reward = -9.452 | State = 420|2|48 | Steps = 203335 | Walltime = 592.504\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.189 | Episodes = 2788 | Steps = 8364 | Steps Per Second = 371.824\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.250 | Q = -3.335 | Reward = -8.449 | State = 420|2|49 | Steps = 203698 | Walltime = 593.505\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.083 | Episodes = 2910 | Steps = 8730 | Steps Per Second = 356.598\n",
            "[Learner] Action = 2.000 | Avg Td Error = 12.480 | Q = -8.486 | Reward = 4.000 | State = 450|2|50 | Steps = 204038 | Walltime = 594.506\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.004 | Episodes = 3024 | Steps = 9072 | Steps Per Second = 383.789\n",
            "[Learner] Action = -3.000 | Avg Td Error = 4.967 | Q = -1.139 | Reward = 4.183 | State = 420|1|51 | Steps = 204415 | Walltime = 595.508\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.613 | Episodes = 3150 | Steps = 9450 | Steps Per Second = 380.413\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.238 | Q = -5.257 | Reward = -2.135 | State = 450|2|50 | Steps = 204784 | Walltime = 596.508\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.116 | Episodes = 3273 | Steps = 9819 | Steps Per Second = 394.127\n",
            "[Learner] Action = 0.000 | Avg Td Error = -12.799 | Q = -1.219 | Reward = -13.555 | State = 420|0|51 | Steps = 205158 | Walltime = 597.510\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.487 | Episodes = 3398 | Steps = 10194 | Steps Per Second = 372.584\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.458 | Q = -0.729 | Reward = -2.187 | State = 390|2|49 | Steps = 205517 | Walltime = 598.512\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.011 | Episodes = 3518 | Steps = 10554 | Steps Per Second = 367.911\n",
            "[Learner] Action = -2.000 | Avg Td Error = -9.772 | Q = -0.148 | Reward = -9.920 | State = 390|6|47 | Steps = 205871 | Walltime = 599.513\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.318 | Episodes = 3637 | Steps = 10911 | Steps Per Second = 349.147\n",
            "[Learner] Action = -2.000 | Avg Td Error = -26.610 | Q = -0.512 | Reward = -26.838 | State = 420|-2|47 | Steps = 206243 | Walltime = 600.514\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.550 | Episodes = 3762 | Steps = 11286 | Steps Per Second = 349.128\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.135 | Q = -0.912 | Reward = -10.047 | State = 390|6|49 | Steps = 206588 | Walltime = 601.515\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.292 | Episodes = 3877 | Steps = 11631 | Steps Per Second = 373.924\n",
            "[Learner] Action = 3.000 | Avg Td Error = 11.035 | Q = -11.964 | Reward = -0.892 | State = 450|2|50 | Steps = 206957 | Walltime = 602.515\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.907 | Episodes = 4000 | Steps = 12000 | Steps Per Second = 368.676\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.309 | Q = -1.222 | Reward = 3.474 | State = 420|-2|51 | Steps = 207320 | Walltime = 603.517\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.523 | Episodes = 4122 | Steps = 12366 | Steps Per Second = 365.910\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.980 | Q = -0.620 | Reward = 2.360 | State = 390|1|49 | Steps = 207685 | Walltime = 604.517\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.137 | Episodes = 4244 | Steps = 12732 | Steps Per Second = 357.205\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.287 | Q = -0.514 | Reward = 2.737 | State = 420|2|48 | Steps = 208052 | Walltime = 605.518\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.197 | Episodes = 4367 | Steps = 13101 | Steps Per Second = 339.062\n",
            "[Learner] Action = 3.000 | Avg Td Error = -10.588 | Q = -1.690 | Reward = -12.010 | State = 420|1|52 | Steps = 208370 | Walltime = 606.519\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -136.955 | Episodes = 4473 | Steps = 13419 | Steps Per Second = 375.968\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.826 | Q = -5.149 | Reward = 2.763 | State = 450|2|50 | Steps = 208740 | Walltime = 607.521\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.312 | Episodes = 4597 | Steps = 13791 | Steps Per Second = 330.842\n",
            "[Learner] Action = 3.000 | Avg Td Error = -30.126 | Q = -1.265 | Reward = -31.390 | State = 390|2|52 | Steps = 209106 | Walltime = 608.522\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.957 | Episodes = 4718 | Steps = 14154 | Steps Per Second = 337.298\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.022 | Q = -0.661 | Reward = 6.229 | State = 420|1|53 | Steps = 209469 | Walltime = 609.522\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.872 | Episodes = 4840 | Steps = 14520 | Steps Per Second = 359.245\n",
            "[Learner] Action = -1.000 | Avg Td Error = 8.954 | Q = -5.118 | Reward = 4.092 | State = 450|2|50 | Steps = 209824 | Walltime = 610.523\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.237 | Episodes = 4961 | Steps = 14883 | Steps Per Second = 383.041\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.891 | Episodes = 5379 | Steps = 16137 | Steps Per Second = 1584.949\n",
            "[Learner] Action = 4.000 | Avg Td Error = -23.136 | Q = -1.097 | Reward = -24.233 | State = 390|-2|48 | Steps = 210000 | Walltime = 612.674\n",
            "Check Point 14\n",
            "[Learner] Action = 1.000 | Avg Td Error = -13.543 | Q = -0.143 | Reward = -13.685 | State = 390|0|47 | Steps = 210341 | Walltime = 613.674\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.862 | Episodes = 115 | Steps = 345 | Steps Per Second = 387.107\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.352 | Q = -3.429 | Reward = -0.304 | State = 450|2|50 | Steps = 210676 | Walltime = 614.675\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.710 | Episodes = 227 | Steps = 681 | Steps Per Second = 389.311\n",
            "[Learner] Action = 0.000 | Avg Td Error = -28.220 | Q = -0.867 | Reward = -28.291 | State = 420|-1|49 | Steps = 211043 | Walltime = 615.677\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.085 | Episodes = 350 | Steps = 1050 | Steps Per Second = 363.489\n",
            "[Learner] Action = 2.000 | Avg Td Error = -11.471 | Q = -0.125 | Reward = -11.596 | State = 390|7|51 | Steps = 211399 | Walltime = 616.679\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -166.412 | Episodes = 469 | Steps = 1407 | Steps Per Second = 388.218\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.454 | Q = -0.222 | Reward = 2.232 | State = 390|-2|46 | Steps = 211772 | Walltime = 617.684\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.380 | Episodes = 592 | Steps = 1776 | Steps Per Second = 329.206\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.893 | Q = -0.262 | Reward = -8.551 | State = 420|1|48 | Steps = 212121 | Walltime = 618.686\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.895 | Episodes = 709 | Steps = 2127 | Steps Per Second = 382.937\n",
            "[Learner] Action = 3.000 | Avg Td Error = -23.652 | Q = -0.687 | Reward = -24.245 | State = 420|3|52 | Steps = 212483 | Walltime = 619.686\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.390 | Episodes = 830 | Steps = 2490 | Steps Per Second = 359.738\n",
            "[Learner] Action = 2.000 | Avg Td Error = -12.379 | Q = -0.392 | Reward = -12.771 | State = 390|4|51 | Steps = 212852 | Walltime = 620.688\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.763 | Episodes = 953 | Steps = 2859 | Steps Per Second = 385.695\n",
            "[Learner] Action = -4.000 | Avg Td Error = 14.511 | Q = -17.086 | Reward = -1.912 | State = 450|2|50 | Steps = 213213 | Walltime = 621.690\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.744 | Episodes = 1074 | Steps = 3222 | Steps Per Second = 372.386\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.686 | Q = -0.004 | Reward = 4.682 | State = 390|-3|44 | Steps = 213569 | Walltime = 622.692\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.658 | Episodes = 1193 | Steps = 3579 | Steps Per Second = 375.050\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.470 | Q = -1.389 | Reward = 1.378 | State = 420|-1|50 | Steps = 213924 | Walltime = 623.692\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.896 | Episodes = 1312 | Steps = 3936 | Steps Per Second = 367.997\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.878 | Q = -0.299 | Reward = 1.727 | State = 420|-1|47 | Steps = 214272 | Walltime = 624.694\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.070 | Episodes = 1428 | Steps = 4284 | Steps Per Second = 380.160\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.427 | Q = -3.106 | Reward = 2.120 | State = 450|2|50 | Steps = 214644 | Walltime = 625.695\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.407 | Episodes = 1552 | Steps = 4656 | Steps Per Second = 328.090\n",
            "[Learner] Action = -4.000 | Avg Td Error = 9.612 | Q = -16.822 | Reward = -6.026 | State = 450|2|50 | Steps = 214975 | Walltime = 626.696\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.532 | Episodes = 1662 | Steps = 4986 | Steps Per Second = 243.945\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.049 | Q = -0.587 | Reward = 1.034 | State = 420|2|49 | Steps = 215339 | Walltime = 627.696\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -129.360 | Episodes = 1786 | Steps = 5358 | Steps Per Second = 369.629\n",
            "[Learner] Action = 0.000 | Avg Td Error = -20.183 | Q = -3.076 | Reward = -22.498 | State = 450|2|50 | Steps = 215670 | Walltime = 628.697\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.266 | Episodes = 1896 | Steps = 5688 | Steps Per Second = 325.241\n",
            "[Learner] Action = 3.000 | Avg Td Error = -45.309 | Q = -1.667 | Reward = -46.812 | State = 420|0|51 | Steps = 215997 | Walltime = 629.700\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 11.132 | Episodes = 2005 | Steps = 6015 | Steps Per Second = 329.508\n",
            "[Learner] Action = -2.000 | Avg Td Error = 6.121 | Q = -9.250 | Reward = -2.165 | State = 450|2|50 | Steps = 216325 | Walltime = 630.701\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.418 | Episodes = 2116 | Steps = 6348 | Steps Per Second = 365.953\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.106 | Q = -1.232 | Reward = -8.833 | State = 420|4|50 | Steps = 216686 | Walltime = 631.703\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.594 | Episodes = 2237 | Steps = 6711 | Steps Per Second = 367.298\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.150 | Q = -0.114 | Reward = 0.036 | State = 390|1|46 | Steps = 217054 | Walltime = 632.704\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.183 | Episodes = 2360 | Steps = 7080 | Steps Per Second = 376.115\n",
            "[Learner] Action = 4.000 | Avg Td Error = -11.176 | Q = -0.668 | Reward = -11.845 | State = 390|4|47 | Steps = 217420 | Walltime = 633.706\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.075 | Episodes = 2482 | Steps = 7446 | Steps Per Second = 378.024\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.004 | Q = -5.275 | Reward = 2.237 | State = 450|2|50 | Steps = 217791 | Walltime = 634.706\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.326 | Episodes = 2606 | Steps = 7818 | Steps Per Second = 374.837\n",
            "[Learner] Action = 3.000 | Avg Td Error = 13.565 | Q = -12.126 | Reward = 1.487 | State = 450|2|50 | Steps = 218162 | Walltime = 635.707\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.272 | Episodes = 2729 | Steps = 8187 | Steps Per Second = 331.810\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.925 | Q = -0.188 | Reward = 3.736 | State = 390|-1|57 | Steps = 218491 | Walltime = 636.707\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.544 | Episodes = 2840 | Steps = 8520 | Steps Per Second = 371.594\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.094 | Q = -1.867 | Reward = 2.867 | State = 420|2|51 | Steps = 218856 | Walltime = 637.708\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.762 | Episodes = 2962 | Steps = 8886 | Steps Per Second = 359.635\n",
            "[Learner] Action = -4.000 | Avg Td Error = -4.525 | Q = -0.544 | Reward = -5.068 | State = 390|5|54 | Steps = 219210 | Walltime = 638.709\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.600 | Episodes = 3080 | Steps = 9240 | Steps Per Second = 359.173\n",
            "[Learner] Action = -3.000 | Avg Td Error = -3.637 | Q = -0.802 | Reward = -4.138 | State = 420|6|48 | Steps = 219585 | Walltime = 639.710\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.247 | Episodes = 3206 | Steps = 9618 | Steps Per Second = 370.107\n",
            "[Learner] Action = -3.000 | Avg Td Error = -23.568 | Q = -0.883 | Reward = -24.254 | State = 420|-1|48 | Steps = 219954 | Walltime = 640.712\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.966 | Episodes = 3329 | Steps = 9987 | Steps Per Second = 378.115\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.472 | Q = -0.110 | Reward = 2.362 | State = 390|4|50 | Steps = 220292 | Walltime = 641.714\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.587 | Episodes = 3442 | Steps = 10326 | Steps Per Second = 376.768\n",
            "[Learner] Action = 0.000 | Avg Td Error = -23.946 | Q = -0.494 | Reward = -23.693 | State = 420|1|49 | Steps = 220663 | Walltime = 642.714\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.445 | Episodes = 3566 | Steps = 10698 | Steps Per Second = 373.902\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.534 | Q = -0.449 | Reward = -3.688 | State = 420|4|49 | Steps = 221032 | Walltime = 643.716\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -124.850 | Episodes = 3689 | Steps = 11067 | Steps Per Second = 395.192\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.571 | Q = -0.328 | Reward = -1.918 | State = 420|2|46 | Steps = 221400 | Walltime = 644.718\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.517 | Episodes = 3811 | Steps = 11433 | Steps Per Second = 380.908\n",
            "[Learner] Action = 4.000 | Avg Td Error = 2.999 | Q = -16.632 | Reward = -13.523 | State = 450|2|50 | Steps = 221757 | Walltime = 645.719\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.412 | Episodes = 3931 | Steps = 11793 | Steps Per Second = 385.187\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.437 | Q = -5.248 | Reward = -0.811 | State = 450|2|50 | Steps = 222121 | Walltime = 646.720\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.180 | Episodes = 4053 | Steps = 12159 | Steps Per Second = 392.199\n",
            "[Learner] Action = -2.000 | Avg Td Error = 13.064 | Q = -9.188 | Reward = 3.960 | State = 450|2|50 | Steps = 222482 | Walltime = 647.722\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.495 | Episodes = 4174 | Steps = 12522 | Steps Per Second = 385.223\n",
            "[Learner] Action = -2.000 | Avg Td Error = -8.629 | Q = -1.200 | Reward = -8.973 | State = 420|1|51 | Steps = 222819 | Walltime = 648.723\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.859 | Episodes = 4286 | Steps = 12858 | Steps Per Second = 348.808\n",
            "[Learner] Action = -4.000 | Avg Td Error = -7.969 | Q = -0.375 | Reward = -7.700 | State = 420|5|46 | Steps = 223186 | Walltime = 649.723\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.068 | Episodes = 4409 | Steps = 13227 | Steps Per Second = 378.479\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.728 | Q = -1.882 | Reward = 0.147 | State = 420|-1|51 | Steps = 223551 | Walltime = 650.723\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.875 | Episodes = 4530 | Steps = 13590 | Steps Per Second = 330.460\n",
            "[Learner] Action = -3.000 | Avg Td Error = -21.977 | Q = -1.199 | Reward = -23.175 | State = 390|-2|49 | Steps = 223915 | Walltime = 651.726\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.392 | Episodes = 4652 | Steps = 13956 | Steps Per Second = 364.976\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.997 | Q = -0.996 | Reward = 3.741 | State = 420|1|50 | Steps = 224280 | Walltime = 652.727\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.584 | Episodes = 4772 | Steps = 14316 | Steps Per Second = 329.430\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.863 | Q = -0.558 | Reward = -7.420 | State = 390|-1|47 | Steps = 224597 | Walltime = 653.728\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.012 | Episodes = 4879 | Steps = 14637 | Steps Per Second = 375.273\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.031 | Q = -3.661 | Reward = -10.635 | State = 420|2|49 | Steps = 224949 | Walltime = 654.730\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.317 | Episodes = 4996 | Steps = 14988 | Steps Per Second = 374.135\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.427 | Episodes = 5594 | Steps = 16782 | Steps Per Second = 1931.079\n",
            "[Learner] Action = 3.000 | Avg Td Error = 1.052 | Q = -0.709 | Reward = 0.343 | State = 390|0|47 | Steps = 225000 | Walltime = 656.491\n",
            "Check Point 15\n",
            "[Learner] Action = -1.000 | Avg Td Error = -49.631 | Q = -0.337 | Reward = -49.968 | State = 390|-6|51 | Steps = 225306 | Walltime = 657.492\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.924 | Episodes = 104 | Steps = 312 | Steps Per Second = 334.661\n",
            "[Learner] Action = 0.000 | Avg Td Error = -11.186 | Q = -0.840 | Reward = -12.025 | State = 390|-1|52 | Steps = 225639 | Walltime = 658.493\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.103 | Episodes = 216 | Steps = 648 | Steps Per Second = 348.461\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.206 | Q = -0.086 | Reward = 1.120 | State = 390|2|46 | Steps = 225973 | Walltime = 659.494\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.664 | Episodes = 328 | Steps = 984 | Steps Per Second = 329.862\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.843 | Q = -12.720 | Reward = -12.889 | State = 450|2|50 | Steps = 226342 | Walltime = 660.495\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.970 | Episodes = 452 | Steps = 1356 | Steps Per Second = 367.470\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.938 | Q = -0.767 | Reward = -2.705 | State = 390|-1|52 | Steps = 226709 | Walltime = 661.497\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.620 | Episodes = 574 | Steps = 1722 | Steps Per Second = 370.969\n",
            "[Learner] Action = -3.000 | Avg Td Error = -48.822 | Q = -2.239 | Reward = -50.908 | State = 420|2|53 | Steps = 227076 | Walltime = 662.497\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.241 | Episodes = 697 | Steps = 2091 | Steps Per Second = 391.467\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.215 | Q = -0.276 | Reward = 1.940 | State = 390|3|49 | Steps = 227414 | Walltime = 663.498\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -76.934 | Episodes = 810 | Steps = 2430 | Steps Per Second = 335.410\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.158 | Q = -0.165 | Reward = -8.112 | State = 420|3|53 | Steps = 227779 | Walltime = 664.500\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.800 | Episodes = 933 | Steps = 2799 | Steps Per Second = 391.857\n",
            "[Learner] Action = 0.000 | Avg Td Error = -9.076 | Q = -3.990 | Reward = -10.862 | State = 450|2|50 | Steps = 228136 | Walltime = 665.500\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.778 | Episodes = 1052 | Steps = 3156 | Steps Per Second = 375.318\n",
            "[Learner] Action = 0.000 | Avg Td Error = -112.314 | Q = -0.140 | Reward = -112.454 | State = 390|-6|48 | Steps = 228490 | Walltime = 666.503\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.464 | Episodes = 1170 | Steps = 3510 | Steps Per Second = 371.090\n",
            "[Learner] Action = -4.000 | Avg Td Error = -4.989 | Q = -1.217 | Reward = -6.206 | State = 390|4|51 | Steps = 228854 | Walltime = 667.503\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.855 | Episodes = 1291 | Steps = 3873 | Steps Per Second = 358.743\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.444 | Q = -9.120 | Reward = -12.403 | State = 450|2|50 | Steps = 229185 | Walltime = 668.505\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.687 | Episodes = 1402 | Steps = 4206 | Steps Per Second = 349.516\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.538 | Q = -1.167 | Reward = 5.114 | State = 420|2|53 | Steps = 229533 | Walltime = 669.506\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.240 | Episodes = 1519 | Steps = 4557 | Steps Per Second = 383.380\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.773 | Q = -3.845 | Reward = -9.425 | State = 450|2|50 | Steps = 229872 | Walltime = 670.506\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.555 | Episodes = 1633 | Steps = 4899 | Steps Per Second = 385.494\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.368 | Q = -0.351 | Reward = -7.667 | State = 420|4|47 | Steps = 230244 | Walltime = 671.507\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -99.046 | Episodes = 1758 | Steps = 5274 | Steps Per Second = 380.758\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.647 | Q = -11.874 | Reward = -17.426 | State = 450|2|50 | Steps = 230584 | Walltime = 672.507\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.918 | Episodes = 1872 | Steps = 5616 | Steps Per Second = 369.239\n",
            "[Learner] Action = -1.000 | Avg Td Error = -4.158 | Q = -5.222 | Reward = -8.974 | State = 450|2|50 | Steps = 230958 | Walltime = 673.507\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.951 | Episodes = 1997 | Steps = 5991 | Steps Per Second = 380.286\n",
            "[Learner] Action = -1.000 | Avg Td Error = -19.233 | Q = -2.329 | Reward = -21.284 | State = 420|2|52 | Steps = 231335 | Walltime = 674.510\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.493 | Episodes = 2123 | Steps = 6369 | Steps Per Second = 398.559\n",
            "[Learner] Action = -2.000 | Avg Td Error = -25.698 | Q = -8.872 | Reward = -34.279 | State = 450|2|50 | Steps = 231682 | Walltime = 675.511\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.383 | Episodes = 2239 | Steps = 6717 | Steps Per Second = 393.622\n",
            "[Learner] Action = 1.000 | Avg Td Error = -28.916 | Q = -0.039 | Reward = -28.934 | State = 420|1|56 | Steps = 232056 | Walltime = 676.512\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.536 | Episodes = 2364 | Steps = 7092 | Steps Per Second = 386.240\n",
            "[Learner] Action = -1.000 | Avg Td Error = -7.754 | Q = -5.338 | Reward = -12.674 | State = 450|2|50 | Steps = 232401 | Walltime = 677.514\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.634 | Episodes = 2479 | Steps = 7437 | Steps Per Second = 388.998\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.256 | Q = -0.581 | Reward = 5.675 | State = 390|-2|48 | Steps = 232780 | Walltime = 678.517\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.994 | Episodes = 2605 | Steps = 7815 | Steps Per Second = 374.413\n",
            "[Learner] Action = -3.000 | Avg Td Error = 11.922 | Q = -12.944 | Reward = -0.200 | State = 450|2|50 | Steps = 233151 | Walltime = 679.519\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -53.623 | Episodes = 2729 | Steps = 8187 | Steps Per Second = 377.118\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.294 | Q = -5.681 | Reward = 1.983 | State = 450|2|50 | Steps = 233517 | Walltime = 680.521\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.663 | Episodes = 2851 | Steps = 8553 | Steps Per Second = 379.243\n",
            "[Learner] Action = -4.000 | Avg Td Error = 4.678 | Q = -16.557 | Reward = -10.221 | State = 450|2|50 | Steps = 233892 | Walltime = 681.521\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.338 | Episodes = 2977 | Steps = 8931 | Steps Per Second = 381.219\n",
            "[Learner] Action = 2.000 | Avg Td Error = -20.152 | Q = -0.825 | Reward = -20.977 | State = 390|0|51 | Steps = 234264 | Walltime = 682.523\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.588 | Episodes = 3101 | Steps = 9303 | Steps Per Second = 333.552\n",
            "[Learner] Action = 1.000 | Avg Td Error = -5.408 | Q = -0.112 | Reward = -5.520 | State = 390|1|46 | Steps = 234634 | Walltime = 683.525\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.233 | Episodes = 3225 | Steps = 9675 | Steps Per Second = 373.657\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.119 | Q = -8.381 | Reward = -12.393 | State = 450|2|50 | Steps = 234997 | Walltime = 684.531\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.494 | Episodes = 3346 | Steps = 10038 | Steps Per Second = 379.896\n",
            "[Learner] Action = 4.000 | Avg Td Error = 0.064 | Q = -0.906 | Reward = -0.796 | State = 420|1|48 | Steps = 235364 | Walltime = 685.533\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.766 | Episodes = 3468 | Steps = 10404 | Steps Per Second = 308.965\n",
            "[Learner] Action = -3.000 | Avg Td Error = 4.996 | Q = -1.261 | Reward = 3.735 | State = 390|2|52 | Steps = 235738 | Walltime = 686.535\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.976 | Episodes = 3593 | Steps = 10779 | Steps Per Second = 394.152\n",
            "[Learner] Action = 3.000 | Avg Td Error = -0.820 | Q = -11.906 | Reward = -12.624 | State = 450|2|50 | Steps = 236110 | Walltime = 687.535\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.874 | Episodes = 3715 | Steps = 11145 | Steps Per Second = 373.026\n",
            "[Learner] Action = 3.000 | Avg Td Error = 0.270 | Q = -2.422 | Reward = -2.106 | State = 420|2|48 | Steps = 236474 | Walltime = 688.537\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -123.092 | Episodes = 3839 | Steps = 11517 | Steps Per Second = 363.500\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.816 | Q = -0.850 | Reward = 2.966 | State = 390|-1|50 | Steps = 236823 | Walltime = 689.539\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.150 | Episodes = 3956 | Steps = 11868 | Steps Per Second = 377.140\n",
            "[Learner] Action = 3.000 | Avg Td Error = -8.989 | Q = -0.083 | Reward = -9.072 | State = 390|8|54 | Steps = 237197 | Walltime = 690.540\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -126.543 | Episodes = 4081 | Steps = 12243 | Steps Per Second = 376.768\n",
            "[Learner] Action = -2.000 | Avg Td Error = 7.112 | Q = -0.863 | Reward = 6.249 | State = 390|-1|54 | Steps = 237567 | Walltime = 691.540\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.504 | Episodes = 4204 | Steps = 12612 | Steps Per Second = 370.598\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.998 | Q = -12.630 | Reward = -20.189 | State = 450|2|50 | Steps = 237900 | Walltime = 692.541\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.027 | Episodes = 4316 | Steps = 12948 | Steps Per Second = 379.518\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.993 | Q = -0.517 | Reward = -7.510 | State = 390|4|48 | Steps = 238270 | Walltime = 693.543\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.853 | Episodes = 4440 | Steps = 13320 | Steps Per Second = 363.311\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.027 | Q = 0.010 | Reward = 2.037 | State = 390|3|48 | Steps = 238635 | Walltime = 694.543\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.760 | Episodes = 4562 | Steps = 13686 | Steps Per Second = 346.637\n",
            "[Learner] Action = -4.000 | Avg Td Error = -8.448 | Q = -0.529 | Reward = -8.726 | State = 420|5|47 | Steps = 239005 | Walltime = 695.543\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.572 | Episodes = 4687 | Steps = 14061 | Steps Per Second = 381.555\n",
            "[Learner] Action = -4.000 | Avg Td Error = -14.700 | Q = -16.397 | Reward = -29.739 | State = 450|2|50 | Steps = 239377 | Walltime = 696.545\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.224 | Episodes = 4811 | Steps = 14433 | Steps Per Second = 390.289\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.557 | Q = -0.864 | Reward = 0.693 | State = 390|2|50 | Steps = 239752 | Walltime = 697.546\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.217 | Episodes = 4937 | Steps = 14811 | Steps Per Second = 401.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.355 | Episodes = 5292 | Steps = 15876 | Steps Per Second = 1976.891\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.039 | Episodes = 5919 | Steps = 17757 | Steps Per Second = 1957.820\n",
            "[Learner] Action = -2.000 | Avg Td Error = -70.755 | Q = -0.529 | Reward = -71.284 | State = 390|2|54 | Steps = 240000 | Walltime = 699.833\n",
            "Check Point 16\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.993 | Q = -12.715 | Reward = -3.748 | State = 450|2|50 | Steps = 240352 | Walltime = 700.835\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.071 | Episodes = 119 | Steps = 357 | Steps Per Second = 346.962\n",
            "[Learner] Action = 4.000 | Avg Td Error = -4.587 | Q = -1.616 | Reward = -6.203 | State = 390|1|52 | Steps = 240711 | Walltime = 701.838\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.008 | Episodes = 239 | Steps = 717 | Steps Per Second = 347.441\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.956 | Q = -1.647 | Reward = -3.603 | State = 390|2|50 | Steps = 241058 | Walltime = 702.839\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.235 | Episodes = 355 | Steps = 1065 | Steps Per Second = 371.396\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.642 | Q = -0.501 | Reward = -10.143 | State = 390|6|50 | Steps = 241432 | Walltime = 703.839\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.514 | Episodes = 480 | Steps = 1440 | Steps Per Second = 332.047\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.304 | Q = -1.857 | Reward = 5.629 | State = 420|2|50 | Steps = 241770 | Walltime = 704.842\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.972 | Episodes = 592 | Steps = 1776 | Steps Per Second = 324.511\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.544 | Q = -1.065 | Reward = -6.884 | State = 420|2|49 | Steps = 242102 | Walltime = 705.843\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.858 | Episodes = 704 | Steps = 2112 | Steps Per Second = 388.878\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.609 | Q = -3.321 | Reward = -4.948 | State = 450|2|50 | Steps = 242472 | Walltime = 706.844\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.096 | Episodes = 828 | Steps = 2484 | Steps Per Second = 379.358\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.319 | Q = -0.681 | Reward = -6.332 | State = 420|-1|53 | Steps = 242846 | Walltime = 707.846\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -81.741 | Episodes = 952 | Steps = 2856 | Steps Per Second = 246.024\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.860 | Q = -3.484 | Reward = 5.115 | State = 450|2|50 | Steps = 243213 | Walltime = 708.846\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.283 | Episodes = 1076 | Steps = 3228 | Steps Per Second = 365.867\n",
            "[Learner] Action = -2.000 | Avg Td Error = 6.423 | Q = -2.304 | Reward = 4.394 | State = 420|-2|52 | Steps = 243580 | Walltime = 709.847\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.297 | Episodes = 1199 | Steps = 3597 | Steps Per Second = 377.039\n",
            "[Learner] Action = -2.000 | Avg Td Error = -10.903 | Q = -0.049 | Reward = -10.952 | State = 390|5|57 | Steps = 243954 | Walltime = 710.848\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.244 | Episodes = 1324 | Steps = 3972 | Steps Per Second = 383.508\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.751 | Q = -0.045 | Reward = -0.730 | State = 420|6|51 | Steps = 244323 | Walltime = 711.850\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.991 | Episodes = 1446 | Steps = 4338 | Steps Per Second = 338.560\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.897 | Q = -1.706 | Reward = 4.732 | State = 420|-1|51 | Steps = 244685 | Walltime = 712.850\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.998 | Episodes = 1568 | Steps = 4704 | Steps Per Second = 384.211\n",
            "[Learner] Action = -2.000 | Avg Td Error = 4.945 | Q = -1.470 | Reward = 3.943 | State = 420|1|50 | Steps = 245055 | Walltime = 713.850\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.340 | Episodes = 1692 | Steps = 5076 | Steps Per Second = 371.123\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.230 | Q = -0.325 | Reward = -0.121 | State = 420|3|48 | Steps = 245428 | Walltime = 714.851\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.195 | Episodes = 1817 | Steps = 5451 | Steps Per Second = 393.683\n",
            "[Learner] Action = 3.000 | Avg Td Error = 3.783 | Q = -2.916 | Reward = 0.919 | State = 420|2|52 | Steps = 245802 | Walltime = 715.854\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -89.692 | Episodes = 1942 | Steps = 5826 | Steps Per Second = 386.714\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.132 | Q = -0.069 | Reward = -3.201 | State = 390|0|44 | Steps = 246166 | Walltime = 716.854\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.317 | Episodes = 2064 | Steps = 6192 | Steps Per Second = 381.879\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.864 | Q = -0.478 | Reward = -3.263 | State = 420|3|49 | Steps = 246523 | Walltime = 717.854\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.578 | Episodes = 2184 | Steps = 6552 | Steps Per Second = 371.166\n",
            "[Learner] Action = 4.000 | Avg Td Error = 8.816 | Q = -16.890 | Reward = -7.910 | State = 450|2|50 | Steps = 246893 | Walltime = 718.855\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.552 | Episodes = 2308 | Steps = 6924 | Steps Per Second = 378.832\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.869 | Q = -0.534 | Reward = -6.403 | State = 390|5|49 | Steps = 247269 | Walltime = 719.856\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.943 | Episodes = 2434 | Steps = 7302 | Steps Per Second = 385.494\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.620 | Q = -1.074 | Reward = -9.694 | State = 390|1|50 | Steps = 247642 | Walltime = 720.859\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.265 | Episodes = 2558 | Steps = 7674 | Steps Per Second = 379.884\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.315 | Q = -8.176 | Reward = -9.386 | State = 450|2|50 | Steps = 248019 | Walltime = 721.861\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.684 | Episodes = 2684 | Steps = 8052 | Steps Per Second = 381.335\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.970 | Q = -0.014 | Reward = 1.007 | State = 420|5|50 | Steps = 248395 | Walltime = 722.863\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.517 | Episodes = 2809 | Steps = 8427 | Steps Per Second = 271.652\n",
            "[Learner] Action = -4.000 | Avg Td Error = -18.348 | Q = -16.564 | Reward = -33.480 | State = 450|2|50 | Steps = 248743 | Walltime = 723.865\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.688 | Episodes = 2925 | Steps = 8775 | Steps Per Second = 389.431\n",
            "[Learner] Action = 2.000 | Avg Td Error = -9.087 | Q = -1.128 | Reward = -10.215 | State = 390|-2|49 | Steps = 249098 | Walltime = 724.865\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.926 | Episodes = 3044 | Steps = 9132 | Steps Per Second = 378.126\n",
            "[Learner] Action = 3.000 | Avg Td Error = -85.703 | Q = -1.327 | Reward = -87.029 | State = 390|-1|49 | Steps = 249469 | Walltime = 725.867\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.122 | Episodes = 3167 | Steps = 9501 | Steps Per Second = 343.871\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.148 | Q = -3.838 | Reward = 0.150 | State = 450|2|50 | Steps = 249802 | Walltime = 726.868\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.391 | Episodes = 3277 | Steps = 9831 | Steps Per Second = 329.249\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.892 | Q = -2.429 | Reward = -7.425 | State = 420|2|51 | Steps = 250143 | Walltime = 727.870\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.300 | Episodes = 3391 | Steps = 10173 | Steps Per Second = 395.465\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.170 | Q = -0.354 | Reward = -4.525 | State = 390|6|51 | Steps = 250491 | Walltime = 728.871\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.177 | Episodes = 3508 | Steps = 10524 | Steps Per Second = 328.862\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.971 | Q = -0.118 | Reward = -5.000 | State = 420|6|50 | Steps = 250824 | Walltime = 729.874\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.295 | Episodes = 3619 | Steps = 10857 | Steps Per Second = 344.700\n",
            "[Learner] Action = 0.000 | Avg Td Error = -41.684 | Q = -1.890 | Reward = -42.770 | State = 420|-2|50 | Steps = 251165 | Walltime = 730.875\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.492 | Episodes = 3735 | Steps = 11205 | Steps Per Second = 374.748\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.396 | Q = -0.285 | Reward = 4.112 | State = 390|3|50 | Steps = 251532 | Walltime = 731.877\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.934 | Episodes = 3856 | Steps = 11568 | Steps Per Second = 332.890\n",
            "[Learner] Action = 4.000 | Avg Td Error = -75.147 | Q = -2.539 | Reward = -77.686 | State = 420|2|53 | Steps = 251875 | Walltime = 732.879\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -84.119 | Episodes = 3972 | Steps = 11916 | Steps Per Second = 390.859\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.105 | Q = -3.755 | Reward = 5.122 | State = 450|2|50 | Steps = 252238 | Walltime = 733.881\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.003 | Episodes = 4093 | Steps = 12279 | Steps Per Second = 373.048\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.750 | Q = -0.515 | Reward = -5.265 | State = 390|3|49 | Steps = 252598 | Walltime = 734.883\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.503 | Episodes = 4214 | Steps = 12642 | Steps Per Second = 380.034\n",
            "[Learner] Action = 2.000 | Avg Td Error = 3.058 | Q = -2.827 | Reward = 0.332 | State = 420|2|52 | Steps = 252965 | Walltime = 735.884\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.201 | Episodes = 4337 | Steps = 13011 | Steps Per Second = 395.951\n",
            "[Learner] Action = -4.000 | Avg Td Error = -1.802 | Q = -1.517 | Reward = -3.319 | State = 390|0|48 | Steps = 253328 | Walltime = 736.885\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.252 | Episodes = 4458 | Steps = 13374 | Steps Per Second = 335.240\n",
            "[Learner] Action = 4.000 | Avg Td Error = 5.937 | Q = -2.161 | Reward = 3.824 | State = 420|0|51 | Steps = 253674 | Walltime = 737.886\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.386 | Episodes = 4573 | Steps = 13719 | Steps Per Second = 397.979\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.227 | Q = -5.601 | Reward = -0.101 | State = 450|2|50 | Steps = 254029 | Walltime = 738.888\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.211 | Episodes = 4691 | Steps = 14073 | Steps Per Second = 327.271\n",
            "[Learner] Action = 4.000 | Avg Td Error = -14.307 | Q = -4.475 | Reward = -18.705 | State = 420|2|51 | Steps = 254374 | Walltime = 739.890\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.537 | Episodes = 4808 | Steps = 14424 | Steps Per Second = 376.756\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.322 | Q = -0.540 | Reward = -4.780 | State = 420|3|48 | Steps = 254743 | Walltime = 740.892\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.389 | Episodes = 4929 | Steps = 14787 | Steps Per Second = 337.814\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.721 | Episodes = 5229 | Steps = 15687 | Steps Per Second = 1745.929\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.472 | Episodes = 5812 | Steps = 17436 | Steps Per Second = 1775.743\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.180 | Q = -0.047 | Reward = -3.081 | State = 420|4|45 | Steps = 255000 | Walltime = 743.365\n",
            "Check Point 17\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.515 | Q = -0.349 | Reward = -3.864 | State = 390|4|47 | Steps = 255354 | Walltime = 744.365\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.393 | Episodes = 120 | Steps = 360 | Steps Per Second = 375.508\n",
            "[Learner] Action = 4.000 | Avg Td Error = -54.849 | Q = -2.740 | Reward = -57.530 | State = 420|2|53 | Steps = 255725 | Walltime = 745.366\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.617 | Episodes = 244 | Steps = 732 | Steps Per Second = 377.367\n",
            "[Learner] Action = 2.000 | Avg Td Error = 3.900 | Q = -8.352 | Reward = -4.239 | State = 450|2|50 | Steps = 256096 | Walltime = 746.366\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -82.929 | Episodes = 368 | Steps = 1104 | Steps Per Second = 374.280\n",
            "[Learner] Action = -2.000 | Avg Td Error = 12.759 | Q = -9.708 | Reward = 3.204 | State = 450|2|50 | Steps = 256472 | Walltime = 747.368\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.055 | Episodes = 494 | Steps = 1482 | Steps Per Second = 383.813\n",
            "[Learner] Action = 1.000 | Avg Td Error = -13.246 | Q = -0.491 | Reward = -13.737 | State = 390|3|52 | Steps = 256843 | Walltime = 748.369\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.988 | Episodes = 617 | Steps = 1851 | Steps Per Second = 376.847\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.108 | Q = -13.138 | Reward = -9.526 | State = 450|2|50 | Steps = 257191 | Walltime = 749.370\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.662 | Episodes = 735 | Steps = 2205 | Steps Per Second = 396.437\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.470 | Q = -0.851 | Reward = -7.321 | State = 390|4|49 | Steps = 257568 | Walltime = 750.373\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.116 | Episodes = 861 | Steps = 2583 | Steps Per Second = 381.104\n",
            "[Learner] Action = 0.000 | Avg Td Error = -14.475 | Q = -0.583 | Reward = -14.413 | State = 420|1|52 | Steps = 257933 | Walltime = 751.375\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.437 | Episodes = 983 | Steps = 2949 | Steps Per Second = 399.331\n",
            "[Learner] Action = -4.000 | Avg Td Error = -13.758 | Q = -0.195 | Reward = -13.953 | State = 390|7|54 | Steps = 258300 | Walltime = 752.378\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.800 | Episodes = 1105 | Steps = 3315 | Steps Per Second = 332.354\n",
            "[Learner] Action = -3.000 | Avg Td Error = -21.391 | Q = -1.043 | Reward = -21.811 | State = 420|-1|48 | Steps = 258650 | Walltime = 753.380\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -177.530 | Episodes = 1223 | Steps = 3669 | Steps Per Second = 384.117\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.093 | Q = -0.131 | Reward = 1.962 | State = 390|6|48 | Steps = 259015 | Walltime = 754.383\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.695 | Episodes = 1345 | Steps = 4035 | Steps Per Second = 389.962\n",
            "[Learner] Action = 1.000 | Avg Td Error = -7.254 | Q = -0.216 | Reward = -7.290 | State = 420|-1|46 | Steps = 259383 | Walltime = 755.385\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.092 | Episodes = 1468 | Steps = 4404 | Steps Per Second = 376.621\n",
            "[Learner] Action = 1.000 | Avg Td Error = -21.349 | Q = -0.335 | Reward = -21.684 | State = 390|-1|47 | Steps = 259750 | Walltime = 756.387\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.724 | Episodes = 1591 | Steps = 4773 | Steps Per Second = 365.580\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.052 | Q = -2.972 | Reward = 3.148 | State = 420|2|50 | Steps = 260093 | Walltime = 757.390\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.508 | Episodes = 1704 | Steps = 5112 | Steps Per Second = 322.176\n",
            "[Learner] Action = -4.000 | Avg Td Error = -48.273 | Q = -0.168 | Reward = -48.435 | State = 420|6|46 | Steps = 260422 | Walltime = 758.391\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.240 | Episodes = 1815 | Steps = 5445 | Steps Per Second = 343.852\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.009 | Q = -1.342 | Reward = -5.352 | State = 390|0|49 | Steps = 260744 | Walltime = 759.392\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.717 | Episodes = 1922 | Steps = 5766 | Steps Per Second = 324.143\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.488 | Q = -0.094 | Reward = -3.582 | State = 390|3|44 | Steps = 261058 | Walltime = 760.393\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.447 | Episodes = 2026 | Steps = 6078 | Steps Per Second = 302.401\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.597 | Q = -0.783 | Reward = -4.668 | State = 420|4|51 | Steps = 261392 | Walltime = 761.393\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.465 | Episodes = 2139 | Steps = 6417 | Steps Per Second = 364.764\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.217 | Q = -5.533 | Reward = 1.227 | State = 450|2|50 | Steps = 261704 | Walltime = 762.396\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.373 | Episodes = 2243 | Steps = 6729 | Steps Per Second = 317.895\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.056 | Q = 0.026 | Reward = 1.082 | State = 390|3|48 | Steps = 262031 | Walltime = 763.398\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -95.793 | Episodes = 2353 | Steps = 7059 | Steps Per Second = 359.101\n",
            "[Learner] Action = 0.000 | Avg Td Error = -14.391 | Q = -0.157 | Reward = -13.837 | State = 420|2|47 | Steps = 262353 | Walltime = 764.399\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.408 | Episodes = 2460 | Steps = 7380 | Steps Per Second = 345.495\n",
            "[Learner] Action = 2.000 | Avg Td Error = -26.383 | Q = -8.154 | Reward = -34.378 | State = 450|2|50 | Steps = 262662 | Walltime = 765.400\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.006 | Episodes = 2562 | Steps = 7686 | Steps Per Second = 281.138\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.134 | Q = -13.219 | Reward = -16.439 | State = 450|2|50 | Steps = 262970 | Walltime = 766.400\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.658 | Episodes = 2666 | Steps = 7998 | Steps Per Second = 289.096\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.805 | Q = -3.746 | Reward = -5.983 | State = 450|2|50 | Steps = 263321 | Walltime = 767.401\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.586 | Episodes = 2785 | Steps = 8355 | Steps Per Second = 378.525\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.587 | Q = -0.171 | Reward = 3.416 | State = 390|1|46 | Steps = 263691 | Walltime = 768.402\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.691 | Episodes = 2909 | Steps = 8727 | Steps Per Second = 370.838\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.981 | Q = -5.687 | Reward = 5.291 | State = 450|2|50 | Steps = 264058 | Walltime = 769.403\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.814 | Episodes = 3032 | Steps = 9096 | Steps Per Second = 322.069\n",
            "[Learner] Action = -2.000 | Avg Td Error = -14.082 | Q = -1.586 | Reward = -15.668 | State = 390|0|51 | Steps = 264426 | Walltime = 770.405\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -78.693 | Episodes = 3155 | Steps = 9465 | Steps Per Second = 366.806\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.186 | Q = -0.250 | Reward = -1.459 | State = 420|4|48 | Steps = 264792 | Walltime = 771.405\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.188 | Episodes = 3278 | Steps = 9834 | Steps Per Second = 385.305\n",
            "[Learner] Action = -2.000 | Avg Td Error = 5.580 | Q = -9.305 | Reward = -2.540 | State = 450|2|50 | Steps = 265162 | Walltime = 772.407\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.138 | Episodes = 3402 | Steps = 10206 | Steps Per Second = 380.482\n",
            "[Learner] Action = 2.000 | Avg Td Error = -16.204 | Q = -7.930 | Reward = -23.901 | State = 450|2|50 | Steps = 265534 | Walltime = 773.410\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -155.712 | Episodes = 3526 | Steps = 10578 | Steps Per Second = 379.369\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.916 | Q = -0.118 | Reward = -2.023 | State = 420|6|53 | Steps = 265908 | Walltime = 774.412\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.252 | Episodes = 3649 | Steps = 10947 | Steps Per Second = 380.148\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.813 | Q = -0.051 | Reward = 0.834 | State = 420|5|50 | Steps = 266250 | Walltime = 775.414\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.690 | Episodes = 3764 | Steps = 11292 | Steps Per Second = 376.678\n",
            "[Learner] Action = 0.000 | Avg Td Error = -9.603 | Q = -3.488 | Reward = -11.721 | State = 450|2|50 | Steps = 266622 | Walltime = 776.416\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.386 | Episodes = 3889 | Steps = 11667 | Steps Per Second = 396.175\n",
            "[Learner] Action = -3.000 | Avg Td Error = -0.222 | Q = -2.538 | Reward = -1.726 | State = 420|2|53 | Steps = 266992 | Walltime = 777.417\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.740 | Episodes = 4012 | Steps = 12036 | Steps Per Second = 376.317\n",
            "[Learner] Action = 2.000 | Avg Td Error = 4.878 | Q = -0.872 | Reward = 4.085 | State = 420|0|48 | Steps = 267365 | Walltime = 778.418\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.775 | Episodes = 4137 | Steps = 12411 | Steps Per Second = 383.497\n",
            "[Learner] Action = 1.000 | Avg Td Error = -3.420 | Q = -0.149 | Reward = -3.569 | State = 390|6|49 | Steps = 267729 | Walltime = 779.420\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.960 | Episodes = 4259 | Steps = 12777 | Steps Per Second = 386.679\n",
            "[Learner] Action = 3.000 | Avg Td Error = -28.235 | Q = -0.004 | Reward = -28.239 | State = 390|-6|46 | Steps = 268099 | Walltime = 780.422\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.394 | Episodes = 4383 | Steps = 13149 | Steps Per Second = 387.787\n",
            "[Learner] Action = -4.000 | Avg Td Error = -11.286 | Q = -0.045 | Reward = -11.331 | State = 390|2|43 | Steps = 268469 | Walltime = 781.424\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -102.459 | Episodes = 4506 | Steps = 13518 | Steps Per Second = 366.166\n",
            "[Learner] Action = 0.000 | Avg Td Error = -20.866 | Q = -3.560 | Reward = -23.471 | State = 450|2|50 | Steps = 268834 | Walltime = 782.426\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -89.176 | Episodes = 4628 | Steps = 13884 | Steps Per Second = 370.991\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.832 | Q = -1.834 | Reward = -1.968 | State = 420|2|50 | Steps = 269195 | Walltime = 783.426\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.402 | Episodes = 4749 | Steps = 14247 | Steps Per Second = 368.925\n",
            "[Learner] Action = -1.000 | Avg Td Error = 8.537 | Q = -5.685 | Reward = 3.186 | State = 450|2|50 | Steps = 269568 | Walltime = 784.429\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.284 | Episodes = 4874 | Steps = 14622 | Steps Per Second = 371.583\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.459 | Q = -0.047 | Reward = 1.411 | State = 390|6|55 | Steps = 269940 | Walltime = 785.430\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -52.366 | Episodes = 4998 | Steps = 14994 | Steps Per Second = 374.569\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.463 | Episodes = 5618 | Steps = 16854 | Steps Per Second = 1976.270\n",
            "[Learner] Action = -3.000 | Avg Td Error = 11.876 | Q = -13.299 | Reward = -0.507 | State = 450|2|50 | Steps = 270000 | Walltime = 787.188\n",
            "Check Point 18\n",
            "[Learner] Action = -4.000 | Avg Td Error = 8.743 | Q = -17.441 | Reward = -7.166 | State = 450|2|50 | Steps = 270346 | Walltime = 788.189\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.103 | Episodes = 117 | Steps = 351 | Steps Per Second = 357.520\n",
            "[Learner] Action = -1.000 | Avg Td Error = -6.308 | Q = -1.259 | Reward = -6.518 | State = 420|1|50 | Steps = 270691 | Walltime = 789.191\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.964 | Episodes = 232 | Steps = 696 | Steps Per Second = 331.933\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.609 | Q = -0.437 | Reward = -7.046 | State = 390|6|54 | Steps = 271041 | Walltime = 790.193\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.778 | Episodes = 349 | Steps = 1047 | Steps Per Second = 342.868\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.081 | Q = -0.523 | Reward = -7.604 | State = 390|3|47 | Steps = 271392 | Walltime = 791.195\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.833 | Episodes = 467 | Steps = 1401 | Steps Per Second = 380.436\n",
            "[Learner] Action = 2.000 | Avg Td Error = -12.863 | Q = -2.984 | Reward = -15.528 | State = 420|2|50 | Steps = 271760 | Walltime = 792.197\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.859 | Episodes = 589 | Steps = 1767 | Steps Per Second = 389.999\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.240 | Q = -3.490 | Reward = -8.171 | State = 450|2|50 | Steps = 272121 | Walltime = 793.199\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.979 | Episodes = 710 | Steps = 2130 | Steps Per Second = 357.733\n",
            "[Learner] Action = 1.000 | Avg Td Error = -14.524 | Q = -5.471 | Reward = -19.831 | State = 450|2|50 | Steps = 272488 | Walltime = 794.199\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.333 | Episodes = 833 | Steps = 2499 | Steps Per Second = 371.583\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.461 | Q = -0.060 | Reward = -1.521 | State = 390|5|48 | Steps = 272854 | Walltime = 795.200\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.295 | Episodes = 956 | Steps = 2868 | Steps Per Second = 379.472\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.802 | Q = -1.057 | Reward = 5.745 | State = 390|-2|52 | Steps = 273224 | Walltime = 796.202\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.776 | Episodes = 1079 | Steps = 3237 | Steps Per Second = 389.094\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.033 | Q = -7.915 | Reward = -13.706 | State = 450|2|50 | Steps = 273591 | Walltime = 797.204\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.883 | Episodes = 1201 | Steps = 3603 | Steps Per Second = 327.561\n",
            "[Learner] Action = 2.000 | Avg Td Error = -18.881 | Q = -0.358 | Reward = -19.239 | State = 390|5|46 | Steps = 273924 | Walltime = 798.205\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.610 | Episodes = 1313 | Steps = 3939 | Steps Per Second = 297.377\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.576 | Q = -3.603 | Reward = -0.184 | State = 450|2|50 | Steps = 274258 | Walltime = 799.207\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.407 | Episodes = 1423 | Steps = 4269 | Steps Per Second = 203.772\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.524 | Q = -0.770 | Reward = -6.294 | State = 390|4|51 | Steps = 274607 | Walltime = 800.208\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.257 | Episodes = 1542 | Steps = 4626 | Steps Per Second = 356.770\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.747 | Q = -0.787 | Reward = -6.533 | State = 390|4|51 | Steps = 274942 | Walltime = 801.210\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.028 | Episodes = 1653 | Steps = 4959 | Steps Per Second = 332.644\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.780 | Q = -0.868 | Reward = 5.913 | State = 390|2|50 | Steps = 275297 | Walltime = 802.212\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.140 | Episodes = 1772 | Steps = 5316 | Steps Per Second = 333.040\n",
            "[Learner] Action = 1.000 | Avg Td Error = -14.318 | Q = -1.417 | Reward = -14.643 | State = 420|0|51 | Steps = 275636 | Walltime = 803.215\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.016 | Episodes = 1885 | Steps = 5655 | Steps Per Second = 328.879\n",
            "[Learner] Action = 0.000 | Avg Td Error = -16.774 | Q = -0.226 | Reward = -17.000 | State = 390|1|48 | Steps = 275989 | Walltime = 804.217\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.215 | Episodes = 2004 | Steps = 6012 | Steps Per Second = 352.828\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.154 | Q = -4.309 | Reward = -0.095 | State = 420|2|50 | Steps = 276357 | Walltime = 805.218\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.333 | Episodes = 2127 | Steps = 6381 | Steps Per Second = 390.168\n",
            "[Learner] Action = -4.000 | Avg Td Error = -7.652 | Q = -1.811 | Reward = -9.463 | State = 390|2|51 | Steps = 276722 | Walltime = 806.218\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.571 | Episodes = 2248 | Steps = 6744 | Steps Per Second = 335.258\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.330 | Q = -1.132 | Reward = -7.369 | State = 420|4|53 | Steps = 277077 | Walltime = 807.219\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.841 | Episodes = 2368 | Steps = 7104 | Steps Per Second = 376.576\n",
            "[Learner] Action = 4.000 | Avg Td Error = -3.748 | Q = -16.492 | Reward = -20.199 | State = 450|2|50 | Steps = 277444 | Walltime = 808.222\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.924 | Episodes = 2491 | Steps = 7473 | Steps Per Second = 382.320\n",
            "[Learner] Action = 4.000 | Avg Td Error = -4.519 | Q = -1.318 | Reward = -5.838 | State = 390|3|52 | Steps = 277813 | Walltime = 809.224\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -74.731 | Episodes = 2614 | Steps = 7842 | Steps Per Second = 380.896\n",
            "[Learner] Action = 4.000 | Avg Td Error = 3.390 | Q = -1.565 | Reward = 1.825 | State = 390|-1|51 | Steps = 278170 | Walltime = 810.224\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.993 | Episodes = 2734 | Steps = 8202 | Steps Per Second = 379.850\n",
            "[Learner] Action = 3.000 | Avg Td Error = -3.594 | Q = -1.146 | Reward = -4.740 | State = 390|2|48 | Steps = 278542 | Walltime = 811.225\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.378 | Episodes = 2858 | Steps = 8574 | Steps Per Second = 362.986\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.764 | Q = -0.038 | Reward = -3.728 | State = 420|5|48 | Steps = 278903 | Walltime = 812.226\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.561 | Episodes = 2979 | Steps = 8937 | Steps Per Second = 315.290\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.182 | Q = -7.806 | Reward = 2.434 | State = 450|2|50 | Steps = 279248 | Walltime = 813.227\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.562 | Episodes = 3094 | Steps = 9282 | Steps Per Second = 343.354\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.040 | Q = -0.080 | Reward = 1.047 | State = 420|5|51 | Steps = 279596 | Walltime = 814.228\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.799 | Episodes = 3211 | Steps = 9633 | Steps Per Second = 389.130\n",
            "[Learner] Action = -2.000 | Avg Td Error = 11.692 | Q = -8.935 | Reward = 2.963 | State = 450|2|50 | Steps = 279964 | Walltime = 815.228\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.161 | Episodes = 3334 | Steps = 10002 | Steps Per Second = 376.227\n",
            "[Learner] Action = -4.000 | Avg Td Error = -3.015 | Q = -5.499 | Reward = -7.923 | State = 420|2|49 | Steps = 280324 | Walltime = 816.228\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.875 | Episodes = 3454 | Steps = 10362 | Steps Per Second = 365.400\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.580 | Q = -3.904 | Reward = 3.716 | State = 450|2|50 | Steps = 280694 | Walltime = 817.229\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.010 | Episodes = 3578 | Steps = 10734 | Steps Per Second = 368.266\n",
            "[Learner] Action = -3.000 | Avg Td Error = -21.147 | Q = -13.366 | Reward = -33.683 | State = 450|2|50 | Steps = 281038 | Walltime = 818.230\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.419 | Episodes = 3692 | Steps = 11076 | Steps Per Second = 358.641\n",
            "[Learner] Action = -4.000 | Avg Td Error = -68.806 | Q = -1.663 | Reward = -70.449 | State = 420|0|53 | Steps = 281380 | Walltime = 819.232\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.888 | Episodes = 3807 | Steps = 11421 | Steps Per Second = 366.070\n",
            "[Learner] Action = 2.000 | Avg Td Error = 0.564 | Q = -7.849 | Reward = -7.159 | State = 450|2|50 | Steps = 281733 | Walltime = 820.233\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.989 | Episodes = 3924 | Steps = 11772 | Steps Per Second = 370.478\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.492 | Q = -9.228 | Reward = -14.717 | State = 450|2|50 | Steps = 282075 | Walltime = 821.234\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.780 | Episodes = 4039 | Steps = 12117 | Steps Per Second = 375.564\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.965 | Q = -0.436 | Reward = -5.257 | State = 420|6|52 | Steps = 282418 | Walltime = 822.235\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.908 | Episodes = 4153 | Steps = 12459 | Steps Per Second = 371.155\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.559 | Q = -3.910 | Reward = 0.607 | State = 450|2|50 | Steps = 282761 | Walltime = 823.236\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.222 | Episodes = 4268 | Steps = 12804 | Steps Per Second = 345.760\n",
            "[Learner] Action = -4.000 | Avg Td Error = -25.709 | Q = -3.870 | Reward = -28.345 | State = 420|2|48 | Steps = 283102 | Walltime = 824.237\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.975 | Episodes = 4381 | Steps = 13143 | Steps Per Second = 341.946\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.309 | Q = -3.839 | Reward = -6.554 | State = 450|2|50 | Steps = 283454 | Walltime = 825.238\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.044 | Episodes = 4501 | Steps = 13503 | Steps Per Second = 355.610\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.508 | Q = -0.887 | Reward = 1.621 | State = 390|2|51 | Steps = 283818 | Walltime = 826.239\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -122.086 | Episodes = 4623 | Steps = 13869 | Steps Per Second = 388.421\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.909 | Q = -0.092 | Reward = 1.817 | State = 390|2|46 | Steps = 284191 | Walltime = 827.241\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.494 | Episodes = 4748 | Steps = 14244 | Steps Per Second = 385.577\n",
            "[Learner] Action = -4.000 | Avg Td Error = 14.568 | Q = -17.653 | Reward = -2.243 | State = 450|2|50 | Steps = 284561 | Walltime = 828.242\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -59.505 | Episodes = 4871 | Steps = 14613 | Steps Per Second = 383.625\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.999 | Q = -1.127 | Reward = 5.872 | State = 390|-3|51 | Steps = 284900 | Walltime = 829.244\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.686 | Episodes = 4982 | Steps = 14946 | Steps Per Second = 315.077\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.795 | Episodes = 5524 | Steps = 16572 | Steps Per Second = 1777.499\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.267 | Q = -0.145 | Reward = -2.412 | State = 390|4|47 | Steps = 285000 | Walltime = 831.157\n",
            "Check Point 19\n",
            "[Learner] Action = -4.000 | Avg Td Error = -5.693 | Q = -1.917 | Reward = -7.610 | State = 390|0|50 | Steps = 285353 | Walltime = 832.159\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.047 | Episodes = 119 | Steps = 357 | Steps Per Second = 339.491\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.489 | Q = -16.137 | Reward = -25.565 | State = 450|2|50 | Steps = 285711 | Walltime = 833.160\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.492 | Episodes = 239 | Steps = 717 | Steps Per Second = 382.576\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.945 | Q = -0.619 | Reward = -7.564 | State = 390|4|47 | Steps = 286074 | Walltime = 834.160\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -80.993 | Episodes = 360 | Steps = 1080 | Steps Per Second = 385.105\n",
            "[Learner] Action = -4.000 | Avg Td Error = 2.150 | Q = -0.844 | Reward = 1.306 | State = 390|-1|53 | Steps = 286434 | Walltime = 835.161\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.052 | Episodes = 480 | Steps = 1440 | Steps Per Second = 349.710\n",
            "[Learner] Action = -3.000 | Avg Td Error = -12.610 | Q = -1.757 | Reward = -13.805 | State = 420|2|47 | Steps = 286796 | Walltime = 836.163\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.323 | Episodes = 601 | Steps = 1803 | Steps Per Second = 382.564\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.184 | Q = -0.148 | Reward = -9.332 | State = 390|8|48 | Steps = 287157 | Walltime = 837.165\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.346 | Episodes = 722 | Steps = 2166 | Steps Per Second = 367.374\n",
            "[Learner] Action = -1.000 | Avg Td Error = -11.054 | Q = -5.495 | Reward = -15.761 | State = 450|2|50 | Steps = 287507 | Walltime = 838.167\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.156 | Episodes = 839 | Steps = 2517 | Steps Per Second = 371.813\n",
            "[Learner] Action = 1.000 | Avg Td Error = -12.898 | Q = -2.246 | Reward = -14.762 | State = 420|2|50 | Steps = 287871 | Walltime = 839.168\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.538 | Episodes = 961 | Steps = 2883 | Steps Per Second = 370.205\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.777 | Q = -0.164 | Reward = -3.941 | State = 390|0|45 | Steps = 288221 | Walltime = 840.168\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.484 | Episodes = 1078 | Steps = 3234 | Steps Per Second = 365.750\n",
            "[Learner] Action = 3.000 | Avg Td Error = 2.133 | Q = -0.682 | Reward = 1.451 | State = 390|0|48 | Steps = 288564 | Walltime = 841.171\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -83.443 | Episodes = 1192 | Steps = 3576 | Steps Per Second = 327.612\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.043 | Q = -0.010 | Reward = 1.105 | State = 420|5|49 | Steps = 288916 | Walltime = 842.173\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.082 | Episodes = 1310 | Steps = 3930 | Steps Per Second = 370.707\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.350 | Q = -3.918 | Reward = -1.085 | State = 450|2|50 | Steps = 289273 | Walltime = 843.175\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.981 | Episodes = 1429 | Steps = 4287 | Steps Per Second = 379.152\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.088 | Q = -0.790 | Reward = 0.299 | State = 390|1|49 | Steps = 289647 | Walltime = 844.178\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -77.140 | Episodes = 1554 | Steps = 4662 | Steps Per Second = 370.729\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.752 | Q = -6.357 | Reward = -14.019 | State = 420|2|50 | Steps = 289993 | Walltime = 845.179\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.555 | Episodes = 1669 | Steps = 5007 | Steps Per Second = 317.550\n",
            "[Learner] Action = -3.000 | Avg Td Error = 3.778 | Q = -13.620 | Reward = -8.239 | State = 450|2|50 | Steps = 290343 | Walltime = 846.180\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.504 | Episodes = 1787 | Steps = 5361 | Steps Per Second = 371.682\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.503 | Q = -2.511 | Reward = 4.799 | State = 420|2|51 | Steps = 290700 | Walltime = 847.183\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.556 | Episodes = 1906 | Steps = 5718 | Steps Per Second = 365.495\n",
            "[Learner] Action = 1.000 | Avg Td Error = 3.170 | Q = -1.331 | Reward = 1.839 | State = 390|2|52 | Steps = 291067 | Walltime = 848.184\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.108 | Episodes = 2029 | Steps = 6087 | Steps Per Second = 382.215\n",
            "[Learner] Action = 3.000 | Avg Td Error = -10.556 | Q = -12.117 | Reward = -22.549 | State = 450|2|50 | Steps = 291433 | Walltime = 849.186\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -184.694 | Episodes = 2151 | Steps = 6453 | Steps Per Second = 366.838\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.132 | Q = -3.735 | Reward = 5.525 | State = 450|2|50 | Steps = 291785 | Walltime = 850.188\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.901 | Episodes = 2269 | Steps = 6807 | Steps Per Second = 388.266\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.193 | Q = -0.760 | Reward = -2.953 | State = 390|3|53 | Steps = 292147 | Walltime = 851.188\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.250 | Episodes = 2390 | Steps = 7170 | Steps Per Second = 367.063\n",
            "[Learner] Action = -4.000 | Avg Td Error = 0.298 | Q = -17.700 | Reward = -15.153 | State = 450|2|50 | Steps = 292516 | Walltime = 852.189\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.730 | Episodes = 2513 | Steps = 7539 | Steps Per Second = 376.362\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.273 | Q = -0.434 | Reward = -4.707 | State = 390|5|46 | Steps = 292883 | Walltime = 853.191\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.458 | Episodes = 2636 | Steps = 7908 | Steps Per Second = 374.235\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.837 | Q = -9.682 | Reward = -7.299 | State = 450|2|50 | Steps = 293250 | Walltime = 854.194\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -152.405 | Episodes = 2758 | Steps = 8274 | Steps Per Second = 378.718\n",
            "[Learner] Action = -4.000 | Avg Td Error = -8.936 | Q = -0.541 | Reward = -9.477 | State = 390|2|45 | Steps = 293601 | Walltime = 855.194\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.202 | Episodes = 2875 | Steps = 8625 | Steps Per Second = 374.102\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.691 | Q = -1.326 | Reward = -6.017 | State = 390|2|49 | Steps = 293943 | Walltime = 856.196\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.377 | Episodes = 2988 | Steps = 8964 | Steps Per Second = 318.192\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.722 | Q = -1.959 | Reward = -7.692 | State = 420|-1|49 | Steps = 294278 | Walltime = 857.198\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.332 | Episodes = 3100 | Steps = 9300 | Steps Per Second = 317.110\n",
            "[Learner] Action = 3.000 | Avg Td Error = 2.922 | Q = -1.457 | Reward = 1.466 | State = 390|-2|50 | Steps = 294628 | Walltime = 858.201\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.773 | Episodes = 3218 | Steps = 9654 | Steps Per Second = 380.011\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.903 | Q = -0.527 | Reward = -10.430 | State = 390|6|48 | Steps = 294971 | Walltime = 859.202\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.638 | Episodes = 3331 | Steps = 9993 | Steps Per Second = 335.822\n",
            "[Learner] Action = 4.000 | Avg Td Error = -3.874 | Q = -1.005 | Reward = -4.475 | State = 420|-2|54 | Steps = 295321 | Walltime = 860.205\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.059 | Episodes = 3449 | Steps = 10347 | Steps Per Second = 355.852\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.714 | Q = -3.981 | Reward = 1.628 | State = 450|2|50 | Steps = 295689 | Walltime = 861.205\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.006 | Episodes = 3572 | Steps = 10716 | Steps Per Second = 368.417\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.379 | Q = -0.842 | Reward = -8.191 | State = 420|4|52 | Steps = 296054 | Walltime = 862.207\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.957 | Episodes = 3694 | Steps = 11082 | Steps Per Second = 380.562\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.064 | Q = -0.250 | Reward = -1.314 | State = 390|5|50 | Steps = 296420 | Walltime = 863.209\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.262 | Episodes = 3816 | Steps = 11448 | Steps Per Second = 369.054\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.469 | Q = -0.298 | Reward = 5.166 | State = 420|3|53 | Steps = 296766 | Walltime = 864.211\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.536 | Episodes = 3930 | Steps = 11790 | Steps Per Second = 326.024\n",
            "[Learner] Action = 0.000 | Avg Td Error = -12.057 | Q = -0.705 | Reward = -12.762 | State = 390|-2|48 | Steps = 297091 | Walltime = 865.212\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.008 | Episodes = 4040 | Steps = 12120 | Steps Per Second = 369.044\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.837 | Q = -3.927 | Reward = -2.283 | State = 450|2|50 | Steps = 297456 | Walltime = 866.212\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.560 | Episodes = 4162 | Steps = 12486 | Steps Per Second = 379.999\n",
            "[Learner] Action = 0.000 | Avg Td Error = -17.954 | Q = -0.229 | Reward = -18.183 | State = 390|-2|46 | Steps = 297820 | Walltime = 867.213\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.366 | Episodes = 4284 | Steps = 12852 | Steps Per Second = 373.314\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.974 | Q = 0.027 | Reward = 1.000 | State = 390|3|48 | Steps = 298185 | Walltime = 868.215\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.635 | Episodes = 4406 | Steps = 13218 | Steps Per Second = 364.363\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.030 | Q = -0.606 | Reward = -1.636 | State = 390|-3|50 | Steps = 298539 | Walltime = 869.217\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.971 | Episodes = 4524 | Steps = 13572 | Steps Per Second = 374.168\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.702 | Q = -0.291 | Reward = -4.993 | State = 390|5|47 | Steps = 298903 | Walltime = 870.219\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.772 | Episodes = 4644 | Steps = 13932 | Steps Per Second = 214.843\n",
            "[Learner] Action = 3.000 | Avg Td Error = 12.363 | Q = -11.831 | Reward = 0.596 | State = 450|2|50 | Steps = 299254 | Walltime = 871.221\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.714 | Episodes = 4763 | Steps = 14289 | Steps Per Second = 374.882\n",
            "[Learner] Action = -2.000 | Avg Td Error = -10.903 | Q = -0.192 | Reward = -10.821 | State = 420|4|54 | Steps = 299596 | Walltime = 872.224\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.162 | Episodes = 4876 | Steps = 14628 | Steps Per Second = 298.598\n",
            "[Learner] Action = -3.000 | Avg Td Error = -16.587 | Q = -3.912 | Reward = -19.553 | State = 420|2|49 | Steps = 299945 | Walltime = 873.226\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -58.960 | Episodes = 4994 | Steps = 14982 | Steps Per Second = 248.086\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.159 | Episodes = 5603 | Steps = 16809 | Steps Per Second = 1162.823\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.414 | Q = -3.518 | Reward = 4.751 | State = 450|2|50 | Steps = 300000 | Walltime = 874.987\n",
            "Check Point 20\n",
            "[Learner] Action = -2.000 | Avg Td Error = -9.983 | Q = -1.418 | Reward = -11.401 | State = 390|-1|52 | Steps = 300355 | Walltime = 875.987\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.406 | Episodes = 120 | Steps = 360 | Steps Per Second = 382.727\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.015 | Q = -5.601 | Reward = 0.935 | State = 450|2|50 | Steps = 300724 | Walltime = 876.988\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.137 | Episodes = 243 | Steps = 729 | Steps Per Second = 368.439\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.622 | Q = -0.083 | Reward = -1.705 | State = 390|1|45 | Steps = 301086 | Walltime = 877.988\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.779 | Episodes = 364 | Steps = 1092 | Steps Per Second = 375.924\n",
            "[Learner] Action = -2.000 | Avg Td Error = 7.606 | Q = -9.476 | Reward = -0.771 | State = 450|2|50 | Steps = 301453 | Walltime = 878.989\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.068 | Episodes = 487 | Steps = 1461 | Steps Per Second = 391.735\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.346 | Q = -0.482 | Reward = -2.527 | State = 420|4|50 | Steps = 301823 | Walltime = 879.990\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.352 | Episodes = 610 | Steps = 1830 | Steps Per Second = 370.020\n",
            "[Learner] Action = 1.000 | Avg Td Error = -10.843 | Q = -0.379 | Reward = -10.905 | State = 420|0|46 | Steps = 302168 | Walltime = 880.991\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.106 | Episodes = 726 | Steps = 2178 | Steps Per Second = 377.435\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.542 | Q = -0.512 | Reward = -0.814 | State = 420|-1|47 | Steps = 302535 | Walltime = 881.993\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.113 | Episodes = 849 | Steps = 2547 | Steps Per Second = 386.762\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.260 | Q = -1.248 | Reward = 6.012 | State = 390|-1|50 | Steps = 302896 | Walltime = 882.995\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.227 | Episodes = 970 | Steps = 2910 | Steps Per Second = 341.407\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.135 | Q = -0.428 | Reward = 2.707 | State = 390|-3|47 | Steps = 303247 | Walltime = 883.996\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.322 | Episodes = 1087 | Steps = 3261 | Steps Per Second = 329.646\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.453 | Q = -0.331 | Reward = -0.784 | State = 390|6|50 | Steps = 303577 | Walltime = 884.996\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.016 | Episodes = 1197 | Steps = 3591 | Steps Per Second = 316.456\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.824 | Q = -3.336 | Reward = 5.685 | State = 450|2|50 | Steps = 303923 | Walltime = 885.998\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.553 | Episodes = 1313 | Steps = 3939 | Steps Per Second = 367.127\n",
            "[Learner] Action = 4.000 | Avg Td Error = 4.057 | Q = -16.138 | Reward = -11.886 | State = 450|2|50 | Steps = 304275 | Walltime = 886.998\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.191 | Episodes = 1430 | Steps = 4290 | Steps Per Second = 319.972\n",
            "[Learner] Action = 3.000 | Avg Td Error = -12.278 | Q = -1.511 | Reward = -12.971 | State = 420|-2|53 | Steps = 304608 | Walltime = 888.000\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.293 | Episodes = 1542 | Steps = 4626 | Steps Per Second = 332.652\n",
            "[Learner] Action = -4.000 | Avg Td Error = -44.095 | Q = -1.819 | Reward = -45.673 | State = 420|4|52 | Steps = 304935 | Walltime = 889.003\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.397 | Episodes = 1651 | Steps = 4953 | Steps Per Second = 318.805\n",
            "[Learner] Action = 0.000 | Avg Td Error = -21.193 | Q = -1.928 | Reward = -22.734 | State = 420|2|50 | Steps = 305280 | Walltime = 890.003\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.015 | Episodes = 1767 | Steps = 5301 | Steps Per Second = 334.066\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.170 | Q = -3.330 | Reward = -2.000 | State = 450|2|50 | Steps = 305620 | Walltime = 891.004\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.405 | Episodes = 1881 | Steps = 5643 | Steps Per Second = 324.712\n",
            "[Learner] Action = -4.000 | Avg Td Error = -15.397 | Q = -0.268 | Reward = -15.665 | State = 390|-5|53 | Steps = 305967 | Walltime = 892.006\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.250 | Episodes = 1998 | Steps = 5994 | Steps Per Second = 371.539\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.336 | Q = -13.297 | Reward = -16.382 | State = 450|2|50 | Steps = 306330 | Walltime = 893.007\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.267 | Episodes = 2119 | Steps = 6357 | Steps Per Second = 385.530\n",
            "[Learner] Action = 2.000 | Avg Td Error = 8.260 | Q = -2.158 | Reward = 6.238 | State = 420|2|53 | Steps = 306693 | Walltime = 894.009\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.138 | Episodes = 2239 | Steps = 6717 | Steps Per Second = 330.851\n",
            "[Learner] Action = 0.000 | Avg Td Error = -19.256 | Q = -3.224 | Reward = -21.428 | State = 420|-2|50 | Steps = 307054 | Walltime = 895.012\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.505 | Episodes = 2360 | Steps = 7080 | Steps Per Second = 375.766\n",
            "[Learner] Action = 4.000 | Avg Td Error = -4.519 | Q = -2.169 | Reward = -6.650 | State = 420|3|51 | Steps = 307422 | Walltime = 896.014\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.554 | Episodes = 2483 | Steps = 7449 | Steps Per Second = 387.059\n",
            "[Learner] Action = 4.000 | Avg Td Error = -13.713 | Q = -0.998 | Reward = -14.662 | State = 420|1|48 | Steps = 307781 | Walltime = 897.017\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.426 | Episodes = 2603 | Steps = 7809 | Steps Per Second = 377.355\n",
            "[Learner] Action = -1.000 | Avg Td Error = -8.178 | Q = -0.136 | Reward = -8.314 | State = 390|7|52 | Steps = 308153 | Walltime = 898.019\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.084 | Episodes = 2727 | Steps = 8181 | Steps Per Second = 390.071\n",
            "[Learner] Action = 2.000 | Avg Td Error = 0.578 | Q = -1.185 | Reward = -0.606 | State = 390|2|52 | Steps = 308505 | Walltime = 899.019\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.933 | Episodes = 2843 | Steps = 8529 | Steps Per Second = 318.781\n",
            "[Learner] Action = -2.000 | Avg Td Error = 6.047 | Q = -9.370 | Reward = -2.224 | State = 450|2|50 | Steps = 308844 | Walltime = 900.019\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.122 | Episodes = 2958 | Steps = 8874 | Steps Per Second = 366.432\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.161 | Q = -3.051 | Reward = -0.767 | State = 450|2|50 | Steps = 309194 | Walltime = 901.020\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -66.045 | Episodes = 3075 | Steps = 9225 | Steps Per Second = 379.839\n",
            "[Learner] Action = 2.000 | Avg Td Error = 7.700 | Q = -1.562 | Reward = 6.139 | State = 390|0|52 | Steps = 309565 | Walltime = 902.022\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.492 | Episodes = 3199 | Steps = 9597 | Steps Per Second = 385.730\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.244 | Q = 0.006 | Reward = 6.250 | State = 390|6|44 | Steps = 309926 | Walltime = 903.025\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.401 | Episodes = 3319 | Steps = 9957 | Steps Per Second = 352.977\n",
            "[Learner] Action = -4.000 | Avg Td Error = -6.377 | Q = -17.715 | Reward = -22.334 | State = 450|2|50 | Steps = 310285 | Walltime = 904.025\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.047 | Episodes = 3439 | Steps = 10317 | Steps Per Second = 367.052\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.200 | Q = -1.545 | Reward = -6.720 | State = 420|4|51 | Steps = 310648 | Walltime = 905.026\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.557 | Episodes = 3561 | Steps = 10683 | Steps Per Second = 373.746\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.944 | Q = -1.699 | Reward = -11.001 | State = 420|0|52 | Steps = 310998 | Walltime = 906.027\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.408 | Episodes = 3677 | Steps = 11031 | Steps Per Second = 338.159\n",
            "[Learner] Action = 2.000 | Avg Td Error = -11.636 | Q = -1.918 | Reward = -13.554 | State = 390|-2|51 | Steps = 311325 | Walltime = 907.027\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.313 | Episodes = 3786 | Steps = 11358 | Steps Per Second = 331.277\n",
            "[Learner] Action = 4.000 | Avg Td Error = 8.672 | Q = -16.497 | Reward = -7.613 | State = 450|2|50 | Steps = 311668 | Walltime = 908.030\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.574 | Episodes = 3900 | Steps = 11700 | Steps Per Second = 137.055\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.739 | Q = -8.236 | Reward = 2.568 | State = 450|2|50 | Steps = 312013 | Walltime = 909.031\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.084 | Episodes = 4019 | Steps = 12057 | Steps Per Second = 382.727\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.769 | Q = -0.817 | Reward = -0.048 | State = 390|3|50 | Steps = 312377 | Walltime = 910.033\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.857 | Episodes = 4139 | Steps = 12417 | Steps Per Second = 345.087\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.712 | Q = -5.737 | Reward = 0.882 | State = 450|2|50 | Steps = 312708 | Walltime = 911.034\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.388 | Episodes = 4251 | Steps = 12753 | Steps Per Second = 378.547\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.382 | Q = -0.289 | Reward = -2.671 | State = 390|5|50 | Steps = 313067 | Walltime = 912.037\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.188 | Episodes = 4371 | Steps = 13113 | Steps Per Second = 361.619\n",
            "[Learner] Action = 4.000 | Avg Td Error = -1.169 | Q = -2.506 | Reward = -3.674 | State = 390|2|49 | Steps = 313424 | Walltime = 913.038\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.838 | Episodes = 4489 | Steps = 13467 | Steps Per Second = 347.921\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.355 | Q = -4.957 | Reward = -0.283 | State = 450|2|50 | Steps = 313759 | Walltime = 914.039\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.311 | Episodes = 4601 | Steps = 13803 | Steps Per Second = 328.913\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.988 | Q = -1.042 | Reward = -9.029 | State = 390|3|48 | Steps = 314103 | Walltime = 915.042\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.918 | Episodes = 4717 | Steps = 14151 | Steps Per Second = 374.793\n",
            "[Learner] Action = -1.000 | Avg Td Error = -9.121 | Q = -0.924 | Reward = -10.045 | State = 390|2|49 | Steps = 314461 | Walltime = 916.044\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -215.907 | Episodes = 4836 | Steps = 14508 | Steps Per Second = 360.304\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.775 | Q = -12.689 | Reward = -2.437 | State = 450|2|50 | Steps = 314820 | Walltime = 917.046\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.393 | Episodes = 4956 | Steps = 14868 | Steps Per Second = 363.542\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.709 | Episodes = 5387 | Steps = 16161 | Steps Per Second = 1829.709\n",
            "[Learner] Action = 2.000 | Avg Td Error = 1.343 | Q = -8.195 | Reward = -6.651 | State = 450|2|50 | Steps = 315000 | Walltime = 919.171\n",
            "Check Point 21\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.159 | Q = -2.867 | Reward = -2.502 | State = 450|2|50 | Steps = 315364 | Walltime = 920.172\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.399 | Episodes = 123 | Steps = 369 | Steps Per Second = 374.804\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.576 | Q = -0.394 | Reward = 3.182 | State = 390|3|51 | Steps = 315701 | Walltime = 921.173\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.015 | Episodes = 235 | Steps = 705 | Steps Per Second = 330.520\n",
            "[Learner] Action = -4.000 | Avg Td Error = -48.745 | Q = -0.145 | Reward = -48.890 | State = 390|-2|43 | Steps = 316023 | Walltime = 922.173\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -82.613 | Episodes = 343 | Steps = 1029 | Steps Per Second = 315.369\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.575 | Q = -0.219 | Reward = -2.794 | State = 390|4|51 | Steps = 316374 | Walltime = 923.174\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.611 | Episodes = 461 | Steps = 1383 | Steps Per Second = 353.076\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.795 | Q = -1.115 | Reward = -5.084 | State = 420|5|52 | Steps = 316730 | Walltime = 924.177\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.604 | Episodes = 579 | Steps = 1737 | Steps Per Second = 369.607\n",
            "[Learner] Action = -4.000 | Avg Td Error = 9.399 | Q = -17.866 | Reward = -6.720 | State = 450|2|50 | Steps = 317083 | Walltime = 925.177\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.642 | Episodes = 698 | Steps = 2094 | Steps Per Second = 364.638\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.663 | Q = -12.885 | Reward = -19.622 | State = 450|2|50 | Steps = 317460 | Walltime = 926.178\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.514 | Episodes = 824 | Steps = 2472 | Steps Per Second = 366.913\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.165 | Q = -0.269 | Reward = -4.434 | State = 390|3|50 | Steps = 317803 | Walltime = 927.179\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.748 | Episodes = 938 | Steps = 2814 | Steps Per Second = 313.820\n",
            "[Learner] Action = 2.000 | Avg Td Error = -7.735 | Q = -1.118 | Reward = -8.853 | State = 390|-1|49 | Steps = 318153 | Walltime = 928.179\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.978 | Episodes = 1056 | Steps = 3168 | Steps Per Second = 368.309\n",
            "[Learner] Action = 1.000 | Avg Td Error = 2.949 | Q = -0.675 | Reward = 2.274 | State = 390|-1|48 | Steps = 318505 | Walltime = 929.182\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.522 | Episodes = 1174 | Steps = 3522 | Steps Per Second = 374.614\n",
            "[Learner] Action = -4.000 | Avg Td Error = -29.594 | Q = -0.810 | Reward = -30.404 | State = 390|-4|48 | Steps = 318860 | Walltime = 930.184\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.227 | Episodes = 1292 | Steps = 3876 | Steps Per Second = 329.068\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.692 | Q = -1.640 | Reward = 5.259 | State = 420|2|53 | Steps = 319210 | Walltime = 931.185\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.589 | Episodes = 1410 | Steps = 4230 | Steps Per Second = 205.674\n",
            "[Learner] Action = -4.000 | Avg Td Error = 10.587 | Q = -18.289 | Reward = -5.817 | State = 450|2|50 | Steps = 319565 | Walltime = 932.187\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.448 | Episodes = 1529 | Steps = 4587 | Steps Per Second = 380.966\n",
            "[Learner] Action = 4.000 | Avg Td Error = -4.325 | Q = -2.213 | Reward = -6.538 | State = 390|3|51 | Steps = 319934 | Walltime = 933.188\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.153 | Episodes = 1652 | Steps = 4956 | Steps Per Second = 349.467\n",
            "[Learner] Action = -4.000 | Avg Td Error = 4.080 | Q = -1.563 | Reward = 2.517 | State = 390|-2|49 | Steps = 320297 | Walltime = 934.189\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -55.238 | Episodes = 1773 | Steps = 5319 | Steps Per Second = 379.976\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.514 | Q = -0.800 | Reward = 4.714 | State = 390|2|54 | Steps = 320662 | Walltime = 935.192\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.473 | Episodes = 1895 | Steps = 5685 | Steps Per Second = 383.696\n",
            "[Learner] Action = 1.000 | Avg Td Error = -31.350 | Q = -1.638 | Reward = -32.100 | State = 420|0|50 | Steps = 321030 | Walltime = 936.195\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.816 | Episodes = 2018 | Steps = 6054 | Steps Per Second = 368.223\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.085 | Q = -0.336 | Reward = -1.422 | State = 390|2|48 | Steps = 321390 | Walltime = 937.195\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.969 | Episodes = 2138 | Steps = 6414 | Steps Per Second = 370.882\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.297 | Q = -0.003 | Reward = 1.294 | State = 390|5|49 | Steps = 321754 | Walltime = 938.197\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.914 | Episodes = 2260 | Steps = 6780 | Steps Per Second = 360.026\n",
            "[Learner] Action = 0.000 | Avg Td Error = -12.524 | Q = -3.636 | Reward = -14.526 | State = 450|2|50 | Steps = 322118 | Walltime = 939.197\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.590 | Episodes = 2382 | Steps = 7146 | Steps Per Second = 377.990\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.109 | Q = -0.402 | Reward = -1.448 | State = 420|6|50 | Steps = 322485 | Walltime = 940.197\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.797 | Episodes = 2505 | Steps = 7515 | Steps Per Second = 377.831\n",
            "[Learner] Action = -4.000 | Avg Td Error = 2.211 | Q = -18.028 | Reward = -13.260 | State = 450|2|50 | Steps = 322855 | Walltime = 941.199\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.876 | Episodes = 2629 | Steps = 7887 | Steps Per Second = 385.908\n",
            "[Learner] Action = -1.000 | Avg Td Error = -13.061 | Q = -1.816 | Reward = -13.429 | State = 420|-1|50 | Steps = 323214 | Walltime = 942.201\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.497 | Episodes = 2749 | Steps = 8247 | Steps Per Second = 391.041\n",
            "[Learner] Action = -2.000 | Avg Td Error = 6.744 | Q = -3.679 | Reward = 3.285 | State = 420|2|50 | Steps = 323578 | Walltime = 943.206\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.928 | Episodes = 2869 | Steps = 8607 | Steps Per Second = 338.350\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.599 | Q = -1.028 | Reward = 1.571 | State = 390|2|53 | Steps = 323929 | Walltime = 944.207\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.607 | Episodes = 2986 | Steps = 8958 | Steps Per Second = 348.886\n",
            "[Learner] Action = 3.000 | Avg Td Error = -3.495 | Q = -1.893 | Reward = -5.240 | State = 420|3|50 | Steps = 324275 | Walltime = 945.209\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.496 | Episodes = 3100 | Steps = 9300 | Steps Per Second = 320.878\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.181 | Q = -0.183 | Reward = -0.223 | State = 420|6|48 | Steps = 324610 | Walltime = 946.210\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.189 | Episodes = 3213 | Steps = 9639 | Steps Per Second = 335.652\n",
            "[Learner] Action = -2.000 | Avg Td Error = -29.606 | Q = -0.836 | Reward = -30.442 | State = 390|-4|47 | Steps = 324963 | Walltime = 947.211\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.069 | Episodes = 3333 | Steps = 9999 | Steps Per Second = 393.105\n",
            "[Learner] Action = 2.000 | Avg Td Error = -7.770 | Q = -0.848 | Reward = -8.521 | State = 420|5|49 | Steps = 325330 | Walltime = 948.213\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.034 | Episodes = 3456 | Steps = 10368 | Steps Per Second = 379.415\n",
            "[Learner] Action = -2.000 | Avg Td Error = -16.556 | Q = -9.454 | Reward = -24.982 | State = 450|2|50 | Steps = 325689 | Walltime = 949.215\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.792 | Episodes = 3574 | Steps = 10722 | Steps Per Second = 326.160\n",
            "[Learner] Action = 3.000 | Avg Td Error = 6.483 | Q = -1.865 | Reward = 4.676 | State = 420|1|51 | Steps = 326051 | Walltime = 950.216\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.015 | Episodes = 3697 | Steps = 11091 | Steps Per Second = 386.299\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.696 | Q = -8.454 | Reward = -11.062 | State = 450|2|50 | Steps = 326418 | Walltime = 951.219\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.317 | Episodes = 3819 | Steps = 11457 | Steps Per Second = 357.205\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.055 | Q = -1.878 | Reward = -6.738 | State = 420|1|50 | Steps = 326758 | Walltime = 952.219\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.582 | Episodes = 3933 | Steps = 11799 | Steps Per Second = 364.268\n",
            "[Learner] Action = -2.000 | Avg Td Error = -1.625 | Q = -0.575 | Reward = -2.200 | State = 390|3|48 | Steps = 327112 | Walltime = 953.220\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.199 | Episodes = 4052 | Steps = 12156 | Steps Per Second = 371.013\n",
            "[Learner] Action = -4.000 | Avg Td Error = -3.309 | Q = -17.803 | Reward = -19.222 | State = 450|2|50 | Steps = 327472 | Walltime = 954.220\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.673 | Episodes = 4171 | Steps = 12513 | Steps Per Second = 207.181\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.071 | Q = -3.833 | Reward = 2.398 | State = 450|2|50 | Steps = 327820 | Walltime = 955.222\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.535 | Episodes = 4289 | Steps = 12867 | Steps Per Second = 255.724\n",
            "[Learner] Action = 0.000 | Avg Td Error = -35.750 | Q = -1.707 | Reward = -37.434 | State = 420|2|53 | Steps = 328180 | Walltime = 956.223\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.040 | Episodes = 4411 | Steps = 13233 | Steps Per Second = 395.602\n",
            "[Learner] Action = -2.000 | Avg Td Error = 5.312 | Q = -9.321 | Reward = -2.430 | State = 450|2|50 | Steps = 328543 | Walltime = 957.225\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.369 | Episodes = 4532 | Steps = 13596 | Steps Per Second = 381.914\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.454 | Q = -0.884 | Reward = -7.130 | State = 420|5|48 | Steps = 328914 | Walltime = 958.227\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.483 | Episodes = 4656 | Steps = 13968 | Steps Per Second = 375.105\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.029 | Q = -0.424 | Reward = 4.605 | State = 390|0|47 | Steps = 329285 | Walltime = 959.228\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.097 | Episodes = 4780 | Steps = 14340 | Steps Per Second = 374.213\n",
            "[Learner] Action = 4.000 | Avg Td Error = -22.774 | Q = -15.932 | Reward = -38.645 | State = 450|2|50 | Steps = 329640 | Walltime = 960.229\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -59.223 | Episodes = 4898 | Steps = 14694 | Steps Per Second = 364.099\n",
            "[Learner] Action = 4.000 | Avg Td Error = 0.576 | Q = -15.995 | Reward = -15.192 | State = 450|2|50 | Steps = 329990 | Walltime = 961.229\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.706 | Episodes = 5076 | Steps = 15228 | Steps Per Second = 1967.310\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.215 | Episodes = 5673 | Steps = 17019 | Steps Per Second = 2007.484\n",
            "[Learner] Action = -4.000 | Avg Td Error = -38.430 | Q = -0.957 | Reward = -39.133 | State = 420|3|53 | Steps = 330000 | Walltime = 962.933\n",
            "Check Point 22\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.768 | Q = -0.870 | Reward = 4.898 | State = 390|2|49 | Steps = 330333 | Walltime = 963.935\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.827 | Episodes = 114 | Steps = 342 | Steps Per Second = 371.517\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.135 | Q = -3.827 | Reward = -1.431 | State = 450|2|50 | Steps = 330684 | Walltime = 964.938\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.497 | Episodes = 231 | Steps = 693 | Steps Per Second = 304.214\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.230 | Q = -0.772 | Reward = -10.001 | State = 390|5|48 | Steps = 331017 | Walltime = 965.938\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.585 | Episodes = 343 | Steps = 1029 | Steps Per Second = 363.133\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.097 | Q = -2.480 | Reward = 0.119 | State = 420|0|51 | Steps = 331368 | Walltime = 966.939\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -187.474 | Episodes = 461 | Steps = 1383 | Steps Per Second = 387.321\n",
            "[Learner] Action = -2.000 | Avg Td Error = -0.618 | Q = -9.463 | Reward = -8.418 | State = 450|2|50 | Steps = 331741 | Walltime = 967.941\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -76.589 | Episodes = 585 | Steps = 1755 | Steps Per Second = 352.423\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.709 | Q = -0.241 | Reward = 2.614 | State = 420|5|53 | Steps = 332111 | Walltime = 968.943\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.804 | Episodes = 710 | Steps = 2130 | Steps Per Second = 372.397\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.224 | Q = -5.589 | Reward = 1.600 | State = 450|2|50 | Steps = 332456 | Walltime = 969.945\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.032 | Episodes = 825 | Steps = 2475 | Steps Per Second = 396.100\n",
            "[Learner] Action = 3.000 | Avg Td Error = -76.013 | Q = -2.396 | Reward = -78.408 | State = 390|2|51 | Steps = 332828 | Walltime = 970.947\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.117 | Episodes = 949 | Steps = 2847 | Steps Per Second = 385.494\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.889 | Q = -2.439 | Reward = -8.328 | State = 390|2|50 | Steps = 333177 | Walltime = 971.947\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -146.183 | Episodes = 1066 | Steps = 3198 | Steps Per Second = 381.254\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.709 | Q = -1.238 | Reward = 3.471 | State = 390|1|50 | Steps = 333552 | Walltime = 972.950\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.349 | Episodes = 1191 | Steps = 3573 | Steps Per Second = 342.411\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.416 | Q = -3.890 | Reward = -2.800 | State = 450|2|50 | Steps = 333900 | Walltime = 973.950\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -99.958 | Episodes = 1308 | Steps = 3924 | Steps Per Second = 377.491\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.545 | Q = -0.421 | Reward = 3.123 | State = 390|2|48 | Steps = 334270 | Walltime = 974.952\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.757 | Episodes = 1431 | Steps = 4293 | Steps Per Second = 340.696\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.089 | Q = -3.711 | Reward = 2.891 | State = 450|2|50 | Steps = 334602 | Walltime = 975.955\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.035 | Episodes = 1542 | Steps = 4626 | Steps Per Second = 338.223\n",
            "[Learner] Action = 0.000 | Avg Td Error = -32.886 | Q = -0.130 | Reward = -33.017 | State = 390|0|45 | Steps = 334973 | Walltime = 976.956\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.387 | Episodes = 1667 | Steps = 5001 | Steps Per Second = 383.602\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.110 | Q = -1.483 | Reward = -6.521 | State = 420|5|51 | Steps = 335348 | Walltime = 977.958\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.085 | Episodes = 1792 | Steps = 5376 | Steps Per Second = 371.024\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.729 | Q = -0.081 | Reward = 1.940 | State = 420|4|51 | Steps = 335721 | Walltime = 978.959\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.589 | Episodes = 1917 | Steps = 5751 | Steps Per Second = 386.477\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.838 | Q = -2.080 | Reward = -11.921 | State = 420|1|49 | Steps = 336085 | Walltime = 979.961\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.637 | Episodes = 2037 | Steps = 6111 | Steps Per Second = 338.678\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.894 | Q = -4.361 | Reward = 1.830 | State = 450|2|50 | Steps = 336427 | Walltime = 980.963\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.477 | Episodes = 2152 | Steps = 6456 | Steps Per Second = 381.856\n",
            "[Learner] Action = -2.000 | Avg Td Error = -10.934 | Q = -1.696 | Reward = -12.630 | State = 390|1|52 | Steps = 336773 | Walltime = 981.966\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.278 | Episodes = 2265 | Steps = 6795 | Steps Per Second = 396.950\n",
            "[Learner] Action = -1.000 | Avg Td Error = -6.149 | Q = -5.940 | Reward = -11.102 | State = 450|2|50 | Steps = 337131 | Walltime = 982.967\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.106 | Episodes = 2387 | Steps = 7161 | Steps Per Second = 389.697\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.144 | Q = -4.455 | Reward = 0.013 | State = 450|2|50 | Steps = 337498 | Walltime = 983.968\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.225 | Episodes = 2510 | Steps = 7530 | Steps Per Second = 379.015\n",
            "[Learner] Action = 3.000 | Avg Td Error = -4.943 | Q = -11.942 | Reward = -16.806 | State = 450|2|50 | Steps = 337857 | Walltime = 984.968\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.389 | Episodes = 2629 | Steps = 7887 | Steps Per Second = 346.627\n",
            "[Learner] Action = 0.000 | Avg Td Error = -17.657 | Q = -4.376 | Reward = -20.324 | State = 450|2|50 | Steps = 338206 | Walltime = 985.968\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.278 | Episodes = 2747 | Steps = 8241 | Steps Per Second = 376.441\n",
            "[Learner] Action = 3.000 | Avg Td Error = -49.824 | Q = -1.521 | Reward = -51.335 | State = 420|2|54 | Steps = 338582 | Walltime = 986.970\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.023 | Episodes = 2873 | Steps = 8619 | Steps Per Second = 393.474\n",
            "[Learner] Action = -4.000 | Avg Td Error = -3.580 | Q = -3.035 | Reward = -6.231 | State = 420|0|51 | Steps = 338951 | Walltime = 987.971\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.855 | Episodes = 2996 | Steps = 8988 | Steps Per Second = 365.846\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.529 | Q = -1.731 | Reward = -3.980 | State = 420|2|49 | Steps = 339320 | Walltime = 988.972\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -138.100 | Episodes = 3118 | Steps = 9354 | Steps Per Second = 324.846\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.204 | Q = -0.879 | Reward = -8.084 | State = 390|-1|47 | Steps = 339663 | Walltime = 989.973\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.129 | Episodes = 3234 | Steps = 9702 | Steps Per Second = 382.599\n",
            "[Learner] Action = -4.000 | Avg Td Error = 0.521 | Q = -17.450 | Reward = -14.258 | State = 450|2|50 | Steps = 340030 | Walltime = 990.975\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -123.988 | Episodes = 3356 | Steps = 10068 | Steps Per Second = 384.716\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.925 | Q = -1.072 | Reward = -7.997 | State = 390|0|49 | Steps = 340405 | Walltime = 991.977\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.291 | Episodes = 3481 | Steps = 10443 | Steps Per Second = 382.146\n",
            "[Learner] Action = -2.000 | Avg Td Error = -10.034 | Q = -0.299 | Reward = -10.333 | State = 390|-3|46 | Steps = 340763 | Walltime = 992.978\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.867 | Episodes = 3601 | Steps = 10803 | Steps Per Second = 385.849\n",
            "[Learner] Action = 3.000 | Avg Td Error = 0.830 | Q = -1.241 | Reward = -0.411 | State = 390|-3|53 | Steps = 341127 | Walltime = 993.978\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.237 | Episodes = 3723 | Steps = 11169 | Steps Per Second = 389.455\n",
            "[Learner] Action = -4.000 | Avg Td Error = -16.541 | Q = -0.878 | Reward = -17.026 | State = 420|6|53 | Steps = 341490 | Walltime = 994.979\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.707 | Episodes = 3845 | Steps = 11535 | Steps Per Second = 396.812\n",
            "[Learner] Action = -2.000 | Avg Td Error = -14.843 | Q = -2.018 | Reward = -16.860 | State = 390|1|51 | Steps = 341860 | Walltime = 995.980\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.983 | Episodes = 3969 | Steps = 11907 | Steps Per Second = 386.204\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.277 | Q = -0.551 | Reward = -8.287 | State = 420|3|51 | Steps = 342230 | Walltime = 996.981\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.127 | Episodes = 4093 | Steps = 12279 | Steps Per Second = 386.833\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.843 | Q = -0.146 | Reward = 0.697 | State = 390|5|52 | Steps = 342583 | Walltime = 997.983\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -84.245 | Episodes = 4211 | Steps = 12633 | Steps Per Second = 395.764\n",
            "[Learner] Action = 1.000 | Avg Td Error = -18.455 | Q = -0.169 | Reward = -18.670 | State = 420|5|54 | Steps = 342923 | Walltime = 998.983\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.053 | Episodes = 4323 | Steps = 12969 | Steps Per Second = 342.280\n",
            "[Learner] Action = -2.000 | Avg Td Error = -10.711 | Q = -1.096 | Reward = -11.806 | State = 390|3|53 | Steps = 343255 | Walltime = 999.985\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.854 | Episodes = 4434 | Steps = 13302 | Steps Per Second = 335.876\n",
            "[Learner] Action = 3.000 | Avg Td Error = -251.820 | Q = -0.828 | Reward = -252.648 | State = 390|-5|51 | Steps = 343603 | Walltime = 1000.987\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.670 | Episodes = 4552 | Steps = 13656 | Steps Per Second = 364.881\n",
            "[Learner] Action = 0.000 | Avg Td Error = -22.162 | Q = -4.204 | Reward = -25.102 | State = 450|2|50 | Steps = 343977 | Walltime = 1001.989\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.787 | Episodes = 4677 | Steps = 14031 | Steps Per Second = 389.697\n",
            "[Learner] Action = 4.000 | Avg Td Error = -49.831 | Q = -2.727 | Reward = -52.558 | State = 390|2|51 | Steps = 344345 | Walltime = 1002.991\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.220 | Episodes = 4799 | Steps = 14397 | Steps Per Second = 372.110\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.538 | Q = -0.726 | Reward = -5.264 | State = 390|2|46 | Steps = 344715 | Walltime = 1003.993\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.633 | Episodes = 4923 | Steps = 14769 | Steps Per Second = 388.926\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.957 | Episodes = 5233 | Steps = 15699 | Steps Per Second = 2000.145\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.527 | Episodes = 5861 | Steps = 17583 | Steps Per Second = 1939.712\n",
            "[Learner] Action = -1.000 | Avg Td Error = -14.880 | Q = -5.559 | Reward = -19.654 | State = 450|2|50 | Steps = 345000 | Walltime = 1006.369\n",
            "Check Point 23\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.793 | Q = -4.324 | Reward = 3.937 | State = 450|2|50 | Steps = 345342 | Walltime = 1007.371\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.989 | Episodes = 115 | Steps = 345 | Steps Per Second = 314.094\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.956 | Q = -5.632 | Reward = -8.034 | State = 450|2|50 | Steps = 345687 | Walltime = 1008.373\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.178 | Episodes = 230 | Steps = 690 | Steps Per Second = 335.670\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.256 | Q = -5.592 | Reward = -4.294 | State = 450|2|50 | Steps = 346052 | Walltime = 1009.373\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.438 | Episodes = 352 | Steps = 1056 | Steps Per Second = 377.604\n",
            "[Learner] Action = 3.000 | Avg Td Error = -14.601 | Q = -11.502 | Reward = -25.954 | State = 450|2|50 | Steps = 346424 | Walltime = 1010.376\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.750 | Episodes = 476 | Steps = 1428 | Steps Per Second = 390.119\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.640 | Q = -1.652 | Reward = -9.149 | State = 420|4|52 | Steps = 346792 | Walltime = 1011.386\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.471 | Episodes = 598 | Steps = 1794 | Steps Per Second = 180.971\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.338 | Q = -5.360 | Reward = 2.473 | State = 450|2|50 | Steps = 347154 | Walltime = 1012.388\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.500 | Episodes = 719 | Steps = 2157 | Steps Per Second = 222.070\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.193 | Q = -3.861 | Reward = 5.560 | State = 450|2|50 | Steps = 347488 | Walltime = 1013.390\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -235.695 | Episodes = 831 | Steps = 2493 | Steps Per Second = 347.681\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.905 | Q = -3.824 | Reward = 5.042 | State = 450|2|50 | Steps = 347862 | Walltime = 1014.392\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.152 | Episodes = 956 | Steps = 2868 | Steps Per Second = 385.494\n",
            "[Learner] Action = -2.000 | Avg Td Error = -20.919 | Q = -9.462 | Reward = -29.242 | State = 450|2|50 | Steps = 348241 | Walltime = 1015.394\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.326 | Episodes = 1083 | Steps = 3249 | Steps Per Second = 395.540\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.809 | Q = -0.089 | Reward = -2.593 | State = 420|4|48 | Steps = 348616 | Walltime = 1016.395\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.618 | Episodes = 1208 | Steps = 3624 | Steps Per Second = 376.058\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.123 | Q = -0.189 | Reward = -1.153 | State = 420|4|54 | Steps = 348958 | Walltime = 1017.396\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.629 | Episodes = 1322 | Steps = 3966 | Steps Per Second = 336.766\n",
            "[Learner] Action = 1.000 | Avg Td Error = -9.315 | Q = -0.429 | Reward = -9.745 | State = 390|6|50 | Steps = 349327 | Walltime = 1018.399\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.291 | Episodes = 1445 | Steps = 4335 | Steps Per Second = 394.956\n",
            "[Learner] Action = 3.000 | Avg Td Error = 12.075 | Q = -11.584 | Reward = 0.537 | State = 450|2|50 | Steps = 349670 | Walltime = 1019.400\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.460 | Episodes = 1559 | Steps = 4677 | Steps Per Second = 329.646\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.859 | Q = -1.670 | Reward = 3.446 | State = 420|2|53 | Steps = 350025 | Walltime = 1020.401\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.703 | Episodes = 1678 | Steps = 5034 | Steps Per Second = 328.390\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.328 | Q = -3.863 | Reward = 5.679 | State = 450|2|50 | Steps = 350377 | Walltime = 1021.403\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.166 | Episodes = 1796 | Steps = 5388 | Steps Per Second = 332.363\n",
            "[Learner] Action = -3.000 | Avg Td Error = -3.029 | Q = -0.264 | Reward = -3.292 | State = 390|8|48 | Steps = 350716 | Walltime = 1022.405\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.063 | Episodes = 1909 | Steps = 5727 | Steps Per Second = 346.828\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.682 | Q = -0.761 | Reward = 5.922 | State = 390|-1|48 | Steps = 351070 | Walltime = 1023.408\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.644 | Episodes = 2027 | Steps = 6081 | Steps Per Second = 342.588\n",
            "[Learner] Action = -3.000 | Avg Td Error = 6.242 | Q = -13.306 | Reward = -5.476 | State = 450|2|50 | Steps = 351406 | Walltime = 1024.409\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -92.711 | Episodes = 2139 | Steps = 6417 | Steps Per Second = 346.436\n",
            "[Learner] Action = 2.000 | Avg Td Error = -22.780 | Q = -0.724 | Reward = -23.504 | State = 390|-3|52 | Steps = 351755 | Walltime = 1025.411\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.181 | Episodes = 2256 | Steps = 6768 | Steps Per Second = 331.793\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.234 | Q = -5.055 | Reward = -6.629 | State = 450|2|50 | Steps = 352122 | Walltime = 1026.411\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -85.680 | Episodes = 2379 | Steps = 7137 | Steps Per Second = 388.302\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.890 | Q = -1.418 | Reward = -7.308 | State = 390|1|51 | Steps = 352492 | Walltime = 1027.413\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.725 | Episodes = 2503 | Steps = 7509 | Steps Per Second = 377.457\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.564 | Q = -0.408 | Reward = 4.155 | State = 390|2|48 | Steps = 352827 | Walltime = 1028.414\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.973 | Episodes = 2614 | Steps = 7842 | Steps Per Second = 335.169\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.936 | Q = -3.973 | Reward = 3.473 | State = 450|2|50 | Steps = 353172 | Walltime = 1029.416\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.147 | Episodes = 2729 | Steps = 8187 | Steps Per Second = 333.031\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.365 | Q = -0.066 | Reward = 0.386 | State = 420|6|52 | Steps = 353546 | Walltime = 1030.417\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.621 | Episodes = 2855 | Steps = 8565 | Steps Per Second = 398.660\n",
            "[Learner] Action = 3.000 | Avg Td Error = -22.733 | Q = -0.823 | Reward = -23.566 | State = 420|5|52 | Steps = 353909 | Walltime = 1031.417\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.924 | Episodes = 2976 | Steps = 8928 | Steps Per Second = 373.469\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.358 | Q = -4.079 | Reward = 3.777 | State = 450|2|50 | Steps = 354275 | Walltime = 1032.420\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.471 | Episodes = 3097 | Steps = 9291 | Steps Per Second = 338.359\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.018 | Q = -5.154 | Reward = 0.333 | State = 450|2|50 | Steps = 354641 | Walltime = 1033.421\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.503 | Episodes = 3220 | Steps = 9660 | Steps Per Second = 376.993\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.762 | Q = -1.965 | Reward = 4.670 | State = 420|-1|50 | Steps = 355014 | Walltime = 1034.422\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.351 | Episodes = 3345 | Steps = 10035 | Steps Per Second = 387.811\n",
            "[Learner] Action = 3.000 | Avg Td Error = -13.875 | Q = -3.405 | Reward = -17.229 | State = 420|2|48 | Steps = 355386 | Walltime = 1035.422\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.267 | Episodes = 3469 | Steps = 10407 | Steps Per Second = 396.375\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.884 | Q = -4.308 | Reward = -10.091 | State = 450|2|50 | Steps = 355750 | Walltime = 1036.425\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.059 | Episodes = 3590 | Steps = 10770 | Steps Per Second = 371.495\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.054 | Q = -1.219 | Reward = -8.272 | State = 390|4|49 | Steps = 356126 | Walltime = 1037.426\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.479 | Episodes = 3716 | Steps = 11148 | Steps Per Second = 384.352\n",
            "[Learner] Action = 3.000 | Avg Td Error = 3.806 | Q = -1.945 | Reward = 1.861 | State = 390|1|51 | Steps = 356500 | Walltime = 1038.426\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.761 | Episodes = 3841 | Steps = 11523 | Steps Per Second = 392.639\n",
            "[Learner] Action = -2.000 | Avg Td Error = -7.553 | Q = -1.875 | Reward = -8.537 | State = 420|1|49 | Steps = 356847 | Walltime = 1039.428\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.468 | Episodes = 3956 | Steps = 11868 | Steps Per Second = 336.433\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.133 | Q = -0.539 | Reward = -0.668 | State = 420|6|49 | Steps = 357217 | Walltime = 1040.430\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.270 | Episodes = 4081 | Steps = 12243 | Steps Per Second = 371.616\n",
            "[Learner] Action = 1.000 | Avg Td Error = -17.186 | Q = -0.031 | Reward = -17.217 | State = 390|6|44 | Steps = 357573 | Walltime = 1041.432\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.080 | Episodes = 4199 | Steps = 12597 | Steps Per Second = 347.979\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.363 | Q = -0.707 | Reward = -9.070 | State = 390|6|52 | Steps = 357941 | Walltime = 1042.434\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.632 | Episodes = 4322 | Steps = 12966 | Steps Per Second = 371.879\n",
            "[Learner] Action = -3.000 | Avg Td Error = 15.456 | Q = -12.914 | Reward = 2.715 | State = 450|2|50 | Steps = 358304 | Walltime = 1043.436\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.747 | Episodes = 4444 | Steps = 13332 | Steps Per Second = 369.999\n",
            "[Learner] Action = 0.000 | Avg Td Error = -33.603 | Q = -0.738 | Reward = -34.025 | State = 420|3|51 | Steps = 358650 | Walltime = 1044.437\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.166 | Episodes = 4560 | Steps = 13680 | Steps Per Second = 369.109\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.601 | Q = -1.647 | Reward = 4.992 | State = 420|2|49 | Steps = 359023 | Walltime = 1045.439\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.328 | Episodes = 4685 | Steps = 14055 | Steps Per Second = 379.770\n",
            "[Learner] Action = -4.000 | Avg Td Error = -10.654 | Q = -4.573 | Reward = -14.355 | State = 420|2|48 | Steps = 359367 | Walltime = 1046.439\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.249 | Episodes = 4800 | Steps = 14400 | Steps Per Second = 387.047\n",
            "[Learner] Action = 0.000 | Avg Td Error = -10.136 | Q = -4.073 | Reward = -12.560 | State = 450|2|50 | Steps = 359735 | Walltime = 1047.441\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.656 | Episodes = 4923 | Steps = 14769 | Steps Per Second = 392.223\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.185 | Episodes = 5246 | Steps = 15738 | Steps Per Second = 1955.386\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.436 | Episodes = 5875 | Steps = 17625 | Steps Per Second = 1869.119\n",
            "[Learner] Action = -2.000 | Avg Td Error = -53.019 | Q = -9.481 | Reward = -62.334 | State = 450|2|50 | Steps = 360000 | Walltime = 1049.744\n",
            "Check Point 24\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.766 | Q = -6.001 | Reward = 1.742 | State = 450|2|50 | Steps = 360365 | Walltime = 1050.744\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.667 | Episodes = 124 | Steps = 372 | Steps Per Second = 385.494\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.149 | Q = -3.938 | Reward = 3.796 | State = 450|2|50 | Steps = 360711 | Walltime = 1051.745\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.968 | Episodes = 240 | Steps = 720 | Steps Per Second = 384.916\n",
            "[Learner] Action = -4.000 | Avg Td Error = 6.728 | Q = -1.405 | Reward = 5.324 | State = 390|-4|50 | Steps = 361084 | Walltime = 1052.747\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -63.653 | Episodes = 365 | Steps = 1095 | Steps Per Second = 397.703\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.736 | Q = -1.171 | Reward = 0.565 | State = 390|1|49 | Steps = 361452 | Walltime = 1053.749\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.397 | Episodes = 488 | Steps = 1464 | Steps Per Second = 370.871\n",
            "[Learner] Action = -1.000 | Avg Td Error = 11.720 | Q = -6.023 | Reward = 5.720 | State = 450|2|50 | Steps = 361823 | Walltime = 1054.752\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.066 | Episodes = 612 | Steps = 1836 | Steps Per Second = 371.901\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.577 | Q = -2.506 | Reward = -4.541 | State = 420|0|51 | Steps = 362187 | Walltime = 1055.754\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.823 | Episodes = 734 | Steps = 2202 | Steps Per Second = 372.187\n",
            "[Learner] Action = 1.000 | Avg Td Error = 2.645 | Q = -0.593 | Reward = 2.172 | State = 420|3|50 | Steps = 362557 | Walltime = 1056.756\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.688 | Episodes = 858 | Steps = 2574 | Steps Per Second = 364.300\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.341 | Q = -0.132 | Reward = -1.472 | State = 390|3|46 | Steps = 362920 | Walltime = 1057.756\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.873 | Episodes = 980 | Steps = 2940 | Steps Per Second = 373.413\n",
            "[Learner] Action = -2.000 | Avg Td Error = -8.448 | Q = -3.137 | Reward = -10.485 | State = 420|2|52 | Steps = 363260 | Walltime = 1058.757\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.556 | Episodes = 1093 | Steps = 3279 | Steps Per Second = 335.276\n",
            "[Learner] Action = -3.000 | Avg Td Error = -24.805 | Q = -0.702 | Reward = -25.506 | State = 390|0|46 | Steps = 363616 | Walltime = 1059.758\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -127.911 | Episodes = 1213 | Steps = 3639 | Steps Per Second = 377.650\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.184 | Q = -3.701 | Reward = 4.459 | State = 450|2|50 | Steps = 363984 | Walltime = 1060.760\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.676 | Episodes = 1336 | Steps = 4008 | Steps Per Second = 391.784\n",
            "[Learner] Action = 3.000 | Avg Td Error = -3.155 | Q = -0.768 | Reward = -3.923 | State = 390|0|56 | Steps = 364357 | Walltime = 1061.763\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.585 | Episodes = 1460 | Steps = 4380 | Steps Per Second = 368.762\n",
            "[Learner] Action = 0.000 | Avg Td Error = -21.851 | Q = -0.257 | Reward = -22.108 | State = 390|2|56 | Steps = 364730 | Walltime = 1062.764\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.667 | Episodes = 1585 | Steps = 4755 | Steps Per Second = 376.903\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.469 | Q = -1.461 | Reward = -4.930 | State = 390|0|50 | Steps = 365074 | Walltime = 1063.765\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.419 | Episodes = 1700 | Steps = 5100 | Steps Per Second = 363.763\n",
            "[Learner] Action = -1.000 | Avg Td Error = -12.945 | Q = -0.068 | Reward = -13.013 | State = 390|-4|46 | Steps = 365436 | Walltime = 1064.766\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.114 | Episodes = 1821 | Steps = 5463 | Steps Per Second = 378.308\n",
            "[Learner] Action = 3.000 | Avg Td Error = 7.103 | Q = -12.297 | Reward = -5.147 | State = 450|2|50 | Steps = 365783 | Walltime = 1065.766\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.565 | Episodes = 1937 | Steps = 5811 | Steps Per Second = 377.310\n",
            "[Learner] Action = 2.000 | Avg Td Error = 2.352 | Q = -8.972 | Reward = -6.354 | State = 450|2|50 | Steps = 366156 | Walltime = 1066.767\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.644 | Episodes = 2061 | Steps = 6183 | Steps Per Second = 386.394\n",
            "[Learner] Action = -4.000 | Avg Td Error = 5.129 | Q = -18.473 | Reward = -10.555 | State = 450|2|50 | Steps = 366518 | Walltime = 1067.768\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.557 | Episodes = 2182 | Steps = 6546 | Steps Per Second = 363.700\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.529 | Q = -0.964 | Reward = 3.565 | State = 390|-2|49 | Steps = 366852 | Walltime = 1068.773\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.732 | Episodes = 2293 | Steps = 6879 | Steps Per Second = 376.002\n",
            "[Learner] Action = 1.000 | Avg Td Error = -4.739 | Q = -5.767 | Reward = -9.899 | State = 450|2|50 | Steps = 367206 | Walltime = 1069.775\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.345 | Episodes = 2411 | Steps = 7233 | Steps Per Second = 377.423\n",
            "[Learner] Action = 4.000 | Avg Td Error = 0.886 | Q = -5.670 | Reward = -4.645 | State = 420|2|49 | Steps = 367555 | Walltime = 1070.776\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.609 | Episodes = 2528 | Steps = 7584 | Steps Per Second = 367.653\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.843 | Q = -0.341 | Reward = -2.137 | State = 420|6|52 | Steps = 367902 | Walltime = 1071.777\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.255 | Episodes = 2644 | Steps = 7932 | Steps Per Second = 353.612\n",
            "[Learner] Action = -4.000 | Avg Td Error = 10.055 | Q = -18.285 | Reward = -6.131 | State = 450|2|50 | Steps = 368252 | Walltime = 1072.778\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.990 | Episodes = 2761 | Steps = 8283 | Steps Per Second = 339.666\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.404 | Q = -0.185 | Reward = -2.595 | State = 420|2|45 | Steps = 368594 | Walltime = 1073.779\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.974 | Episodes = 2875 | Steps = 8625 | Steps Per Second = 368.687\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.690 | Q = -0.621 | Reward = 1.689 | State = 420|2|55 | Steps = 368932 | Walltime = 1074.781\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.870 | Episodes = 2988 | Steps = 8964 | Steps Per Second = 370.380\n",
            "[Learner] Action = -3.000 | Avg Td Error = 14.393 | Q = -12.694 | Reward = 2.273 | State = 450|2|50 | Steps = 369286 | Walltime = 1075.782\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.375 | Episodes = 3106 | Steps = 9318 | Steps Per Second = 349.739\n",
            "[Learner] Action = -4.000 | Avg Td Error = -10.338 | Q = -1.985 | Reward = -11.149 | State = 420|4|50 | Steps = 369607 | Walltime = 1076.782\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -103.775 | Episodes = 3213 | Steps = 9639 | Steps Per Second = 207.800\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.325 | Q = -12.618 | Reward = -2.630 | State = 450|2|50 | Steps = 369951 | Walltime = 1077.783\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.088 | Episodes = 3328 | Steps = 9984 | Steps Per Second = 230.718\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.022 | Q = -8.965 | Reward = 1.148 | State = 450|2|50 | Steps = 370303 | Walltime = 1078.783\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.674 | Episodes = 3448 | Steps = 10344 | Steps Per Second = 341.380\n",
            "[Learner] Action = -3.000 | Avg Td Error = 0.023 | Q = -3.524 | Reward = -3.246 | State = 420|2|48 | Steps = 370670 | Walltime = 1079.783\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.777 | Episodes = 3571 | Steps = 10713 | Steps Per Second = 373.336\n",
            "[Learner] Action = 0.000 | Avg Td Error = -54.566 | Q = -2.594 | Reward = -57.140 | State = 420|2|51 | Steps = 371043 | Walltime = 1080.784\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.397 | Episodes = 3696 | Steps = 11088 | Steps Per Second = 382.250\n",
            "[Learner] Action = 3.000 | Avg Td Error = -31.379 | Q = -12.441 | Reward = -43.707 | State = 450|2|50 | Steps = 371421 | Walltime = 1081.786\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.116 | Episodes = 3822 | Steps = 11466 | Steps Per Second = 383.836\n",
            "[Learner] Action = 3.000 | Avg Td Error = -8.939 | Q = -1.107 | Reward = -10.046 | State = 390|3|52 | Steps = 371789 | Walltime = 1082.788\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.671 | Episodes = 3945 | Steps = 11835 | Steps Per Second = 376.599\n",
            "[Learner] Action = -1.000 | Avg Td Error = -5.671 | Q = -6.440 | Reward = -11.017 | State = 450|2|50 | Steps = 372160 | Walltime = 1083.790\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.217 | Episodes = 4069 | Steps = 12207 | Steps Per Second = 356.406\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.864 | Q = -3.339 | Reward = 2.047 | State = 420|-2|49 | Steps = 372513 | Walltime = 1084.791\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.431 | Episodes = 4187 | Steps = 12561 | Steps Per Second = 384.716\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.404 | Q = -3.788 | Reward = 4.681 | State = 450|2|50 | Steps = 372877 | Walltime = 1085.793\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.180 | Episodes = 4309 | Steps = 12927 | Steps Per Second = 389.672\n",
            "[Learner] Action = -4.000 | Avg Td Error = 1.232 | Q = -2.109 | Reward = -0.877 | State = 390|-2|51 | Steps = 373254 | Walltime = 1086.795\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.869 | Episodes = 4434 | Steps = 13302 | Steps Per Second = 385.152\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.628 | Q = -2.322 | Reward = -11.775 | State = 420|5|50 | Steps = 373622 | Walltime = 1087.796\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.457 | Episodes = 4557 | Steps = 13671 | Steps Per Second = 381.601\n",
            "[Learner] Action = -4.000 | Avg Td Error = 16.298 | Q = -18.405 | Reward = -1.235 | State = 450|2|50 | Steps = 373986 | Walltime = 1088.797\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.511 | Episodes = 4678 | Steps = 14034 | Steps Per Second = 373.635\n",
            "[Learner] Action = 4.000 | Avg Td Error = -22.523 | Q = -16.104 | Reward = -38.551 | State = 450|2|50 | Steps = 374344 | Walltime = 1089.799\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.349 | Episodes = 4796 | Steps = 14388 | Steps Per Second = 316.019\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.521 | Q = -1.498 | Reward = 6.052 | State = 420|-1|48 | Steps = 374686 | Walltime = 1090.801\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.422 | Episodes = 4913 | Steps = 14739 | Steps Per Second = 390.035\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.684 | Episodes = 5151 | Steps = 15453 | Steps Per Second = 1976.581\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.510 | Episodes = 5779 | Steps = 17337 | Steps Per Second = 1957.211\n",
            "[Learner] Action = -2.000 | Avg Td Error = -18.764 | Q = -3.133 | Reward = -20.427 | State = 420|2|49 | Steps = 375000 | Walltime = 1093.307\n",
            "Check Point 25\n",
            "[Learner] Action = 4.000 | Avg Td Error = -12.442 | Q = -0.059 | Reward = -12.501 | State = 390|-1|44 | Steps = 375358 | Walltime = 1094.308\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.201 | Episodes = 121 | Steps = 363 | Steps Per Second = 393.757\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.520 | Q = -12.377 | Reward = -21.731 | State = 450|2|50 | Steps = 375731 | Walltime = 1095.310\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.036 | Episodes = 246 | Steps = 738 | Steps Per Second = 377.242\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.829 | Q = -1.049 | Reward = 1.222 | State = 420|2|48 | Steps = 376100 | Walltime = 1096.311\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.934 | Episodes = 369 | Steps = 1107 | Steps Per Second = 383.520\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.061 | Q = -1.208 | Reward = -0.158 | State = 420|1|52 | Steps = 376449 | Walltime = 1097.313\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -67.344 | Episodes = 485 | Steps = 1455 | Steps Per Second = 331.103\n",
            "[Learner] Action = 2.000 | Avg Td Error = 2.504 | Q = -1.810 | Reward = 0.695 | State = 390|-1|51 | Steps = 376816 | Walltime = 1098.314\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.176 | Episodes = 608 | Steps = 1824 | Steps Per Second = 365.814\n",
            "[Learner] Action = 0.000 | Avg Td Error = -12.432 | Q = -3.629 | Reward = -14.370 | State = 450|2|50 | Steps = 377180 | Walltime = 1099.316\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.054 | Episodes = 730 | Steps = 2190 | Steps Per Second = 376.565\n",
            "[Learner] Action = 2.000 | Avg Td Error = -15.557 | Q = -2.961 | Reward = -17.608 | State = 420|-1|52 | Steps = 377556 | Walltime = 1100.318\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.720 | Episodes = 856 | Steps = 2568 | Steps Per Second = 382.181\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.443 | Q = -0.726 | Reward = 1.589 | State = 420|3|50 | Steps = 377935 | Walltime = 1101.321\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.867 | Episodes = 983 | Steps = 2949 | Steps Per Second = 392.382\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.752 | Q = -1.123 | Reward = -8.371 | State = 420|3|48 | Steps = 378293 | Walltime = 1102.322\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -134.872 | Episodes = 1102 | Steps = 3306 | Steps Per Second = 339.968\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.396 | Q = -0.261 | Reward = 3.209 | State = 420|2|47 | Steps = 378662 | Walltime = 1103.324\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.940 | Episodes = 1226 | Steps = 3678 | Steps Per Second = 372.110\n",
            "[Learner] Action = 3.000 | Avg Td Error = -13.406 | Q = -1.382 | Reward = -14.788 | State = 390|3|49 | Steps = 379020 | Walltime = 1104.324\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.357 | Episodes = 1345 | Steps = 4035 | Steps Per Second = 337.805\n",
            "[Learner] Action = 2.000 | Avg Td Error = -7.128 | Q = -1.530 | Reward = -8.658 | State = 390|0|50 | Steps = 379334 | Walltime = 1105.325\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.106 | Episodes = 1450 | Steps = 4350 | Steps Per Second = 330.165\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.074 | Q = -0.096 | Reward = 3.978 | State = 390|-1|45 | Steps = 379700 | Walltime = 1106.327\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.222 | Episodes = 1573 | Steps = 4719 | Steps Per Second = 382.634\n",
            "[Learner] Action = 2.000 | Avg Td Error = -9.855 | Q = -2.110 | Reward = -10.648 | State = 420|-1|49 | Steps = 380058 | Walltime = 1107.329\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.651 | Episodes = 1692 | Steps = 5076 | Steps Per Second = 374.748\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.033 | Q = -0.433 | Reward = 5.600 | State = 390|-3|47 | Steps = 380384 | Walltime = 1108.330\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -63.720 | Episodes = 1800 | Steps = 5400 | Steps Per Second = 310.919\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.391 | Q = -5.694 | Reward = 0.160 | State = 450|2|50 | Steps = 380730 | Walltime = 1109.331\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.547 | Episodes = 1916 | Steps = 5748 | Steps Per Second = 323.377\n",
            "[Learner] Action = 2.000 | Avg Td Error = 13.324 | Q = -9.303 | Reward = 4.020 | State = 450|2|50 | Steps = 381094 | Walltime = 1110.332\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.441 | Episodes = 2039 | Steps = 6117 | Steps Per Second = 378.513\n",
            "[Learner] Action = 0.000 | Avg Td Error = -118.287 | Q = -1.435 | Reward = -119.723 | State = 390|-1|54 | Steps = 381463 | Walltime = 1111.333\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -187.353 | Episodes = 2162 | Steps = 6486 | Steps Per Second = 356.992\n",
            "[Learner] Action = 4.000 | Avg Td Error = -10.321 | Q = -1.523 | Reward = -11.845 | State = 390|4|52 | Steps = 381832 | Walltime = 1112.337\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.072 | Episodes = 2285 | Steps = 6855 | Steps Per Second = 385.058\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.070 | Q = -0.279 | Reward = 2.791 | State = 390|2|56 | Steps = 382204 | Walltime = 1113.337\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.129 | Episodes = 2409 | Steps = 7227 | Steps Per Second = 392.407\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.874 | Q = -4.036 | Reward = 2.920 | State = 420|2|51 | Steps = 382541 | Walltime = 1114.339\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.180 | Episodes = 2521 | Steps = 7563 | Steps Per Second = 341.667\n",
            "[Learner] Action = 3.000 | Avg Td Error = -11.074 | Q = -0.744 | Reward = -11.817 | State = 390|6|52 | Steps = 382881 | Walltime = 1115.341\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.288 | Episodes = 2635 | Steps = 7905 | Steps Per Second = 343.270\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.687 | Q = -0.448 | Reward = -4.135 | State = 390|4|47 | Steps = 383221 | Walltime = 1116.343\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.217 | Episodes = 2749 | Steps = 8247 | Steps Per Second = 336.046\n",
            "[Learner] Action = -1.000 | Avg Td Error = 11.664 | Q = -6.131 | Reward = 5.673 | State = 450|2|50 | Steps = 383561 | Walltime = 1117.344\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.185 | Episodes = 2864 | Steps = 8592 | Steps Per Second = 379.209\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.698 | Q = -3.827 | Reward = -4.761 | State = 450|2|50 | Steps = 383918 | Walltime = 1118.350\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.407 | Episodes = 2980 | Steps = 8940 | Steps Per Second = 290.551\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.565 | Q = -9.234 | Reward = -12.675 | State = 450|2|50 | Steps = 384252 | Walltime = 1119.353\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.698 | Episodes = 3093 | Steps = 9279 | Steps Per Second = 262.002\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.743 | Q = -3.986 | Reward = 3.287 | State = 450|2|50 | Steps = 384591 | Walltime = 1120.356\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.236 | Episodes = 3207 | Steps = 9621 | Steps Per Second = 237.279\n",
            "[Learner] Action = -3.000 | Avg Td Error = -22.242 | Q = -12.665 | Reward = -33.834 | State = 450|2|50 | Steps = 384962 | Walltime = 1121.356\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -53.693 | Episodes = 3333 | Steps = 9999 | Steps Per Second = 205.499\n",
            "[Learner] Action = -2.000 | Avg Td Error = -0.544 | Q = -9.295 | Reward = -7.639 | State = 450|2|50 | Steps = 385340 | Walltime = 1122.358\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.227 | Episodes = 3460 | Steps = 10380 | Steps Per Second = 302.249\n",
            "[Learner] Action = -4.000 | Avg Td Error = -17.557 | Q = -0.087 | Reward = -17.642 | State = 420|5|45 | Steps = 385714 | Walltime = 1123.359\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.834 | Episodes = 3585 | Steps = 10755 | Steps Per Second = 385.364\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.060 | Q = -0.421 | Reward = 1.639 | State = 390|4|52 | Steps = 386081 | Walltime = 1124.360\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.399 | Episodes = 3708 | Steps = 11124 | Steps Per Second = 381.821\n",
            "[Learner] Action = -3.000 | Avg Td Error = -28.391 | Q = -1.464 | Reward = -29.854 | State = 390|0|48 | Steps = 386458 | Walltime = 1125.361\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.015 | Episodes = 3834 | Steps = 11502 | Steps Per Second = 373.469\n",
            "[Learner] Action = 4.000 | Avg Td Error = 13.752 | Q = -16.733 | Reward = -2.800 | State = 450|2|50 | Steps = 386820 | Walltime = 1126.362\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.001 | Episodes = 3955 | Steps = 11865 | Steps Per Second = 385.258\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.342 | Q = -0.600 | Reward = -1.943 | State = 390|3|51 | Steps = 387175 | Walltime = 1127.362\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.294 | Episodes = 4072 | Steps = 12216 | Steps Per Second = 340.825\n",
            "[Learner] Action = 3.000 | Avg Td Error = 10.261 | Q = -12.132 | Reward = -1.807 | State = 450|2|50 | Steps = 387551 | Walltime = 1128.364\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.551 | Episodes = 4200 | Steps = 12600 | Steps Per Second = 347.969\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.511 | Q = -0.515 | Reward = -4.480 | State = 420|0|47 | Steps = 387894 | Walltime = 1129.365\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.539 | Episodes = 4313 | Steps = 12939 | Steps Per Second = 337.208\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.054 | Q = -0.808 | Reward = -7.862 | State = 390|4|48 | Steps = 388237 | Walltime = 1130.368\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.327 | Episodes = 4428 | Steps = 13284 | Steps Per Second = 363.395\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.064 | Q = -0.924 | Reward = 5.140 | State = 390|1|51 | Steps = 388608 | Walltime = 1131.369\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.733 | Episodes = 4554 | Steps = 13662 | Steps Per Second = 380.045\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.098 | Q = -6.250 | Reward = -4.617 | State = 450|2|50 | Steps = 388989 | Walltime = 1132.369\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.011 | Episodes = 4681 | Steps = 14043 | Steps Per Second = 384.411\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.681 | Q = -0.472 | Reward = -8.153 | State = 390|5|46 | Steps = 389356 | Walltime = 1133.372\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.085 | Episodes = 4803 | Steps = 14409 | Steps Per Second = 340.557\n",
            "[Learner] Action = 2.000 | Avg Td Error = 11.970 | Q = -9.019 | Reward = 2.944 | State = 450|2|50 | Steps = 389721 | Walltime = 1134.373\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.386 | Episodes = 4925 | Steps = 14775 | Steps Per Second = 388.565\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.192 | Episodes = 5253 | Steps = 15759 | Steps Per Second = 1962.094\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.263 | Episodes = 5888 | Steps = 17664 | Steps Per Second = 1760.094\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.544 | Q = -1.294 | Reward = 4.250 | State = 390|2|51 | Steps = 390000 | Walltime = 1136.716\n",
            "Check Point 26\n",
            "[Learner] Action = -4.000 | Avg Td Error = -10.354 | Q = -0.427 | Reward = -10.781 | State = 390|2|44 | Steps = 390364 | Walltime = 1137.718\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.904 | Episodes = 123 | Steps = 369 | Steps Per Second = 374.804\n",
            "[Learner] Action = 3.000 | Avg Td Error = -26.227 | Q = -1.870 | Reward = -28.097 | State = 390|0|50 | Steps = 390728 | Walltime = 1138.720\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.535 | Episodes = 245 | Steps = 735 | Steps Per Second = 362.986\n",
            "[Learner] Action = 2.000 | Avg Td Error = -11.008 | Q = -9.063 | Reward = -19.730 | State = 450|2|50 | Steps = 391101 | Walltime = 1139.720\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.521 | Episodes = 370 | Steps = 1110 | Steps Per Second = 378.058\n",
            "[Learner] Action = -1.000 | Avg Td Error = -58.505 | Q = -1.338 | Reward = -59.843 | State = 390|0|53 | Steps = 391477 | Walltime = 1140.721\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.870 | Episodes = 496 | Steps = 1488 | Steps Per Second = 378.445\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.012 | Q = -0.851 | Reward = -0.662 | State = 420|2|47 | Steps = 391824 | Walltime = 1141.726\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.159 | Episodes = 611 | Steps = 1833 | Steps Per Second = 313.382\n",
            "[Learner] Action = -2.000 | Avg Td Error = -12.486 | Q = -0.816 | Reward = -13.303 | State = 390|0|48 | Steps = 392195 | Walltime = 1142.726\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.802 | Episodes = 736 | Steps = 2208 | Steps Per Second = 381.208\n",
            "[Learner] Action = -3.000 | Avg Td Error = -61.844 | Q = -2.370 | Reward = -63.545 | State = 420|0|50 | Steps = 392544 | Walltime = 1143.728\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.347 | Episodes = 852 | Steps = 2556 | Steps Per Second = 375.744\n",
            "[Learner] Action = -2.000 | Avg Td Error = -0.027 | Q = -0.891 | Reward = -0.918 | State = 390|-4|51 | Steps = 392908 | Walltime = 1144.728\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.264 | Episodes = 974 | Steps = 2922 | Steps Per Second = 329.361\n",
            "[Learner] Action = 1.000 | Avg Td Error = 2.830 | Q = -0.392 | Reward = 3.255 | State = 420|1|54 | Steps = 393244 | Walltime = 1145.730\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.774 | Episodes = 1087 | Steps = 3261 | Steps Per Second = 386.026\n",
            "[Learner] Action = -3.000 | Avg Td Error = 10.379 | Q = -12.707 | Reward = -0.932 | State = 450|2|50 | Steps = 393615 | Walltime = 1146.730\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.369 | Episodes = 1211 | Steps = 3633 | Steps Per Second = 351.763\n",
            "[Learner] Action = 1.000 | Avg Td Error = -7.653 | Q = -0.288 | Reward = -7.919 | State = 420|6|52 | Steps = 393963 | Walltime = 1147.732\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.527 | Episodes = 1328 | Steps = 3984 | Steps Per Second = 254.452\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.627 | Q = -4.214 | Reward = -8.473 | State = 450|2|50 | Steps = 394308 | Walltime = 1148.734\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.828 | Episodes = 1444 | Steps = 4332 | Steps Per Second = 208.344\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.873 | Q = -2.161 | Reward = -1.288 | State = 390|1|51 | Steps = 394649 | Walltime = 1149.735\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.629 | Episodes = 1558 | Steps = 4674 | Steps Per Second = 216.737\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.297 | Q = -1.281 | Reward = -6.761 | State = 420|5|48 | Steps = 394984 | Walltime = 1150.737\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.552 | Episodes = 1670 | Steps = 5010 | Steps Per Second = 331.225\n",
            "[Learner] Action = 0.000 | Avg Td Error = -12.991 | Q = -0.683 | Reward = -13.673 | State = 390|-3|50 | Steps = 395342 | Walltime = 1151.739\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.817 | Episodes = 1790 | Steps = 5370 | Steps Per Second = 317.815\n",
            "[Learner] Action = 4.000 | Avg Td Error = -28.665 | Q = -0.458 | Reward = -29.133 | State = 420|-1|46 | Steps = 395675 | Walltime = 1152.739\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.685 | Episodes = 1903 | Steps = 5709 | Steps Per Second = 381.196\n",
            "[Learner] Action = 1.000 | Avg Td Error = -13.763 | Q = -5.766 | Reward = -19.388 | State = 450|2|50 | Steps = 396042 | Walltime = 1153.739\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -66.658 | Episodes = 2026 | Steps = 6078 | Steps Per Second = 363.395\n",
            "[Learner] Action = -2.000 | Avg Td Error = 7.670 | Q = -1.421 | Reward = 6.250 | State = 390|-2|48 | Steps = 396378 | Walltime = 1154.741\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.607 | Episodes = 2139 | Steps = 6417 | Steps Per Second = 380.344\n",
            "[Learner] Action = 1.000 | Avg Td Error = -18.282 | Q = -1.095 | Reward = -19.378 | State = 390|-2|48 | Steps = 396742 | Walltime = 1155.743\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.674 | Episodes = 2261 | Steps = 6783 | Steps Per Second = 385.659\n",
            "[Learner] Action = -2.000 | Avg Td Error = 3.385 | Q = -9.414 | Reward = -4.006 | State = 450|2|50 | Steps = 397090 | Walltime = 1156.743\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -86.682 | Episodes = 2378 | Steps = 7134 | Steps Per Second = 374.402\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.880 | Q = -1.034 | Reward = -2.108 | State = 420|2|48 | Steps = 397459 | Walltime = 1157.744\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.883 | Episodes = 2500 | Steps = 7500 | Steps Per Second = 330.486\n",
            "[Learner] Action = 2.000 | Avg Td Error = -26.699 | Q = -8.790 | Reward = -35.258 | State = 450|2|50 | Steps = 397790 | Walltime = 1158.745\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.856 | Episodes = 2613 | Steps = 7839 | Steps Per Second = 382.483\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.202 | Q = -3.654 | Reward = -6.949 | State = 420|2|48 | Steps = 398164 | Walltime = 1159.746\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.060 | Episodes = 2738 | Steps = 8214 | Steps Per Second = 372.022\n",
            "[Learner] Action = 1.000 | Avg Td Error = 11.144 | Q = -5.819 | Reward = 5.339 | State = 450|2|50 | Steps = 398527 | Walltime = 1160.747\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.010 | Episodes = 2859 | Steps = 8577 | Steps Per Second = 369.835\n",
            "[Learner] Action = 4.000 | Avg Td Error = -65.033 | Q = -1.800 | Reward = -66.832 | State = 390|0|53 | Steps = 398840 | Walltime = 1161.748\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.358 | Episodes = 2960 | Steps = 8880 | Steps Per Second = 290.270\n",
            "[Learner] Action = -4.000 | Avg Td Error = -42.826 | Q = -1.381 | Reward = -43.941 | State = 420|1|53 | Steps = 399128 | Walltime = 1162.751\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.262 | Episodes = 3057 | Steps = 9171 | Steps Per Second = 326.016\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.899 | Q = -1.856 | Reward = 3.998 | State = 420|1|50 | Steps = 399461 | Walltime = 1163.751\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.798 | Episodes = 3172 | Steps = 9516 | Steps Per Second = 375.497\n",
            "[Learner] Action = 4.000 | Avg Td Error = 12.580 | Q = -16.680 | Reward = -3.921 | State = 450|2|50 | Steps = 399803 | Walltime = 1164.757\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.710 | Episodes = 3285 | Steps = 9855 | Steps Per Second = 390.701\n",
            "[Learner] Action = -4.000 | Avg Td Error = 8.899 | Q = -19.158 | Reward = -8.141 | State = 450|2|50 | Steps = 400167 | Walltime = 1165.760\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -88.777 | Episodes = 3405 | Steps = 10215 | Steps Per Second = 346.962\n",
            "[Learner] Action = 3.000 | Avg Td Error = 5.469 | Q = -1.305 | Reward = 4.164 | State = 390|1|48 | Steps = 400494 | Walltime = 1166.760\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.807 | Episodes = 3517 | Steps = 10551 | Steps Per Second = 395.502\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.291 | Q = -1.569 | Reward = -0.835 | State = 420|-2|48 | Steps = 400852 | Walltime = 1167.760\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.510 | Episodes = 3636 | Steps = 10908 | Steps Per Second = 370.522\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.331 | Q = -3.945 | Reward = 1.037 | State = 450|2|50 | Steps = 401198 | Walltime = 1168.762\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.817 | Episodes = 3752 | Steps = 11256 | Steps Per Second = 381.173\n",
            "[Learner] Action = -1.000 | Avg Td Error = 10.742 | Q = -6.223 | Reward = 4.883 | State = 450|2|50 | Steps = 401547 | Walltime = 1169.764\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.447 | Episodes = 3868 | Steps = 11604 | Steps Per Second = 385.234\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.656 | Q = -5.968 | Reward = 3.723 | State = 450|2|50 | Steps = 401916 | Walltime = 1170.767\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -84.364 | Episodes = 3991 | Steps = 11973 | Steps Per Second = 391.162\n",
            "[Learner] Action = -2.000 | Avg Td Error = -24.433 | Q = -9.456 | Reward = -33.460 | State = 450|2|50 | Steps = 402283 | Walltime = 1171.767\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.800 | Episodes = 4112 | Steps = 12336 | Steps Per Second = 335.106\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.686 | Q = -6.270 | Reward = 1.466 | State = 450|2|50 | Steps = 402616 | Walltime = 1172.767\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.271 | Episodes = 4223 | Steps = 12669 | Steps Per Second = 316.042\n",
            "[Learner] Action = -4.000 | Avg Td Error = 9.474 | Q = -19.342 | Reward = -7.715 | State = 450|2|50 | Steps = 402947 | Walltime = 1173.768\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.488 | Episodes = 4336 | Steps = 13008 | Steps Per Second = 377.627\n",
            "[Learner] Action = -1.000 | Avg Td Error = -36.800 | Q = -6.221 | Reward = -42.845 | State = 450|2|50 | Steps = 403315 | Walltime = 1174.770\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.250 | Episodes = 4457 | Steps = 13371 | Steps Per Second = 281.932\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.353 | Q = -4.165 | Reward = 2.916 | State = 450|2|50 | Steps = 403674 | Walltime = 1175.772\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.883 | Episodes = 4577 | Steps = 13731 | Steps Per Second = 336.100\n",
            "[Learner] Action = -4.000 | Avg Td Error = -4.560 | Q = -2.240 | Reward = -6.800 | State = 390|3|51 | Steps = 403963 | Walltime = 1176.775\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.230 | Episodes = 4669 | Steps = 14007 | Steps Per Second = 250.202\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.885 | Q = -1.365 | Reward = 5.519 | State = 390|1|50 | Steps = 404168 | Walltime = 1177.778\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.812 | Episodes = 4737 | Steps = 14211 | Steps Per Second = 256.647\n",
            "[Learner] Action = 4.000 | Avg Td Error = -29.120 | Q = -3.117 | Reward = -32.094 | State = 420|2|47 | Steps = 404392 | Walltime = 1178.781\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.347 | Episodes = 4811 | Steps = 14433 | Steps Per Second = 245.928\n",
            "[Learner] Action = 1.000 | Avg Td Error = -35.251 | Q = -0.494 | Reward = -35.745 | State = 390|3|55 | Steps = 404615 | Walltime = 1179.782\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.972 | Episodes = 4886 | Steps = 14658 | Steps Per Second = 233.276\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.922 | Q = -0.486 | Reward = -1.363 | State = 420|4|48 | Steps = 404895 | Walltime = 1180.784\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.746 | Episodes = 4988 | Steps = 14964 | Steps Per Second = 386.679\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.151 | Episodes = 5573 | Steps = 16719 | Steps Per Second = 1988.136\n",
            "[Learner] Action = -2.000 | Avg Td Error = 13.565 | Q = -9.584 | Reward = 4.142 | State = 450|2|50 | Steps = 405000 | Walltime = 1182.661\n",
            "Check Point 27\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.854 | Q = -2.527 | Reward = 6.249 | State = 420|-1|52 | Steps = 405358 | Walltime = 1183.663\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.285 | Episodes = 121 | Steps = 363 | Steps Per Second = 385.801\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.607 | Q = -8.126 | Reward = -6.341 | State = 420|2|50 | Steps = 405723 | Walltime = 1184.664\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.517 | Episodes = 243 | Steps = 729 | Steps Per Second = 388.565\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.590 | Q = -0.097 | Reward = -3.688 | State = 390|5|47 | Steps = 406077 | Walltime = 1185.664\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.398 | Episodes = 361 | Steps = 1083 | Steps Per Second = 368.503\n",
            "[Learner] Action = 1.000 | Avg Td Error = -7.075 | Q = -0.036 | Reward = -7.110 | State = 390|-2|44 | Steps = 406398 | Walltime = 1186.664\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.560 | Episodes = 468 | Steps = 1404 | Steps Per Second = 328.956\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.736 | Q = -0.919 | Reward = -4.655 | State = 390|2|47 | Steps = 406735 | Walltime = 1187.667\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.134 | Episodes = 580 | Steps = 1740 | Steps Per Second = 308.163\n",
            "[Learner] Action = -3.000 | Avg Td Error = -12.651 | Q = -1.386 | Reward = -14.041 | State = 420|6|48 | Steps = 407067 | Walltime = 1188.669\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.323 | Episodes = 692 | Steps = 2076 | Steps Per Second = 359.903\n",
            "[Learner] Action = -4.000 | Avg Td Error = 1.171 | Q = -4.928 | Reward = -3.511 | State = 420|2|48 | Steps = 407416 | Walltime = 1189.670\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.129 | Episodes = 809 | Steps = 2427 | Steps Per Second = 342.905\n",
            "[Learner] Action = -1.000 | Avg Td Error = -9.746 | Q = -6.182 | Reward = -14.780 | State = 450|2|50 | Steps = 407748 | Walltime = 1190.670\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.147 | Episodes = 920 | Steps = 2760 | Steps Per Second = 374.514\n",
            "[Learner] Action = 3.000 | Avg Td Error = -16.555 | Q = -0.143 | Reward = -16.698 | State = 390|4|44 | Steps = 408094 | Walltime = 1191.672\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -158.179 | Episodes = 1036 | Steps = 3108 | Steps Per Second = 337.869\n",
            "[Learner] Action = 4.000 | Avg Td Error = -66.585 | Q = -3.327 | Reward = -69.912 | State = 390|-2|50 | Steps = 408454 | Walltime = 1192.672\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.512 | Episodes = 1156 | Steps = 3468 | Steps Per Second = 366.806\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.893 | Q = -0.656 | Reward = 6.237 | State = 390|-1|47 | Steps = 408811 | Walltime = 1193.673\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.919 | Episodes = 1276 | Steps = 3828 | Steps Per Second = 368.816\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.233 | Q = -0.369 | Reward = -0.602 | State = 390|2|48 | Steps = 409167 | Walltime = 1194.674\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -75.701 | Episodes = 1396 | Steps = 4188 | Steps Per Second = 373.170\n",
            "[Learner] Action = -4.000 | Avg Td Error = -17.109 | Q = -1.224 | Reward = -17.098 | State = 420|3|53 | Steps = 409509 | Walltime = 1195.675\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.553 | Episodes = 1510 | Steps = 4530 | Steps Per Second = 366.977\n",
            "[Learner] Action = 2.000 | Avg Td Error = -7.474 | Q = -0.160 | Reward = -7.634 | State = 390|-2|44 | Steps = 409866 | Walltime = 1196.675\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.199 | Episodes = 1630 | Steps = 4890 | Steps Per Second = 371.112\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.729 | Q = -0.053 | Reward = 0.823 | State = 420|5|48 | Steps = 410206 | Walltime = 1197.676\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.154 | Episodes = 1744 | Steps = 5232 | Steps Per Second = 358.763\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.384 | Q = -2.783 | Reward = 3.486 | State = 420|-1|51 | Steps = 410563 | Walltime = 1198.677\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.816 | Episodes = 1864 | Steps = 5592 | Steps Per Second = 383.625\n",
            "[Learner] Action = 3.000 | Avg Td Error = -2.508 | Q = -12.454 | Reward = -14.833 | State = 450|2|50 | Steps = 410926 | Walltime = 1199.678\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.656 | Episodes = 1985 | Steps = 5955 | Steps Per Second = 389.504\n",
            "[Learner] Action = -4.000 | Avg Td Error = -14.162 | Q = -0.436 | Reward = -14.598 | State = 390|1|44 | Steps = 411286 | Walltime = 1200.680\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.429 | Episodes = 2105 | Steps = 6315 | Steps Per Second = 351.203\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.572 | Q = -0.129 | Reward = -0.707 | State = 420|2|45 | Steps = 411649 | Walltime = 1201.680\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.416 | Episodes = 2227 | Steps = 6681 | Steps Per Second = 372.562\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.208 | Q = -0.445 | Reward = 4.763 | State = 390|-1|47 | Steps = 412009 | Walltime = 1202.683\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -201.019 | Episodes = 2345 | Steps = 7035 | Steps Per Second = 336.549\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.510 | Q = -12.593 | Reward = -8.014 | State = 450|2|50 | Steps = 412344 | Walltime = 1203.684\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.849 | Episodes = 2458 | Steps = 7374 | Steps Per Second = 313.562\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.066 | Q = -0.479 | Reward = 2.587 | State = 390|-4|48 | Steps = 412668 | Walltime = 1204.686\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.441 | Episodes = 2568 | Steps = 7704 | Steps Per Second = 354.328\n",
            "[Learner] Action = 0.000 | Avg Td Error = -9.340 | Q = -0.821 | Reward = -9.553 | State = 420|3|51 | Steps = 413007 | Walltime = 1205.687\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.123 | Episodes = 2679 | Steps = 8037 | Steps Per Second = 350.431\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.480 | Q = -0.553 | Reward = -2.015 | State = 420|6|50 | Steps = 413350 | Walltime = 1206.687\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.438 | Episodes = 2796 | Steps = 8388 | Steps Per Second = 373.004\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.287 | Q = -0.403 | Reward = -5.690 | State = 390|7|50 | Steps = 413709 | Walltime = 1207.687\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.654 | Episodes = 2916 | Steps = 8748 | Steps Per Second = 366.945\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.498 | Q = -0.805 | Reward = -4.303 | State = 390|5|50 | Steps = 414049 | Walltime = 1208.688\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.160 | Episodes = 3030 | Steps = 9090 | Steps Per Second = 368.061\n",
            "[Learner] Action = 3.000 | Avg Td Error = 10.707 | Q = -4.955 | Reward = 6.245 | State = 420|-2|50 | Steps = 414405 | Walltime = 1209.691\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.584 | Episodes = 3149 | Steps = 9447 | Steps Per Second = 375.665\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.251 | Q = -8.148 | Reward = -18.680 | State = 420|2|50 | Steps = 414767 | Walltime = 1210.692\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -106.703 | Episodes = 3270 | Steps = 9810 | Steps Per Second = 361.152\n",
            "[Learner] Action = 2.000 | Avg Td Error = -14.940 | Q = -8.888 | Reward = -23.425 | State = 450|2|50 | Steps = 415132 | Walltime = 1211.693\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.282 | Episodes = 3393 | Steps = 10179 | Steps Per Second = 375.991\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.589 | Q = -12.532 | Reward = -7.874 | State = 450|2|50 | Steps = 415501 | Walltime = 1212.695\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.037 | Episodes = 3516 | Steps = 10548 | Steps Per Second = 386.738\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.034 | Q = -1.046 | Reward = -3.041 | State = 420|0|48 | Steps = 415864 | Walltime = 1213.699\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.408 | Episodes = 3637 | Steps = 10911 | Steps Per Second = 391.540\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.293 | Q = -0.024 | Reward = 1.269 | State = 390|5|49 | Steps = 416223 | Walltime = 1214.702\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.737 | Episodes = 3756 | Steps = 11268 | Steps Per Second = 351.272\n",
            "[Learner] Action = 3.000 | Avg Td Error = -16.875 | Q = -1.009 | Reward = -17.884 | State = 390|-2|45 | Steps = 416565 | Walltime = 1215.705\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.500 | Episodes = 3870 | Steps = 11610 | Steps Per Second = 365.209\n",
            "[Learner] Action = -3.000 | Avg Td Error = -2.600 | Q = -13.337 | Reward = -13.132 | State = 450|2|50 | Steps = 416928 | Walltime = 1216.705\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.425 | Episodes = 3991 | Steps = 11973 | Steps Per Second = 380.724\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.716 | Q = -3.765 | Reward = 5.425 | State = 420|-1|50 | Steps = 417292 | Walltime = 1217.707\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.752 | Episodes = 4113 | Steps = 12339 | Steps Per Second = 386.228\n",
            "[Learner] Action = 0.000 | Avg Td Error = -105.854 | Q = -1.507 | Reward = -107.361 | State = 390|1|52 | Steps = 417660 | Walltime = 1218.709\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.500 | Episodes = 4236 | Steps = 12708 | Steps Per Second = 358.989\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.116 | Q = -0.755 | Reward = -2.870 | State = 390|3|48 | Steps = 418020 | Walltime = 1219.712\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.513 | Episodes = 4354 | Steps = 13062 | Steps Per Second = 330.729\n",
            "[Learner] Action = 1.000 | Avg Td Error = -25.590 | Q = -6.061 | Reward = -31.395 | State = 450|2|50 | Steps = 418346 | Walltime = 1220.715\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.985 | Episodes = 4465 | Steps = 13395 | Steps Per Second = 371.331\n",
            "[Learner] Action = -2.000 | Avg Td Error = -12.889 | Q = -2.351 | Reward = -13.734 | State = 420|1|50 | Steps = 418706 | Walltime = 1221.715\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.620 | Episodes = 4586 | Steps = 13758 | Steps Per Second = 367.911\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.049 | Q = -0.626 | Reward = -5.675 | State = 390|2|46 | Steps = 419049 | Walltime = 1222.717\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.981 | Episodes = 4701 | Steps = 14103 | Steps Per Second = 379.988\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.552 | Q = -4.236 | Reward = -7.357 | State = 450|2|50 | Steps = 419417 | Walltime = 1223.718\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.138 | Episodes = 4821 | Steps = 14463 | Steps Per Second = 332.890\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.672 | Q = -4.166 | Reward = 4.480 | State = 450|2|50 | Steps = 419745 | Walltime = 1224.718\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.632 | Episodes = 4932 | Steps = 14796 | Steps Per Second = 331.452\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.741 | Episodes = 5228 | Steps = 15684 | Steps Per Second = 1969.465\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.251 | Episodes = 5851 | Steps = 17553 | Steps Per Second = 1975.650\n",
            "[Learner] Action = 4.000 | Avg Td Error = 0.095 | Q = -16.736 | Reward = -16.585 | State = 450|2|50 | Steps = 420000 | Walltime = 1227.126\n",
            "Check Point 28\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.042 | Q = -1.000 | Reward = -8.042 | State = 390|5|49 | Steps = 420349 | Walltime = 1228.127\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.843 | Episodes = 118 | Steps = 354 | Steps Per Second = 321.772\n",
            "[Learner] Action = 0.000 | Avg Td Error = -33.665 | Q = -0.474 | Reward = -33.721 | State = 420|0|54 | Steps = 420686 | Walltime = 1229.130\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.834 | Episodes = 231 | Steps = 693 | Steps Per Second = 365.740\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.915 | Q = -0.164 | Reward = 1.751 | State = 390|2|47 | Steps = 421041 | Walltime = 1230.131\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.368 | Episodes = 350 | Steps = 1050 | Steps Per Second = 372.661\n",
            "[Learner] Action = 4.000 | Avg Td Error = -19.060 | Q = -2.509 | Reward = -21.496 | State = 420|4|51 | Steps = 421406 | Walltime = 1231.133\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.669 | Episodes = 472 | Steps = 1416 | Steps Per Second = 373.247\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.300 | Q = -0.960 | Reward = -2.375 | State = 420|2|48 | Steps = 421768 | Walltime = 1232.137\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.928 | Episodes = 592 | Steps = 1776 | Steps Per Second = 374.592\n",
            "[Learner] Action = -2.000 | Avg Td Error = -33.501 | Q = -0.902 | Reward = -34.234 | State = 420|3|52 | Steps = 422127 | Walltime = 1233.138\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.150 | Episodes = 712 | Steps = 2136 | Steps Per Second = 383.988\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.515 | Q = -0.383 | Reward = -4.897 | State = 390|6|53 | Steps = 422470 | Walltime = 1234.139\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.241 | Episodes = 826 | Steps = 2478 | Steps Per Second = 313.585\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.422 | Q = -0.405 | Reward = 0.018 | State = 390|3|48 | Steps = 422818 | Walltime = 1235.140\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.866 | Episodes = 943 | Steps = 2829 | Steps Per Second = 388.517\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.064 | Q = -6.385 | Reward = 0.815 | State = 450|2|50 | Steps = 423146 | Walltime = 1236.143\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.884 | Episodes = 1052 | Steps = 3156 | Steps Per Second = 333.066\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.276 | Q = -0.974 | Reward = -5.250 | State = 390|2|46 | Steps = 423490 | Walltime = 1237.143\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.048 | Episodes = 1167 | Steps = 3501 | Steps Per Second = 379.701\n",
            "[Learner] Action = 1.000 | Avg Td Error = -8.958 | Q = -1.113 | Reward = -10.071 | State = 390|2|53 | Steps = 423850 | Walltime = 1238.144\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.952 | Episodes = 1288 | Steps = 3864 | Steps Per Second = 364.902\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.214 | Q = -2.091 | Reward = -1.588 | State = 420|1|50 | Steps = 424205 | Walltime = 1239.144\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.171 | Episodes = 1407 | Steps = 4221 | Steps Per Second = 366.091\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.397 | Q = -0.527 | Reward = 3.870 | State = 390|4|52 | Steps = 424562 | Walltime = 1240.144\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.056 | Episodes = 1526 | Steps = 4578 | Steps Per Second = 335.831\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.501 | Q = -13.502 | Reward = -19.361 | State = 450|2|50 | Steps = 424897 | Walltime = 1241.146\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.737 | Episodes = 1638 | Steps = 4914 | Steps Per Second = 308.155\n",
            "[Learner] Action = 4.000 | Avg Td Error = 9.351 | Q = -16.788 | Reward = -7.247 | State = 450|2|50 | Steps = 425243 | Walltime = 1242.147\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.262 | Episodes = 1754 | Steps = 5262 | Steps Per Second = 333.844\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.193 | Q = -1.023 | Reward = 6.170 | State = 390|0|54 | Steps = 425577 | Walltime = 1243.149\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.146 | Episodes = 1865 | Steps = 5595 | Steps Per Second = 333.534\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.106 | Q = -4.288 | Reward = 5.998 | State = 450|2|50 | Steps = 425927 | Walltime = 1244.152\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.480 | Episodes = 1983 | Steps = 5949 | Steps Per Second = 388.625\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.812 | Q = -0.132 | Reward = -0.765 | State = 420|5|51 | Steps = 426294 | Walltime = 1245.154\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.309 | Episodes = 2105 | Steps = 6315 | Steps Per Second = 381.208\n",
            "[Learner] Action = 1.000 | Avg Td Error = -4.785 | Q = -0.404 | Reward = -5.189 | State = 390|3|48 | Steps = 426651 | Walltime = 1246.155\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.046 | Episodes = 2225 | Steps = 6675 | Steps Per Second = 367.878\n",
            "[Learner] Action = -1.000 | Avg Td Error = 8.642 | Q = -6.373 | Reward = 3.066 | State = 450|2|50 | Steps = 427019 | Walltime = 1247.156\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.093 | Episodes = 2348 | Steps = 7044 | Steps Per Second = 384.928\n",
            "[Learner] Action = -1.000 | Avg Td Error = 9.098 | Q = -6.337 | Reward = 3.553 | State = 450|2|50 | Steps = 427380 | Walltime = 1248.159\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -100.221 | Episodes = 2468 | Steps = 7404 | Steps Per Second = 333.959\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.075 | Q = -1.420 | Reward = -0.377 | State = 420|-1|48 | Steps = 427739 | Walltime = 1249.159\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.289 | Episodes = 2588 | Steps = 7764 | Steps Per Second = 326.761\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.300 | Q = -6.164 | Reward = 1.617 | State = 450|2|50 | Steps = 428085 | Walltime = 1250.161\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -89.323 | Episodes = 2705 | Steps = 8115 | Steps Per Second = 388.457\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.608 | Q = -0.570 | Reward = -2.045 | State = 420|6|50 | Steps = 428449 | Walltime = 1251.163\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.407 | Episodes = 2826 | Steps = 8478 | Steps Per Second = 376.126\n",
            "[Learner] Action = -3.000 | Avg Td Error = -9.115 | Q = -1.726 | Reward = -10.840 | State = 390|3|50 | Steps = 428818 | Walltime = 1252.163\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.112 | Episodes = 2949 | Steps = 8847 | Steps Per Second = 392.345\n",
            "[Learner] Action = -1.000 | Avg Td Error = -17.538 | Q = -0.808 | Reward = -17.928 | State = 420|1|54 | Steps = 429184 | Walltime = 1253.165\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.550 | Episodes = 3070 | Steps = 9210 | Steps Per Second = 322.540\n",
            "[Learner] Action = 3.000 | Avg Td Error = -24.797 | Q = -2.275 | Reward = -26.682 | State = 420|1|51 | Steps = 429535 | Walltime = 1254.168\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.632 | Episodes = 3189 | Steps = 9567 | Steps Per Second = 397.916\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.215 | Q = -1.057 | Reward = -0.843 | State = 390|2|49 | Steps = 429904 | Walltime = 1255.169\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.419 | Episodes = 3312 | Steps = 9936 | Steps Per Second = 365.442\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.537 | Q = -1.149 | Reward = -10.686 | State = 390|5|48 | Steps = 430241 | Walltime = 1256.174\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.938 | Episodes = 3423 | Steps = 10269 | Steps Per Second = 339.574\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.069 | Q = -0.710 | Reward = -3.779 | State = 390|6|50 | Steps = 430595 | Walltime = 1257.177\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.201 | Episodes = 3542 | Steps = 10626 | Steps Per Second = 367.760\n",
            "[Learner] Action = -1.000 | Avg Td Error = -23.327 | Q = -6.229 | Reward = -28.875 | State = 450|2|50 | Steps = 430959 | Walltime = 1258.177\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.478 | Episodes = 3664 | Steps = 10992 | Steps Per Second = 197.851\n",
            "[Learner] Action = -4.000 | Avg Td Error = 1.551 | Q = -19.303 | Reward = -14.286 | State = 450|2|50 | Steps = 431320 | Walltime = 1259.179\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -62.216 | Episodes = 3785 | Steps = 11355 | Steps Per Second = 202.849\n",
            "[Learner] Action = 3.000 | Avg Td Error = -43.265 | Q = -12.579 | Reward = -55.732 | State = 450|2|50 | Steps = 431682 | Walltime = 1260.180\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.418 | Episodes = 3907 | Steps = 11721 | Steps Per Second = 210.646\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.080 | Q = -0.040 | Reward = 1.040 | State = 390|5|50 | Steps = 432051 | Walltime = 1261.180\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -206.865 | Episodes = 4031 | Steps = 12093 | Steps Per Second = 247.549\n",
            "[Learner] Action = 1.000 | Avg Td Error = -4.834 | Q = -0.577 | Reward = -5.411 | State = 390|3|53 | Steps = 432411 | Walltime = 1262.181\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.454 | Episodes = 4153 | Steps = 12459 | Steps Per Second = 200.390\n",
            "[Learner] Action = -2.000 | Avg Td Error = 5.663 | Q = -2.433 | Reward = 3.807 | State = 420|1|50 | Steps = 432775 | Walltime = 1263.183\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.805 | Episodes = 4275 | Steps = 12825 | Steps Per Second = 238.335\n",
            "[Learner] Action = -4.000 | Avg Td Error = -6.835 | Q = -8.276 | Reward = -13.782 | State = 420|2|49 | Steps = 433136 | Walltime = 1264.184\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -94.794 | Episodes = 4396 | Steps = 13188 | Steps Per Second = 380.977\n",
            "[Learner] Action = 2.000 | Avg Td Error = -10.869 | Q = -2.711 | Reward = -11.843 | State = 420|-2|49 | Steps = 433482 | Walltime = 1265.186\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.258 | Episodes = 4509 | Steps = 13527 | Steps Per Second = 345.333\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.569 | Q = -0.850 | Reward = 5.718 | State = 390|-3|48 | Steps = 433829 | Walltime = 1266.187\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.516 | Episodes = 4625 | Steps = 13875 | Steps Per Second = 171.315\n",
            "[Learner] Action = 2.000 | Avg Td Error = 11.078 | Q = -8.894 | Reward = 2.305 | State = 450|2|50 | Steps = 434170 | Walltime = 1267.187\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.501 | Episodes = 4740 | Steps = 14220 | Steps Per Second = 336.802\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.271 | Q = -2.961 | Reward = 2.248 | State = 420|-1|51 | Steps = 434495 | Walltime = 1268.187\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.185 | Episodes = 4847 | Steps = 14541 | Steps Per Second = 341.769\n",
            "[Learner] Action = 3.000 | Avg Td Error = 1.535 | Q = -0.126 | Reward = 1.409 | State = 390|5|56 | Steps = 434816 | Walltime = 1269.188\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -96.734 | Episodes = 4954 | Steps = 14862 | Steps Per Second = 324.060\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.767 | Episodes = 5366 | Steps = 16098 | Steps Per Second = 1952.353\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.329 | Episodes = 5985 | Steps = 17955 | Steps Per Second = 1977.823\n",
            "[Learner] Action = 3.000 | Avg Td Error = 10.525 | Q = -12.375 | Reward = -1.785 | State = 450|2|50 | Steps = 435000 | Walltime = 1271.367\n",
            "Check Point 29\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.714 | Q = -1.896 | Reward = 5.817 | State = 390|-2|51 | Steps = 435333 | Walltime = 1272.370\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.838 | Episodes = 112 | Steps = 336 | Steps Per Second = 284.077\n",
            "[Learner] Action = -1.000 | Avg Td Error = -12.511 | Q = -0.603 | Reward = -13.113 | State = 390|1|48 | Steps = 435666 | Walltime = 1273.372\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.277 | Episodes = 223 | Steps = 669 | Steps Per Second = 311.628\n",
            "[Learner] Action = -1.000 | Avg Td Error = 12.417 | Q = -6.405 | Reward = 6.014 | State = 450|2|50 | Steps = 436029 | Walltime = 1274.373\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.379 | Episodes = 344 | Steps = 1032 | Steps Per Second = 321.435\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.820 | Q = -0.306 | Reward = -1.623 | State = 420|4|50 | Steps = 436396 | Walltime = 1275.374\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.694 | Episodes = 467 | Steps = 1401 | Steps Per Second = 381.682\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.343 | Q = -3.433 | Reward = 6.060 | State = 420|2|52 | Steps = 436758 | Walltime = 1276.376\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.475 | Episodes = 587 | Steps = 1761 | Steps Per Second = 261.997\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.643 | Q = -4.795 | Reward = -2.599 | State = 450|2|50 | Steps = 437113 | Walltime = 1277.377\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.075 | Episodes = 706 | Steps = 2118 | Steps Per Second = 383.883\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.370 | Q = -5.856 | Reward = 3.571 | State = 450|2|50 | Steps = 437455 | Walltime = 1278.380\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.279 | Episodes = 820 | Steps = 2460 | Steps Per Second = 330.330\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.547 | Q = -2.179 | Reward = 1.922 | State = 420|2|53 | Steps = 437819 | Walltime = 1279.381\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.464 | Episodes = 942 | Steps = 2826 | Steps Per Second = 349.244\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.107 | Q = -4.618 | Reward = -4.886 | State = 450|2|50 | Steps = 438174 | Walltime = 1280.384\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.304 | Episodes = 1060 | Steps = 3180 | Steps Per Second = 307.613\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.428 | Q = -5.799 | Reward = 3.685 | State = 450|2|50 | Steps = 438498 | Walltime = 1281.384\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.258 | Episodes = 1169 | Steps = 3507 | Steps Per Second = 319.169\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.891 | Q = -16.680 | Reward = -14.486 | State = 450|2|50 | Steps = 438858 | Walltime = 1282.386\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.035 | Episodes = 1289 | Steps = 3867 | Steps Per Second = 373.247\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.211 | Q = -1.674 | Reward = 3.537 | State = 390|0|52 | Steps = 439188 | Walltime = 1283.388\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -117.154 | Episodes = 1399 | Steps = 4197 | Steps Per Second = 319.663\n",
            "[Learner] Action = 2.000 | Avg Td Error = -13.479 | Q = -2.971 | Reward = -15.089 | State = 420|-2|52 | Steps = 439534 | Walltime = 1284.389\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.119 | Episodes = 1515 | Steps = 4545 | Steps Per Second = 367.728\n",
            "[Learner] Action = 4.000 | Avg Td Error = -14.064 | Q = -1.024 | Reward = -15.039 | State = 420|5|47 | Steps = 439897 | Walltime = 1285.391\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.223 | Episodes = 1636 | Steps = 4908 | Steps Per Second = 377.684\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.948 | Q = -4.677 | Reward = 5.678 | State = 450|2|50 | Steps = 440245 | Walltime = 1286.393\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.250 | Episodes = 1752 | Steps = 5256 | Steps Per Second = 377.310\n",
            "[Learner] Action = -4.000 | Avg Td Error = -11.692 | Q = -1.517 | Reward = -12.963 | State = 420|5|48 | Steps = 440602 | Walltime = 1287.396\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -104.242 | Episodes = 1871 | Steps = 5613 | Steps Per Second = 369.152\n",
            "[Learner] Action = -4.000 | Avg Td Error = -7.427 | Q = -18.831 | Reward = -23.132 | State = 450|2|50 | Steps = 440955 | Walltime = 1288.396\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.787 | Episodes = 1989 | Steps = 5967 | Steps Per Second = 336.694\n",
            "[Learner] Action = -2.000 | Avg Td Error = -15.147 | Q = -4.103 | Reward = -17.851 | State = 420|2|51 | Steps = 441316 | Walltime = 1289.398\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.972 | Episodes = 2110 | Steps = 6330 | Steps Per Second = 370.773\n",
            "[Learner] Action = 2.000 | Avg Td Error = -44.086 | Q = -0.798 | Reward = -44.883 | State = 390|4|53 | Steps = 441674 | Walltime = 1290.400\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.965 | Episodes = 2230 | Steps = 6690 | Steps Per Second = 366.304\n",
            "[Learner] Action = 4.000 | Avg Td Error = 7.099 | Q = -16.729 | Reward = -9.325 | State = 450|2|50 | Steps = 442010 | Walltime = 1291.400\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.852 | Episodes = 2342 | Steps = 7026 | Steps Per Second = 369.531\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.087 | Q = -0.194 | Reward = -3.133 | State = 420|6|53 | Steps = 442369 | Walltime = 1292.402\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.950 | Episodes = 2462 | Steps = 7386 | Steps Per Second = 369.260\n",
            "[Learner] Action = -1.000 | Avg Td Error = 9.095 | Q = -6.789 | Reward = 3.192 | State = 450|2|50 | Steps = 442737 | Walltime = 1293.403\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.004 | Episodes = 2585 | Steps = 7755 | Steps Per Second = 332.608\n",
            "[Learner] Action = -2.000 | Avg Td Error = 2.752 | Q = -3.772 | Reward = -0.516 | State = 420|2|49 | Steps = 443071 | Walltime = 1294.404\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -55.356 | Episodes = 2696 | Steps = 8088 | Steps Per Second = 247.646\n",
            "[Learner] Action = 0.000 | Avg Td Error = -20.615 | Q = -1.493 | Reward = -22.108 | State = 390|0|52 | Steps = 443405 | Walltime = 1295.404\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.637 | Episodes = 2808 | Steps = 8424 | Steps Per Second = 290.786\n",
            "[Learner] Action = -3.000 | Avg Td Error = -17.194 | Q = -2.158 | Reward = -19.352 | State = 390|3|52 | Steps = 443766 | Walltime = 1296.406\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.621 | Episodes = 2930 | Steps = 8790 | Steps Per Second = 363.279\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.441 | Q = -1.680 | Reward = 0.008 | State = 420|2|48 | Steps = 444100 | Walltime = 1297.407\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.543 | Episodes = 3042 | Steps = 9126 | Steps Per Second = 376.655\n",
            "[Learner] Action = 0.000 | Avg Td Error = -30.017 | Q = -0.716 | Reward = -30.733 | State = 390|2|54 | Steps = 444466 | Walltime = 1298.408\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.673 | Episodes = 3164 | Steps = 9492 | Steps Per Second = 341.509\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.566 | Q = -2.353 | Reward = -10.919 | State = 390|3|49 | Steps = 444826 | Walltime = 1299.411\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.117 | Episodes = 3285 | Steps = 9855 | Steps Per Second = 370.434\n",
            "[Learner] Action = -1.000 | Avg Td Error = 11.819 | Q = -6.679 | Reward = 5.336 | State = 450|2|50 | Steps = 445194 | Walltime = 1300.412\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -87.355 | Episodes = 3408 | Steps = 10224 | Steps Per Second = 342.607\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.331 | Q = -0.467 | Reward = 1.864 | State = 390|-1|46 | Steps = 445529 | Walltime = 1301.413\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.234 | Episodes = 3519 | Steps = 10557 | Steps Per Second = 332.266\n",
            "[Learner] Action = -1.000 | Avg Td Error = -6.935 | Q = -0.310 | Reward = -7.245 | State = 390|7|48 | Steps = 445896 | Walltime = 1302.414\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.654 | Episodes = 3643 | Steps = 10929 | Steps Per Second = 343.233\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.330 | Q = -0.025 | Reward = -1.355 | State = 390|10|51 | Steps = 446253 | Walltime = 1303.416\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.417 | Episodes = 3763 | Steps = 11289 | Steps Per Second = 353.810\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.511 | Q = -1.340 | Reward = -7.708 | State = 420|5|50 | Steps = 446590 | Walltime = 1304.416\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.583 | Episodes = 3874 | Steps = 11622 | Steps Per Second = 353.731\n",
            "[Learner] Action = -4.000 | Avg Td Error = -18.992 | Q = -1.286 | Reward = -20.278 | State = 390|5|48 | Steps = 446932 | Walltime = 1305.416\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.536 | Episodes = 3989 | Steps = 11967 | Steps Per Second = 386.548\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.636 | Q = -1.491 | Reward = -7.590 | State = 420|4|48 | Steps = 447294 | Walltime = 1306.417\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.467 | Episodes = 4111 | Steps = 12333 | Steps Per Second = 386.014\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.496 | Q = -1.715 | Reward = 2.797 | State = 420|2|48 | Steps = 447653 | Walltime = 1307.420\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.266 | Episodes = 4231 | Steps = 12693 | Steps Per Second = 383.906\n",
            "[Learner] Action = 3.000 | Avg Td Error = -0.888 | Q = -1.728 | Reward = -2.615 | State = 390|2|48 | Steps = 448022 | Walltime = 1308.422\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.657 | Episodes = 4353 | Steps = 13059 | Steps Per Second = 322.366\n",
            "[Learner] Action = 1.000 | Avg Td Error = -68.484 | Q = -2.291 | Reward = -70.306 | State = 420|-2|53 | Steps = 448376 | Walltime = 1309.424\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.417 | Episodes = 4472 | Steps = 13416 | Steps Per Second = 270.624\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.407 | Q = -4.092 | Reward = 4.483 | State = 450|2|50 | Steps = 448710 | Walltime = 1310.426\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.813 | Episodes = 4583 | Steps = 13749 | Steps Per Second = 368.352\n",
            "[Learner] Action = -2.000 | Avg Td Error = 6.212 | Q = -1.506 | Reward = 4.706 | State = 390|-2|48 | Steps = 449050 | Walltime = 1311.430\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.335 | Episodes = 4696 | Steps = 14088 | Steps Per Second = 342.700\n",
            "[Learner] Action = 3.000 | Avg Td Error = -11.926 | Q = -11.839 | Reward = -23.573 | State = 450|2|50 | Steps = 449380 | Walltime = 1312.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.212 | Episodes = 4807 | Steps = 14421 | Steps Per Second = 387.465\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.836 | Q = -0.288 | Reward = -1.123 | State = 390|5|47 | Steps = 449713 | Walltime = 1313.435\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.916 | Episodes = 4917 | Steps = 14751 | Steps Per Second = 303.700\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.955 | Episodes = 5195 | Steps = 15585 | Steps Per Second = 1923.404\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.110 | Episodes = 5830 | Steps = 17490 | Steps Per Second = 1998.874\n",
            "[Learner] Action = 0.000 | Avg Td Error = -35.875 | Q = -1.544 | Reward = -37.418 | State = 390|-1|50 | Steps = 450000 | Walltime = 1315.834\n",
            "Check Point 30\n",
            "[Learner] Action = 1.000 | Avg Td Error = -8.916 | Q = -0.311 | Reward = -9.227 | State = 390|5|54 | Steps = 450342 | Walltime = 1316.836\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.734 | Episodes = 116 | Steps = 348 | Steps Per Second = 369.825\n",
            "[Learner] Action = -2.000 | Avg Td Error = 6.021 | Q = -9.939 | Reward = -2.187 | State = 450|2|50 | Steps = 450697 | Walltime = 1317.838\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.815 | Episodes = 235 | Steps = 705 | Steps Per Second = 392.590\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.116 | Q = -1.991 | Reward = -1.406 | State = 420|2|49 | Steps = 451063 | Walltime = 1318.839\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -76.620 | Episodes = 357 | Steps = 1071 | Steps Per Second = 384.599\n",
            "[Learner] Action = 2.000 | Avg Td Error = -28.556 | Q = -0.605 | Reward = -29.156 | State = 420|5|53 | Steps = 451417 | Walltime = 1319.839\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -121.371 | Episodes = 475 | Steps = 1425 | Steps Per Second = 368.892\n",
            "[Learner] Action = -3.000 | Avg Td Error = 10.652 | Q = -13.919 | Reward = -1.688 | State = 450|2|50 | Steps = 451777 | Walltime = 1320.843\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.408 | Episodes = 595 | Steps = 1785 | Steps Per Second = 387.596\n",
            "[Learner] Action = 3.000 | Avg Td Error = 9.705 | Q = -6.506 | Reward = 3.284 | State = 420|2|51 | Steps = 452125 | Walltime = 1321.843\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.020 | Episodes = 712 | Steps = 2136 | Steps Per Second = 372.860\n",
            "[Learner] Action = 3.000 | Avg Td Error = 6.157 | Q = -0.557 | Reward = 5.600 | State = 390|-5|50 | Steps = 452484 | Walltime = 1322.844\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.842 | Episodes = 832 | Steps = 2496 | Steps Per Second = 372.375\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.958 | Q = -1.727 | Reward = 6.231 | State = 390|-1|51 | Steps = 452857 | Walltime = 1323.845\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.270 | Episodes = 957 | Steps = 2871 | Steps Per Second = 389.371\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.429 | Q = -0.510 | Reward = -6.939 | State = 390|-1|46 | Steps = 453190 | Walltime = 1324.846\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.295 | Episodes = 1068 | Steps = 3204 | Steps Per Second = 326.888\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.156 | Q = -0.009 | Reward = 1.147 | State = 390|8|52 | Steps = 453528 | Walltime = 1325.847\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.296 | Episodes = 1182 | Steps = 3546 | Steps Per Second = 382.320\n",
            "[Learner] Action = -2.000 | Avg Td Error = -23.701 | Q = -0.699 | Reward = -22.707 | State = 420|2|46 | Steps = 453882 | Walltime = 1326.848\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.269 | Episodes = 1300 | Steps = 3900 | Steps Per Second = 384.340\n",
            "[Learner] Action = 3.000 | Avg Td Error = -32.669 | Q = -3.435 | Reward = -34.995 | State = 420|-1|49 | Steps = 454222 | Walltime = 1327.849\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.737 | Episodes = 1413 | Steps = 4239 | Steps Per Second = 305.181\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.670 | Q = -4.535 | Reward = 5.235 | State = 450|2|50 | Steps = 454559 | Walltime = 1328.851\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.316 | Episodes = 1527 | Steps = 4581 | Steps Per Second = 329.085\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.044 | Q = -13.840 | Reward = -17.839 | State = 450|2|50 | Steps = 454923 | Walltime = 1329.851\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.906 | Episodes = 1649 | Steps = 4947 | Steps Per Second = 386.168\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.258 | Q = -4.560 | Reward = -5.078 | State = 450|2|50 | Steps = 455293 | Walltime = 1330.854\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.066 | Episodes = 1773 | Steps = 5319 | Steps Per Second = 386.038\n",
            "[Learner] Action = -2.000 | Avg Td Error = -0.755 | Q = -1.696 | Reward = -2.451 | State = 390|2|50 | Steps = 455659 | Walltime = 1331.854\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.655 | Episodes = 1895 | Steps = 5685 | Steps Per Second = 378.331\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.614 | Q = -1.743 | Reward = 2.949 | State = 420|2|48 | Steps = 456020 | Walltime = 1332.856\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.638 | Episodes = 2015 | Steps = 6045 | Steps Per Second = 383.895\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.171 | Q = -0.294 | Reward = 1.057 | State = 420|6|50 | Steps = 456381 | Walltime = 1333.857\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.201 | Episodes = 2136 | Steps = 6408 | Steps Per Second = 349.234\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.244 | Q = -2.156 | Reward = -11.400 | State = 390|2|47 | Steps = 456744 | Walltime = 1334.859\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.324 | Episodes = 2257 | Steps = 6771 | Steps Per Second = 374.180\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.118 | Q = -1.906 | Reward = -8.337 | State = 420|1|50 | Steps = 457108 | Walltime = 1335.860\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.274 | Episodes = 2378 | Steps = 7134 | Steps Per Second = 367.814\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.427 | Q = -3.448 | Reward = -0.521 | State = 420|-2|51 | Steps = 457472 | Walltime = 1336.861\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -81.115 | Episodes = 2500 | Steps = 7500 | Steps Per Second = 370.227\n",
            "[Learner] Action = 4.000 | Avg Td Error = 8.274 | Q = -4.193 | Reward = 4.674 | State = 420|-1|52 | Steps = 457832 | Walltime = 1337.863\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -91.349 | Episodes = 2620 | Steps = 7860 | Steps Per Second = 380.390\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.744 | Q = -0.725 | Reward = -6.470 | State = 390|5|49 | Steps = 458202 | Walltime = 1338.864\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.601 | Episodes = 2743 | Steps = 8229 | Steps Per Second = 349.934\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.381 | Q = -0.474 | Reward = 2.906 | State = 390|0|46 | Steps = 458536 | Walltime = 1339.865\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -52.192 | Episodes = 2856 | Steps = 8568 | Steps Per Second = 360.005\n",
            "[Learner] Action = -2.000 | Avg Td Error = -25.663 | Q = -1.525 | Reward = -27.189 | State = 390|-2|48 | Steps = 458900 | Walltime = 1340.866\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.912 | Episodes = 2978 | Steps = 8934 | Steps Per Second = 326.524\n",
            "[Learner] Action = 3.000 | Avg Td Error = 14.936 | Q = -12.103 | Reward = 2.815 | State = 450|2|50 | Steps = 459267 | Walltime = 1341.868\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.084 | Episodes = 3101 | Steps = 9303 | Steps Per Second = 356.386\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.425 | Q = -4.028 | Reward = 5.846 | State = 450|2|50 | Steps = 459618 | Walltime = 1342.869\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -58.885 | Episodes = 3216 | Steps = 9648 | Steps Per Second = 313.382\n",
            "[Learner] Action = 2.000 | Avg Td Error = 2.370 | Q = -0.909 | Reward = 1.461 | State = 390|-4|48 | Steps = 459953 | Walltime = 1343.871\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.839 | Episodes = 3329 | Steps = 9987 | Steps Per Second = 322.962\n",
            "[Learner] Action = -2.000 | Avg Td Error = -23.237 | Q = -2.006 | Reward = -24.098 | State = 420|-1|48 | Steps = 460285 | Walltime = 1344.871\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.796 | Episodes = 3437 | Steps = 10311 | Steps Per Second = 123.374\n",
            "[Learner] Action = -3.000 | Avg Td Error = -13.140 | Q = -2.122 | Reward = -14.257 | State = 420|-1|48 | Steps = 460597 | Walltime = 1345.873\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.759 | Episodes = 3546 | Steps = 10638 | Steps Per Second = 332.802\n",
            "[Learner] Action = -4.000 | Avg Td Error = 6.976 | Q = -17.999 | Reward = -8.524 | State = 450|2|50 | Steps = 460939 | Walltime = 1346.875\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.377 | Episodes = 3661 | Steps = 10983 | Steps Per Second = 377.843\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.706 | Q = -0.387 | Reward = -7.897 | State = 420|0|46 | Steps = 461293 | Walltime = 1347.878\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.386 | Episodes = 3779 | Steps = 11337 | Steps Per Second = 368.849\n",
            "[Learner] Action = 0.000 | Avg Td Error = -10.470 | Q = -0.125 | Reward = -10.643 | State = 420|5|51 | Steps = 461652 | Walltime = 1348.879\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.299 | Episodes = 3899 | Steps = 11697 | Steps Per Second = 376.430\n",
            "[Learner] Action = 1.000 | Avg Td Error = -6.001 | Q = -5.693 | Reward = -10.924 | State = 450|2|50 | Steps = 461996 | Walltime = 1349.880\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.874 | Episodes = 4012 | Steps = 12036 | Steps Per Second = 338.469\n",
            "[Learner] Action = -3.000 | Avg Td Error = 6.995 | Q = -1.416 | Reward = 5.578 | State = 390|-3|52 | Steps = 462331 | Walltime = 1350.881\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.500 | Episodes = 4126 | Steps = 12378 | Steps Per Second = 355.872\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.276 | Q = -3.141 | Reward = -3.755 | State = 420|-2|52 | Steps = 462697 | Walltime = 1351.882\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.959 | Episodes = 4248 | Steps = 12744 | Steps Per Second = 362.798\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.474 | Q = -0.437 | Reward = 1.037 | State = 390|2|48 | Steps = 463065 | Walltime = 1352.885\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.837 | Episodes = 4370 | Steps = 13110 | Steps Per Second = 333.782\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.311 | Q = -2.745 | Reward = 0.285 | State = 420|0|51 | Steps = 463410 | Walltime = 1353.886\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.520 | Episodes = 4486 | Steps = 13458 | Steps Per Second = 381.891\n",
            "[Learner] Action = -1.000 | Avg Td Error = -7.968 | Q = -6.481 | Reward = -13.005 | State = 450|2|50 | Steps = 463759 | Walltime = 1354.887\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -394.144 | Episodes = 4602 | Steps = 13806 | Steps Per Second = 332.503\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.175 | Q = -3.974 | Reward = 5.340 | State = 450|2|50 | Steps = 464105 | Walltime = 1355.889\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.677 | Episodes = 4719 | Steps = 14157 | Steps Per Second = 387.668\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.093 | Q = -1.136 | Reward = -13.230 | State = 390|-2|47 | Steps = 464472 | Walltime = 1356.892\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.321 | Episodes = 4839 | Steps = 14517 | Steps Per Second = 344.549\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.106 | Q = -3.856 | Reward = -6.517 | State = 450|2|50 | Steps = 464804 | Walltime = 1357.893\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.054 | Episodes = 4952 | Steps = 14856 | Steps Per Second = 373.891\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.096 | Episodes = 5358 | Steps = 16074 | Steps Per Second = 1712.893\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.493 | Episodes = 5959 | Steps = 17877 | Steps Per Second = 1952.655\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.361 | Q = -0.279 | Reward = -1.640 | State = 390|6|53 | Steps = 465000 | Walltime = 1360.102\n",
            "Check Point 31\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.674 | Q = -3.880 | Reward = 4.923 | State = 450|2|50 | Steps = 465360 | Walltime = 1361.102\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.233 | Episodes = 122 | Steps = 366 | Steps Per Second = 381.555\n",
            "[Learner] Action = -3.000 | Avg Td Error = 15.170 | Q = -14.501 | Reward = 1.368 | State = 450|2|50 | Steps = 465730 | Walltime = 1362.104\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.107 | Episodes = 246 | Steps = 738 | Steps Per Second = 375.542\n",
            "[Learner] Action = -2.000 | Avg Td Error = -19.230 | Q = -9.962 | Reward = -27.707 | State = 450|2|50 | Steps = 466092 | Walltime = 1363.105\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.196 | Episodes = 367 | Steps = 1101 | Steps Per Second = 388.098\n",
            "[Learner] Action = 0.000 | Avg Td Error = -50.447 | Q = -2.596 | Reward = -51.272 | State = 420|-2|49 | Steps = 466457 | Walltime = 1364.106\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.962 | Episodes = 489 | Steps = 1467 | Steps Per Second = 329.871\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.495 | Q = -4.204 | Reward = 0.996 | State = 450|2|50 | Steps = 466815 | Walltime = 1365.107\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.445 | Episodes = 609 | Steps = 1827 | Steps Per Second = 387.417\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.116 | Q = -0.112 | Reward = 2.003 | State = 390|5|55 | Steps = 467185 | Walltime = 1366.109\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.272 | Episodes = 733 | Steps = 2199 | Steps Per Second = 388.445\n",
            "[Learner] Action = 2.000 | Avg Td Error = -18.459 | Q = -3.888 | Reward = -21.916 | State = 420|2|50 | Steps = 467555 | Walltime = 1367.109\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.757 | Episodes = 857 | Steps = 2571 | Steps Per Second = 386.346\n",
            "[Learner] Action = 3.000 | Avg Td Error = 12.160 | Q = -12.733 | Reward = -0.484 | State = 450|2|50 | Steps = 467918 | Walltime = 1368.110\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.274 | Episodes = 978 | Steps = 2934 | Steps Per Second = 364.395\n",
            "[Learner] Action = 3.000 | Avg Td Error = 5.728 | Q = -1.040 | Reward = 4.689 | State = 390|-3|49 | Steps = 468281 | Walltime = 1369.110\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.288 | Episodes = 1099 | Steps = 3297 | Steps Per Second = 346.570\n",
            "[Learner] Action = -1.000 | Avg Td Error = -11.331 | Q = -6.582 | Reward = -16.810 | State = 450|2|50 | Steps = 468633 | Walltime = 1370.111\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.839 | Episodes = 1217 | Steps = 3651 | Steps Per Second = 388.433\n",
            "[Learner] Action = -2.000 | Avg Td Error = 12.502 | Q = -9.888 | Reward = 3.151 | State = 450|2|50 | Steps = 469007 | Walltime = 1371.113\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.470 | Episodes = 1342 | Steps = 4026 | Steps Per Second = 375.128\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.344 | Q = -0.800 | Reward = -4.808 | State = 420|0|48 | Steps = 469377 | Walltime = 1372.113\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -87.336 | Episodes = 1466 | Steps = 4398 | Steps Per Second = 368.881\n",
            "[Learner] Action = 3.000 | Avg Td Error = 1.103 | Q = -12.750 | Reward = -11.517 | State = 450|2|50 | Steps = 469726 | Walltime = 1373.114\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.448 | Episodes = 1582 | Steps = 4746 | Steps Per Second = 338.059\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.260 | Q = -1.155 | Reward = 4.170 | State = 420|3|51 | Steps = 470076 | Walltime = 1374.114\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.438 | Episodes = 1700 | Steps = 5100 | Steps Per Second = 375.901\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.712 | Q = -4.583 | Reward = 6.195 | State = 450|2|50 | Steps = 470440 | Walltime = 1375.116\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.551 | Episodes = 1821 | Steps = 5463 | Steps Per Second = 393.573\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.990 | Q = -1.307 | Reward = 0.683 | State = 390|3|50 | Steps = 470783 | Walltime = 1376.118\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.711 | Episodes = 1935 | Steps = 5805 | Steps Per Second = 314.714\n",
            "[Learner] Action = -4.000 | Avg Td Error = -2.267 | Q = -8.699 | Reward = -9.432 | State = 420|2|49 | Steps = 471134 | Walltime = 1377.120\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.549 | Episodes = 2053 | Steps = 6159 | Steps Per Second = 387.465\n",
            "[Learner] Action = 4.000 | Avg Td Error = 2.709 | Q = -16.922 | Reward = -13.920 | State = 450|2|50 | Steps = 471495 | Walltime = 1378.122\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.548 | Episodes = 2174 | Steps = 6522 | Steps Per Second = 381.012\n",
            "[Learner] Action = 4.000 | Avg Td Error = 9.195 | Q = -16.861 | Reward = -7.436 | State = 450|2|50 | Steps = 471833 | Walltime = 1379.125\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.323 | Episodes = 2286 | Steps = 6858 | Steps Per Second = 305.930\n",
            "[Learner] Action = 3.000 | Avg Td Error = -30.098 | Q = -1.488 | Reward = -31.586 | State = 390|3|54 | Steps = 472152 | Walltime = 1380.126\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.958 | Episodes = 2393 | Steps = 7179 | Steps Per Second = 276.620\n",
            "[Learner] Action = 2.000 | Avg Td Error = 1.660 | Q = -8.763 | Reward = -6.756 | State = 450|2|50 | Steps = 472479 | Walltime = 1381.128\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.567 | Episodes = 2504 | Steps = 7512 | Steps Per Second = 350.861\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.788 | Q = -4.723 | Reward = -2.682 | State = 450|2|50 | Steps = 472818 | Walltime = 1382.130\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.548 | Episodes = 2616 | Steps = 7848 | Steps Per Second = 308.276\n",
            "[Learner] Action = 4.000 | Avg Td Error = 7.382 | Q = -2.034 | Reward = 5.348 | State = 390|1|53 | Steps = 473149 | Walltime = 1383.131\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.645 | Episodes = 2728 | Steps = 8184 | Steps Per Second = 365.389\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.594 | Q = -1.151 | Reward = -7.744 | State = 390|1|54 | Steps = 473495 | Walltime = 1384.132\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.897 | Episodes = 2844 | Steps = 8532 | Steps Per Second = 341.806\n",
            "[Learner] Action = 1.000 | Avg Td Error = -30.123 | Q = -0.712 | Reward = -30.835 | State = 390|3|53 | Steps = 473836 | Walltime = 1385.133\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.821 | Episodes = 2959 | Steps = 8877 | Steps Per Second = 375.497\n",
            "[Learner] Action = -3.000 | Avg Td Error = -15.672 | Q = -13.886 | Reward = -26.517 | State = 450|2|50 | Steps = 474172 | Walltime = 1386.135\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.141 | Episodes = 3069 | Steps = 9207 | Steps Per Second = 308.639\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.054 | Q = -3.402 | Reward = 4.474 | State = 420|-1|52 | Steps = 474505 | Walltime = 1387.136\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.707 | Episodes = 3182 | Steps = 9546 | Steps Per Second = 332.345\n",
            "[Learner] Action = 2.000 | Avg Td Error = -38.227 | Q = -4.535 | Reward = -42.774 | State = 420|2|52 | Steps = 474838 | Walltime = 1388.139\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.209 | Episodes = 3289 | Steps = 9867 | Steps Per Second = 309.649\n",
            "[Learner] Action = 0.000 | Avg Td Error = -18.967 | Q = -4.496 | Reward = -21.676 | State = 450|2|50 | Steps = 475162 | Walltime = 1389.139\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.399 | Episodes = 3402 | Steps = 10206 | Steps Per Second = 375.508\n",
            "[Learner] Action = -3.000 | Avg Td Error = -35.879 | Q = -13.822 | Reward = -48.933 | State = 450|2|50 | Steps = 475522 | Walltime = 1390.142\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.220 | Episodes = 3521 | Steps = 10563 | Steps Per Second = 329.827\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.757 | Q = 0.016 | Reward = 1.860 | State = 420|5|46 | Steps = 475874 | Walltime = 1391.144\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.250 | Episodes = 3640 | Steps = 10920 | Steps Per Second = 367.256\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.754 | Q = -4.888 | Reward = 1.066 | State = 420|2|51 | Steps = 476219 | Walltime = 1392.145\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.449 | Episodes = 3753 | Steps = 11259 | Steps Per Second = 335.392\n",
            "[Learner] Action = 1.000 | Avg Td Error = -9.404 | Q = -5.461 | Reward = -14.068 | State = 450|2|50 | Steps = 476554 | Walltime = 1393.146\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.821 | Episodes = 3865 | Steps = 11595 | Steps Per Second = 324.277\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.161 | Q = -0.576 | Reward = -7.667 | State = 420|5|54 | Steps = 476887 | Walltime = 1394.148\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.256 | Episodes = 3976 | Steps = 11928 | Steps Per Second = 336.127\n",
            "[Learner] Action = -3.000 | Avg Td Error = 3.128 | Q = -6.753 | Reward = -2.246 | State = 420|2|51 | Steps = 477230 | Walltime = 1395.149\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.034 | Episodes = 4091 | Steps = 12273 | Steps Per Second = 337.036\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.211 | Q = -0.028 | Reward = 6.182 | State = 420|-1|45 | Steps = 477585 | Walltime = 1396.150\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.269 | Episodes = 4212 | Steps = 12636 | Steps Per Second = 372.783\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.407 | Q = -4.596 | Reward = 0.560 | State = 450|2|50 | Steps = 477957 | Walltime = 1397.151\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.690 | Episodes = 4335 | Steps = 13005 | Steps Per Second = 311.327\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.752 | Q = -0.426 | Reward = 1.326 | State = 390|7|47 | Steps = 478318 | Walltime = 1398.152\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -62.392 | Episodes = 4457 | Steps = 13371 | Steps Per Second = 396.362\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.083 | Q = -1.632 | Reward = -5.899 | State = 420|3|49 | Steps = 478675 | Walltime = 1399.152\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.971 | Episodes = 4576 | Steps = 13728 | Steps Per Second = 358.202\n",
            "[Learner] Action = 4.000 | Avg Td Error = -26.326 | Q = -2.427 | Reward = -28.753 | State = 390|-2|53 | Steps = 479038 | Walltime = 1400.153\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -85.952 | Episodes = 4698 | Steps = 14094 | Steps Per Second = 369.401\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.122 | Q = -0.306 | Reward = -1.428 | State = 390|4|47 | Steps = 479408 | Walltime = 1401.154\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.718 | Episodes = 4822 | Steps = 14466 | Steps Per Second = 380.804\n",
            "[Learner] Action = -3.000 | Avg Td Error = -14.247 | Q = -13.493 | Reward = -24.630 | State = 450|2|50 | Steps = 479755 | Walltime = 1402.154\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.704 | Episodes = 4936 | Steps = 14808 | Steps Per Second = 326.008\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -66.212 | Episodes = 5294 | Steps = 15882 | Steps Per Second = 1952.050\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.039 | Episodes = 5919 | Steps = 17757 | Steps Per Second = 1963.932\n",
            "[Learner] Action = 1.000 | Avg Td Error = -28.839 | Q = -1.879 | Reward = -30.718 | State = 390|-1|52 | Steps = 480000 | Walltime = 1404.445\n",
            "Check Point 32\n",
            "[Learner] Action = -2.000 | Avg Td Error = -15.661 | Q = -0.345 | Reward = -16.006 | State = 390|6|47 | Steps = 480359 | Walltime = 1405.446\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.885 | Episodes = 121 | Steps = 363 | Steps Per Second = 393.302\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.954 | Q = -0.706 | Reward = 6.248 | State = 390|2|55 | Steps = 480732 | Walltime = 1406.447\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -68.116 | Episodes = 246 | Steps = 738 | Steps Per Second = 385.069\n",
            "[Learner] Action = -4.000 | Avg Td Error = -5.340 | Q = -2.218 | Reward = -7.557 | State = 390|-4|51 | Steps = 481096 | Walltime = 1407.448\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.005 | Episodes = 367 | Steps = 1101 | Steps Per Second = 305.122\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.744 | Q = -0.170 | Reward = 2.574 | State = 390|5|54 | Steps = 481456 | Walltime = 1408.452\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -53.062 | Episodes = 487 | Steps = 1461 | Steps Per Second = 280.037\n",
            "[Learner] Action = 4.000 | Avg Td Error = -36.606 | Q = -16.441 | Reward = -53.037 | State = 450|2|50 | Steps = 481781 | Walltime = 1409.452\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.456 | Episodes = 596 | Steps = 1788 | Steps Per Second = 332.029\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.402 | Q = -0.468 | Reward = -0.870 | State = 390|0|46 | Steps = 482148 | Walltime = 1410.455\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.353 | Episodes = 719 | Steps = 2157 | Steps Per Second = 363.920\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.691 | Q = -3.410 | Reward = 1.281 | State = 390|2|51 | Steps = 482481 | Walltime = 1411.456\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.052 | Episodes = 830 | Steps = 2490 | Steps Per Second = 308.020\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.870 | Q = -1.568 | Reward = -1.961 | State = 420|1|52 | Steps = 482807 | Walltime = 1412.458\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.899 | Episodes = 939 | Steps = 2817 | Steps Per Second = 332.310\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.478 | Q = -4.795 | Reward = 2.608 | State = 450|2|50 | Steps = 483171 | Walltime = 1413.458\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.178 | Episodes = 1061 | Steps = 3183 | Steps Per Second = 392.872\n",
            "[Learner] Action = -3.000 | Avg Td Error = -3.161 | Q = -3.117 | Reward = -5.192 | State = 420|0|49 | Steps = 483502 | Walltime = 1414.459\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.530 | Episodes = 1171 | Steps = 3513 | Steps Per Second = 312.270\n",
            "[Learner] Action = 1.000 | Avg Td Error = -18.682 | Q = -2.061 | Reward = -20.348 | State = 420|2|54 | Steps = 483859 | Walltime = 1415.459\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.667 | Episodes = 1290 | Steps = 3870 | Steps Per Second = 322.597\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.997 | Q = -0.350 | Reward = -1.347 | State = 390|3|48 | Steps = 484220 | Walltime = 1416.461\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.151 | Episodes = 1411 | Steps = 4233 | Steps Per Second = 354.089\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.992 | Q = -4.502 | Reward = 4.558 | State = 450|2|50 | Steps = 484554 | Walltime = 1417.464\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.696 | Episodes = 1522 | Steps = 4566 | Steps Per Second = 301.084\n",
            "[Learner] Action = 0.000 | Avg Td Error = -15.760 | Q = -4.397 | Reward = -17.726 | State = 450|2|50 | Steps = 484921 | Walltime = 1418.464\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.259 | Episodes = 1645 | Steps = 4935 | Steps Per Second = 370.129\n",
            "[Learner] Action = 2.000 | Avg Td Error = 4.272 | Q = -4.108 | Reward = 0.293 | State = 420|2|50 | Steps = 485245 | Walltime = 1419.464\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.456 | Episodes = 1753 | Steps = 5259 | Steps Per Second = 374.068\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.488 | Q = -0.376 | Reward = -7.735 | State = 420|4|46 | Steps = 485611 | Walltime = 1420.465\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.177 | Episodes = 1876 | Steps = 5628 | Steps Per Second = 365.283\n",
            "[Learner] Action = 4.000 | Avg Td Error = -29.723 | Q = -2.080 | Reward = -31.803 | State = 390|5|50 | Steps = 485962 | Walltime = 1421.468\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.300 | Episodes = 1993 | Steps = 5979 | Steps Per Second = 393.228\n",
            "[Learner] Action = 2.000 | Avg Td Error = -8.336 | Q = -8.640 | Reward = -16.407 | State = 450|2|50 | Steps = 486296 | Walltime = 1422.470\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.636 | Episodes = 2104 | Steps = 6312 | Steps Per Second = 368.439\n",
            "[Learner] Action = -3.000 | Avg Td Error = 15.694 | Q = -14.163 | Reward = 2.274 | State = 450|2|50 | Steps = 486628 | Walltime = 1423.474\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.511 | Episodes = 2215 | Steps = 6645 | Steps Per Second = 407.438\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.542 | Q = -4.155 | Reward = 0.038 | State = 450|2|50 | Steps = 486960 | Walltime = 1424.476\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.797 | Episodes = 2326 | Steps = 6978 | Steps Per Second = 375.173\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.443 | Q = -0.818 | Reward = -6.261 | State = 390|-3|48 | Steps = 487325 | Walltime = 1425.477\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.918 | Episodes = 2448 | Steps = 7344 | Steps Per Second = 367.706\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.026 | Q = -5.578 | Reward = 4.542 | State = 450|2|50 | Steps = 487693 | Walltime = 1426.479\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.129 | Episodes = 2571 | Steps = 7713 | Steps Per Second = 387.405\n",
            "[Learner] Action = 4.000 | Avg Td Error = 9.941 | Q = -16.112 | Reward = -5.937 | State = 450|2|50 | Steps = 488057 | Walltime = 1427.480\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.555 | Episodes = 2693 | Steps = 8079 | Steps Per Second = 387.036\n",
            "[Learner] Action = 4.000 | Avg Td Error = -17.185 | Q = -1.889 | Reward = -19.074 | State = 390|4|52 | Steps = 488394 | Walltime = 1428.484\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.768 | Episodes = 2805 | Steps = 8415 | Steps Per Second = 343.955\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.410 | Q = -6.452 | Reward = 2.291 | State = 450|2|50 | Steps = 488749 | Walltime = 1429.486\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.464 | Episodes = 2923 | Steps = 8769 | Steps Per Second = 388.926\n",
            "[Learner] Action = -2.000 | Avg Td Error = -32.023 | Q = -2.593 | Reward = -34.616 | State = 390|-2|50 | Steps = 489121 | Walltime = 1430.487\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.788 | Episodes = 3047 | Steps = 9141 | Steps Per Second = 393.118\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.592 | Q = -0.359 | Reward = 1.358 | State = 420|4|50 | Steps = 489472 | Walltime = 1431.488\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.939 | Episodes = 3164 | Steps = 9492 | Steps Per Second = 356.023\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.496 | Q = -4.242 | Reward = 5.727 | State = 450|2|50 | Steps = 489830 | Walltime = 1432.490\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.024 | Episodes = 3284 | Steps = 9852 | Steps Per Second = 379.255\n",
            "[Learner] Action = -2.000 | Avg Td Error = 12.411 | Q = -10.119 | Reward = 2.817 | State = 450|2|50 | Steps = 490199 | Walltime = 1433.492\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.200 | Episodes = 3407 | Steps = 10221 | Steps Per Second = 372.915\n",
            "[Learner] Action = -4.000 | Avg Td Error = 1.868 | Q = -4.111 | Reward = -2.243 | State = 390|-1|52 | Steps = 490531 | Walltime = 1434.495\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -70.070 | Episodes = 3518 | Steps = 10554 | Steps Per Second = 254.710\n",
            "[Learner] Action = -1.000 | Avg Td Error = -22.042 | Q = -3.896 | Reward = -25.133 | State = 420|2|52 | Steps = 490898 | Walltime = 1435.496\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.260 | Episodes = 3641 | Steps = 10923 | Steps Per Second = 201.837\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.531 | Q = -0.159 | Reward = 2.519 | State = 420|5|52 | Steps = 491230 | Walltime = 1436.497\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.206 | Episodes = 3753 | Steps = 11259 | Steps Per Second = 190.080\n",
            "[Learner] Action = 3.000 | Avg Td Error = -0.603 | Q = -6.691 | Reward = -7.252 | State = 420|2|50 | Steps = 491591 | Walltime = 1437.497\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.741 | Episodes = 3874 | Steps = 11622 | Steps Per Second = 328.441\n",
            "[Learner] Action = -2.000 | Avg Td Error = 10.652 | Q = -9.968 | Reward = 1.530 | State = 450|2|50 | Steps = 491950 | Walltime = 1438.498\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.778 | Episodes = 3995 | Steps = 11985 | Steps Per Second = 388.529\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.885 | Q = -1.469 | Reward = -6.354 | State = 390|1|50 | Steps = 492318 | Walltime = 1439.498\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.707 | Episodes = 4118 | Steps = 12354 | Steps Per Second = 376.193\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.914 | Q = -2.340 | Reward = -10.254 | State = 390|2|49 | Steps = 492668 | Walltime = 1440.498\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.569 | Episodes = 4235 | Steps = 12705 | Steps Per Second = 381.185\n",
            "[Learner] Action = 4.000 | Avg Td Error = -14.630 | Q = -2.386 | Reward = -17.016 | State = 390|-2|49 | Steps = 493013 | Walltime = 1441.501\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.009 | Episodes = 4350 | Steps = 13050 | Steps Per Second = 369.933\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.600 | Q = -0.536 | Reward = -6.135 | State = 390|3|46 | Steps = 493347 | Walltime = 1442.501\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.767 | Episodes = 4462 | Steps = 13386 | Steps Per Second = 382.239\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.329 | Q = -0.568 | Reward = -2.897 | State = 390|6|50 | Steps = 493675 | Walltime = 1443.504\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.477 | Episodes = 4572 | Steps = 13716 | Steps Per Second = 373.613\n",
            "[Learner] Action = -1.000 | Avg Td Error = 8.858 | Q = -6.755 | Reward = 3.085 | State = 450|2|50 | Steps = 494044 | Walltime = 1444.505\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.487 | Episodes = 4695 | Steps = 14085 | Steps Per Second = 380.758\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.563 | Q = -1.849 | Reward = -0.140 | State = 420|-2|53 | Steps = 494410 | Walltime = 1445.507\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.677 | Episodes = 4817 | Steps = 14451 | Steps Per Second = 332.126\n",
            "[Learner] Action = -4.000 | Avg Td Error = -64.163 | Q = -18.971 | Reward = -82.737 | State = 450|2|50 | Steps = 494773 | Walltime = 1446.507\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -80.813 | Episodes = 4939 | Steps = 14817 | Steps Per Second = 350.958\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.513 | Episodes = 5309 | Steps = 15927 | Steps Per Second = 1449.978\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.378 | Episodes = 5914 | Steps = 17742 | Steps Per Second = 1705.695\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.778 | Q = -5.653 | Reward = -8.868 | State = 420|2|49 | Steps = 495000 | Walltime = 1448.774\n",
            "Check Point 33\n",
            "[Learner] Action = 4.000 | Avg Td Error = 9.043 | Q = -5.925 | Reward = 3.435 | State = 420|2|52 | Steps = 495344 | Walltime = 1449.774\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.438 | Episodes = 116 | Steps = 348 | Steps Per Second = 371.353\n",
            "[Learner] Action = 1.000 | Avg Td Error = -3.584 | Q = -3.503 | Reward = -6.283 | State = 420|2|52 | Steps = 495717 | Walltime = 1450.775\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.183 | Episodes = 241 | Steps = 723 | Steps Per Second = 393.573\n",
            "[Learner] Action = 2.000 | Avg Td Error = 12.306 | Q = -9.060 | Reward = 3.248 | State = 450|2|50 | Steps = 496086 | Walltime = 1451.778\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.368 | Episodes = 364 | Steps = 1092 | Steps Per Second = 376.689\n",
            "[Learner] Action = 4.000 | Avg Td Error = -17.424 | Q = -2.252 | Reward = -19.676 | State = 390|5|51 | Steps = 496413 | Walltime = 1452.779\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.139 | Episodes = 472 | Steps = 1416 | Steps Per Second = 178.322\n",
            "[Learner] Action = -4.000 | Avg Td Error = -29.898 | Q = -18.905 | Reward = -47.930 | State = 450|2|50 | Steps = 496747 | Walltime = 1453.781\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.030 | Episodes = 584 | Steps = 1752 | Steps Per Second = 286.405\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.010 | Q = -2.917 | Reward = 4.809 | State = 420|-2|49 | Steps = 497110 | Walltime = 1454.782\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.228 | Episodes = 706 | Steps = 2118 | Steps Per Second = 379.712\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.740 | Q = -0.498 | Reward = -10.238 | State = 390|2|44 | Steps = 497464 | Walltime = 1455.784\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.838 | Episodes = 824 | Steps = 2472 | Steps Per Second = 378.775\n",
            "[Learner] Action = -4.000 | Avg Td Error = -15.709 | Q = -2.172 | Reward = -17.880 | State = 390|5|49 | Steps = 497832 | Walltime = 1456.785\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.862 | Episodes = 947 | Steps = 2841 | Steps Per Second = 388.338\n",
            "[Learner] Action = 4.000 | Avg Td Error = -22.159 | Q = -4.839 | Reward = -26.142 | State = 420|-2|49 | Steps = 498198 | Walltime = 1457.787\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.276 | Episodes = 1069 | Steps = 3207 | Steps Per Second = 334.946\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.357 | Q = -0.044 | Reward = 0.342 | State = 420|0|45 | Steps = 498557 | Walltime = 1458.787\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.035 | Episodes = 1190 | Steps = 3570 | Steps Per Second = 396.675\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.987 | Q = -0.922 | Reward = 2.065 | State = 390|2|54 | Steps = 498910 | Walltime = 1459.788\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.773 | Episodes = 1308 | Steps = 3924 | Steps Per Second = 374.358\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.827 | Q = -0.671 | Reward = -1.884 | State = 420|4|50 | Steps = 499248 | Walltime = 1460.789\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.913 | Episodes = 1421 | Steps = 4263 | Steps Per Second = 381.474\n",
            "[Learner] Action = -4.000 | Avg Td Error = -19.820 | Q = -1.157 | Reward = -20.977 | State = 390|4|54 | Steps = 499609 | Walltime = 1461.790\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.582 | Episodes = 1542 | Steps = 4626 | Steps Per Second = 374.002\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.482 | Q = -1.784 | Reward = 5.699 | State = 390|1|52 | Steps = 499977 | Walltime = 1462.792\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.706 | Episodes = 1665 | Steps = 4995 | Steps Per Second = 392.603\n",
            "[Learner] Action = -1.000 | Avg Td Error = 9.004 | Q = -2.953 | Reward = 6.248 | State = 420|-2|49 | Steps = 500346 | Walltime = 1463.794\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.312 | Episodes = 1788 | Steps = 5364 | Steps Per Second = 355.831\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.686 | Q = -4.529 | Reward = 1.801 | State = 450|2|50 | Steps = 500709 | Walltime = 1464.795\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.680 | Episodes = 1909 | Steps = 5727 | Steps Per Second = 383.286\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.529 | Q = -15.115 | Reward = -6.510 | State = 450|2|50 | Steps = 501065 | Walltime = 1465.795\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.408 | Episodes = 2027 | Steps = 6081 | Steps Per Second = 335.053\n",
            "[Learner] Action = 2.000 | Avg Td Error = 1.337 | Q = -8.601 | Reward = -7.172 | State = 450|2|50 | Steps = 501398 | Walltime = 1466.796\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.801 | Episodes = 2140 | Steps = 6420 | Steps Per Second = 378.217\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.342 | Q = -0.012 | Reward = 0.330 | State = 390|1|44 | Steps = 501764 | Walltime = 1467.797\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.943 | Episodes = 2262 | Steps = 6786 | Steps Per Second = 360.645\n",
            "[Learner] Action = -1.000 | Avg Td Error = -58.435 | Q = -0.736 | Reward = -59.171 | State = 390|0|56 | Steps = 502127 | Walltime = 1468.798\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.386 | Episodes = 2383 | Steps = 7149 | Steps Per Second = 395.453\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.611 | Q = -0.144 | Reward = 1.467 | State = 390|3|47 | Steps = 502469 | Walltime = 1469.800\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.354 | Episodes = 2497 | Steps = 7491 | Steps Per Second = 394.795\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.688 | Q = -4.059 | Reward = 0.993 | State = 420|-2|50 | Steps = 502824 | Walltime = 1470.803\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.400 | Episodes = 2615 | Steps = 7845 | Steps Per Second = 391.759\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.731 | Q = -0.666 | Reward = 4.065 | State = 390|1|48 | Steps = 503195 | Walltime = 1471.803\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.579 | Episodes = 2738 | Steps = 8214 | Steps Per Second = 334.563\n",
            "[Learner] Action = -4.000 | Avg Td Error = 7.421 | Q = -18.941 | Reward = -8.563 | State = 450|2|50 | Steps = 503533 | Walltime = 1472.804\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -140.281 | Episodes = 2852 | Steps = 8556 | Steps Per Second = 387.190\n",
            "[Learner] Action = -3.000 | Avg Td Error = 0.065 | Q = -2.914 | Reward = -1.558 | State = 420|1|49 | Steps = 503900 | Walltime = 1473.806\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.965 | Episodes = 2974 | Steps = 8922 | Steps Per Second = 366.507\n",
            "[Learner] Action = 4.000 | Avg Td Error = -90.801 | Q = -2.588 | Reward = -93.389 | State = 390|1|49 | Steps = 504266 | Walltime = 1474.808\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.410 | Episodes = 3096 | Steps = 9288 | Steps Per Second = 374.391\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.524 | Q = -4.258 | Reward = 5.392 | State = 450|2|50 | Steps = 504632 | Walltime = 1475.809\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.366 | Episodes = 3218 | Steps = 9654 | Steps Per Second = 368.892\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.003 | Q = -0.569 | Reward = -1.435 | State = 420|1|47 | Steps = 504977 | Walltime = 1476.811\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -164.293 | Episodes = 3333 | Steps = 9999 | Steps Per Second = 395.353\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.160 | Q = -0.230 | Reward = -0.332 | State = 420|4|49 | Steps = 505343 | Walltime = 1477.813\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.361 | Episodes = 3455 | Steps = 10365 | Steps Per Second = 390.398\n",
            "[Learner] Action = 4.000 | Avg Td Error = 5.714 | Q = -15.940 | Reward = -9.959 | State = 450|2|50 | Steps = 505702 | Walltime = 1478.815\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.227 | Episodes = 3573 | Steps = 10719 | Steps Per Second = 331.129\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.242 | Q = -0.567 | Reward = -4.672 | State = 420|5|47 | Steps = 506028 | Walltime = 1479.815\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.361 | Episodes = 3685 | Steps = 11055 | Steps Per Second = 379.621\n",
            "[Learner] Action = 3.000 | Avg Td Error = 6.882 | Q = -2.385 | Reward = 4.497 | State = 390|-1|50 | Steps = 506396 | Walltime = 1480.817\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.575 | Episodes = 3807 | Steps = 11421 | Steps Per Second = 386.370\n",
            "[Learner] Action = -3.000 | Avg Td Error = 4.867 | Q = -3.169 | Reward = 1.698 | State = 390|-1|49 | Steps = 506765 | Walltime = 1481.818\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.948 | Episodes = 3931 | Steps = 11793 | Steps Per Second = 385.187\n",
            "[Learner] Action = 2.000 | Avg Td Error = 3.348 | Q = -1.291 | Reward = 2.057 | State = 390|-2|48 | Steps = 507131 | Walltime = 1482.819\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.254 | Episodes = 4052 | Steps = 12156 | Steps Per Second = 334.839\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.981 | Q = -0.664 | Reward = -10.644 | State = 390|-5|49 | Steps = 507469 | Walltime = 1483.821\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -171.506 | Episodes = 4166 | Steps = 12498 | Steps Per Second = 383.497\n",
            "[Learner] Action = 3.000 | Avg Td Error = -18.642 | Q = -1.844 | Reward = -20.486 | State = 390|4|52 | Steps = 507837 | Walltime = 1484.824\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.496 | Episodes = 4289 | Steps = 12867 | Steps Per Second = 395.055\n",
            "[Learner] Action = 2.000 | Avg Td Error = 2.893 | Q = -0.689 | Reward = 2.204 | State = 390|-2|46 | Steps = 508210 | Walltime = 1485.826\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -173.118 | Episodes = 4413 | Steps = 13239 | Steps Per Second = 384.223\n",
            "[Learner] Action = -1.000 | Avg Td Error = -27.061 | Q = -3.549 | Reward = -29.862 | State = 420|-2|51 | Steps = 508581 | Walltime = 1486.829\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.550 | Episodes = 4537 | Steps = 13611 | Steps Per Second = 385.105\n",
            "[Learner] Action = -2.000 | Avg Td Error = -35.257 | Q = -0.076 | Reward = -35.332 | State = 390|-5|46 | Steps = 508948 | Walltime = 1487.830\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.324 | Episodes = 4660 | Steps = 13980 | Steps Per Second = 375.430\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.239 | Q = -0.786 | Reward = -2.025 | State = 390|6|47 | Steps = 509318 | Walltime = 1488.832\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -101.289 | Episodes = 4783 | Steps = 14349 | Steps Per Second = 374.046\n",
            "[Learner] Action = 2.000 | Avg Td Error = 0.940 | Q = -8.286 | Reward = -7.276 | State = 450|2|50 | Steps = 509656 | Walltime = 1489.833\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.377 | Episodes = 4895 | Steps = 14685 | Steps Per Second = 333.959\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.758 | Q = -2.665 | Reward = -8.423 | State = 390|0|49 | Steps = 509997 | Walltime = 1490.834\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.271 | Episodes = 5050 | Steps = 15150 | Steps Per Second = 2026.234\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.095 | Episodes = 5689 | Steps = 17067 | Steps Per Second = 1980.002\n",
            "[Learner] Action = 2.000 | Avg Td Error = 2.891 | Q = -0.362 | Reward = 2.527 | State = 420|0|46 | Steps = 510000 | Walltime = 1492.425\n",
            "Check Point 34\n",
            "[Learner] Action = 1.000 | Avg Td Error = 2.034 | Q = -0.566 | Reward = 1.655 | State = 420|1|47 | Steps = 510345 | Walltime = 1493.425\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.688 | Episodes = 117 | Steps = 351 | Steps Per Second = 379.965\n",
            "[Learner] Action = -1.000 | Avg Td Error = 5.664 | Q = -1.281 | Reward = 6.144 | State = 420|1|53 | Steps = 510712 | Walltime = 1494.427\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.487 | Episodes = 240 | Steps = 720 | Steps Per Second = 355.068\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.369 | Q = -0.927 | Reward = -0.559 | State = 390|1|55 | Steps = 511058 | Walltime = 1495.427\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.076 | Episodes = 356 | Steps = 1068 | Steps Per Second = 314.644\n",
            "[Learner] Action = 3.000 | Avg Td Error = 7.056 | Q = -1.371 | Reward = 5.686 | State = 390|2|55 | Steps = 511408 | Walltime = 1496.428\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.237 | Episodes = 473 | Steps = 1419 | Steps Per Second = 323.427\n",
            "[Learner] Action = -1.000 | Avg Td Error = -28.251 | Q = -1.893 | Reward = -29.972 | State = 420|2|54 | Steps = 511767 | Walltime = 1497.428\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.717 | Episodes = 593 | Steps = 1779 | Steps Per Second = 385.423\n",
            "[Learner] Action = 1.000 | Avg Td Error = -10.946 | Q = -0.256 | Reward = -11.185 | State = 420|6|53 | Steps = 512119 | Walltime = 1498.429\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -53.343 | Episodes = 711 | Steps = 2133 | Steps Per Second = 373.757\n",
            "[Learner] Action = -3.000 | Avg Td Error = -2.074 | Q = -15.482 | Reward = -14.047 | State = 450|2|50 | Steps = 512484 | Walltime = 1499.432\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.097 | Episodes = 833 | Steps = 2499 | Steps Per Second = 360.666\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.178 | Q = -1.676 | Reward = -7.378 | State = 420|4|48 | Steps = 512816 | Walltime = 1500.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.291 | Episodes = 944 | Steps = 2832 | Steps Per Second = 363.374\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.794 | Q = -4.442 | Reward = 0.830 | State = 450|2|50 | Steps = 513145 | Walltime = 1501.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.123 | Episodes = 1054 | Steps = 3162 | Steps Per Second = 316.074\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.982 | Q = -8.178 | Reward = -1.978 | State = 450|2|50 | Steps = 513512 | Walltime = 1502.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.884 | Episodes = 1177 | Steps = 3531 | Steps Per Second = 382.273\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.537 | Q = -0.920 | Reward = -0.699 | State = 420|0|48 | Steps = 513863 | Walltime = 1503.436\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.538 | Episodes = 1293 | Steps = 3879 | Steps Per Second = 335.455\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.359 | Q = -4.477 | Reward = 5.066 | State = 450|2|50 | Steps = 514192 | Walltime = 1504.436\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.153 | Episodes = 1403 | Steps = 4209 | Steps Per Second = 321.288\n",
            "[Learner] Action = -3.000 | Avg Td Error = -0.172 | Q = -1.101 | Reward = 0.573 | State = 420|3|53 | Steps = 514555 | Walltime = 1505.439\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.638 | Episodes = 1525 | Steps = 4575 | Steps Per Second = 328.536\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.362 | Q = -0.336 | Reward = -1.698 | State = 390|3|46 | Steps = 514923 | Walltime = 1506.439\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.894 | Episodes = 1648 | Steps = 4944 | Steps Per Second = 367.020\n",
            "[Learner] Action = 3.000 | Avg Td Error = -1.207 | Q = -1.617 | Reward = -2.824 | State = 390|0|47 | Steps = 515289 | Walltime = 1507.442\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.422 | Episodes = 1770 | Steps = 5310 | Steps Per Second = 250.043\n",
            "[Learner] Action = -1.000 | Avg Td Error = -45.425 | Q = -6.911 | Reward = -52.031 | State = 450|2|50 | Steps = 515611 | Walltime = 1508.444\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.853 | Episodes = 1877 | Steps = 5631 | Steps Per Second = 208.682\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.199 | Q = -2.142 | Reward = 2.136 | State = 420|1|53 | Steps = 515973 | Walltime = 1509.446\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.265 | Episodes = 1999 | Steps = 5997 | Steps Per Second = 389.854\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.294 | Q = -9.831 | Reward = -6.761 | State = 450|2|50 | Steps = 516302 | Walltime = 1510.448\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.927 | Episodes = 2109 | Steps = 6327 | Steps Per Second = 388.505\n",
            "[Learner] Action = -1.000 | Avg Td Error = -61.443 | Q = -0.283 | Reward = -61.726 | State = 390|1|46 | Steps = 516660 | Walltime = 1511.449\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.702 | Episodes = 2229 | Steps = 6687 | Steps Per Second = 382.867\n",
            "[Learner] Action = -4.000 | Avg Td Error = -19.161 | Q = -0.475 | Reward = -19.636 | State = 390|-5|47 | Steps = 516997 | Walltime = 1512.449\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -84.610 | Episodes = 2342 | Steps = 7026 | Steps Per Second = 376.306\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.479 | Q = -4.335 | Reward = 5.724 | State = 450|2|50 | Steps = 517340 | Walltime = 1513.452\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.377 | Episodes = 2457 | Steps = 7371 | Steps Per Second = 390.555\n",
            "[Learner] Action = 1.000 | Avg Td Error = 8.850 | Q = -3.630 | Reward = 5.355 | State = 420|2|51 | Steps = 517674 | Walltime = 1514.454\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.651 | Episodes = 2568 | Steps = 7704 | Steps Per Second = 370.173\n",
            "[Learner] Action = 3.000 | Avg Td Error = -15.162 | Q = -13.073 | Reward = -27.887 | State = 450|2|50 | Steps = 518006 | Walltime = 1515.456\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -73.269 | Episodes = 2679 | Steps = 8037 | Steps Per Second = 356.861\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.408 | Q = -9.986 | Reward = -12.308 | State = 450|2|50 | Steps = 518376 | Walltime = 1516.457\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.663 | Episodes = 2803 | Steps = 8409 | Steps Per Second = 382.983\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.930 | Q = -2.529 | Reward = 0.269 | State = 420|2|53 | Steps = 518711 | Walltime = 1517.458\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.039 | Episodes = 2915 | Steps = 8745 | Steps Per Second = 382.587\n",
            "[Learner] Action = 4.000 | Avg Td Error = -2.030 | Q = -15.993 | Reward = -17.966 | State = 450|2|50 | Steps = 519072 | Walltime = 1518.459\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.949 | Episodes = 3036 | Steps = 9108 | Steps Per Second = 383.065\n",
            "[Learner] Action = -3.000 | Avg Td Error = -10.531 | Q = -0.409 | Reward = -10.940 | State = 390|8|48 | Steps = 519433 | Walltime = 1519.461\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.851 | Episodes = 3156 | Steps = 9468 | Steps Per Second = 362.840\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.392 | Q = -6.838 | Reward = 1.955 | State = 450|2|50 | Steps = 519783 | Walltime = 1520.462\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -127.899 | Episodes = 3274 | Steps = 9822 | Steps Per Second = 368.741\n",
            "[Learner] Action = -4.000 | Avg Td Error = -22.215 | Q = -19.463 | Reward = -39.822 | State = 450|2|50 | Steps = 520148 | Walltime = 1521.462\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.364 | Episodes = 3397 | Steps = 10191 | Steps Per Second = 389.890\n",
            "[Learner] Action = -2.000 | Avg Td Error = -1.819 | Q = -2.262 | Reward = -2.579 | State = 420|3|51 | Steps = 520511 | Walltime = 1522.465\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.160 | Episodes = 3518 | Steps = 10554 | Steps Per Second = 366.080\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.079 | Q = -0.064 | Reward = -4.521 | State = 420|3|48 | Steps = 520873 | Walltime = 1523.467\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.454 | Episodes = 3638 | Steps = 10914 | Steps Per Second = 378.650\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.659 | Q = -1.239 | Reward = -6.898 | State = 390|5|48 | Steps = 521227 | Walltime = 1524.467\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.035 | Episodes = 3758 | Steps = 11274 | Steps Per Second = 340.281\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.007 | Q = -0.063 | Reward = 1.944 | State = 390|3|48 | Steps = 521592 | Walltime = 1525.470\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.157 | Episodes = 3880 | Steps = 11640 | Steps Per Second = 381.972\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.732 | Q = -0.402 | Reward = -1.134 | State = 390|6|52 | Steps = 521948 | Walltime = 1526.471\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.949 | Episodes = 3999 | Steps = 11997 | Steps Per Second = 392.652\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.594 | Q = -0.170 | Reward = 2.451 | State = 420|5|52 | Steps = 522288 | Walltime = 1527.471\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.101 | Episodes = 4111 | Steps = 12333 | Steps Per Second = 337.633\n",
            "[Learner] Action = -4.000 | Avg Td Error = 3.451 | Q = -1.156 | Reward = 3.442 | State = 420|1|54 | Steps = 522643 | Walltime = 1528.471\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.258 | Episodes = 4231 | Steps = 12693 | Steps Per Second = 373.591\n",
            "[Learner] Action = -4.000 | Avg Td Error = -7.553 | Q = -0.693 | Reward = -8.246 | State = 390|-3|46 | Steps = 522978 | Walltime = 1529.471\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -78.421 | Episodes = 4344 | Steps = 13032 | Steps Per Second = 391.723\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.589 | Q = -2.962 | Reward = 3.693 | State = 420|2|51 | Steps = 523335 | Walltime = 1530.472\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.042 | Episodes = 4463 | Steps = 13389 | Steps Per Second = 392.113\n",
            "[Learner] Action = 3.000 | Avg Td Error = -0.675 | Q = -2.508 | Reward = -3.052 | State = 420|0|48 | Steps = 523702 | Walltime = 1531.473\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.742 | Episodes = 4586 | Steps = 13758 | Steps Per Second = 377.027\n",
            "[Learner] Action = 3.000 | Avg Td Error = -33.629 | Q = -12.767 | Reward = -46.226 | State = 450|2|50 | Steps = 524051 | Walltime = 1532.473\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.916 | Episodes = 4703 | Steps = 14109 | Steps Per Second = 375.733\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.083 | Q = -4.144 | Reward = 6.092 | State = 450|2|50 | Steps = 524403 | Walltime = 1533.475\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.509 | Episodes = 4820 | Steps = 14460 | Steps Per Second = 374.592\n",
            "[Learner] Action = 0.000 | Avg Td Error = -19.000 | Q = -1.545 | Reward = -20.546 | State = 390|-1|50 | Steps = 524760 | Walltime = 1534.476\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.006 | Episodes = 4939 | Steps = 14817 | Steps Per Second = 377.820\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.906 | Episodes = 5311 | Steps = 15933 | Steps Per Second = 1974.100\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.281 | Episodes = 5902 | Steps = 17706 | Steps Per Second = 1999.509\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.915 | Q = -4.043 | Reward = -2.988 | State = 450|2|50 | Steps = 525000 | Walltime = 1536.797\n",
            "Check Point 35\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.750 | Q = -0.661 | Reward = 3.089 | State = 390|3|51 | Steps = 525329 | Walltime = 1537.798\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -96.311 | Episodes = 111 | Steps = 333 | Steps Per Second = 364.627\n",
            "[Learner] Action = -4.000 | Avg Td Error = -10.271 | Q = -19.567 | Reward = -26.176 | State = 450|2|50 | Steps = 525663 | Walltime = 1538.799\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.394 | Episodes = 223 | Steps = 669 | Steps Per Second = 388.038\n",
            "[Learner] Action = 3.000 | Avg Td Error = 10.662 | Q = -12.752 | Reward = -2.065 | State = 450|2|50 | Steps = 526026 | Walltime = 1539.799\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.956 | Episodes = 345 | Steps = 1035 | Steps Per Second = 378.468\n",
            "[Learner] Action = 4.000 | Avg Td Error = -11.089 | Q = -6.264 | Reward = -17.252 | State = 420|2|52 | Steps = 526381 | Walltime = 1540.799\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.719 | Episodes = 463 | Steps = 1389 | Steps Per Second = 339.785\n",
            "[Learner] Action = -4.000 | Avg Td Error = -4.140 | Q = -1.970 | Reward = -6.110 | State = 390|-3|50 | Steps = 526730 | Walltime = 1541.801\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.095 | Episodes = 579 | Steps = 1737 | Steps Per Second = 363.552\n",
            "[Learner] Action = -2.000 | Avg Td Error = -1.772 | Q = -4.708 | Reward = -4.638 | State = 420|2|51 | Steps = 527095 | Walltime = 1542.803\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.859 | Episodes = 702 | Steps = 2106 | Steps Per Second = 370.576\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.445 | Q = -8.212 | Reward = -1.549 | State = 450|2|50 | Steps = 527439 | Walltime = 1543.805\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.325 | Episodes = 817 | Steps = 2451 | Steps Per Second = 384.987\n",
            "[Learner] Action = 3.000 | Avg Td Error = -4.580 | Q = -0.646 | Reward = -5.200 | State = 420|4|54 | Steps = 527806 | Walltime = 1544.805\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -53.239 | Episodes = 940 | Steps = 2820 | Steps Per Second = 379.781\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.296 | Q = -3.018 | Reward = 5.060 | State = 420|2|51 | Steps = 528175 | Walltime = 1545.807\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.486 | Episodes = 1063 | Steps = 3189 | Steps Per Second = 379.770\n",
            "[Learner] Action = -2.000 | Avg Td Error = -9.483 | Q = -0.249 | Reward = -9.722 | State = 420|6|54 | Steps = 528521 | Walltime = 1546.808\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.029 | Episodes = 1179 | Steps = 3537 | Steps Per Second = 329.793\n",
            "[Learner] Action = 0.000 | Avg Td Error = -19.515 | Q = -4.254 | Reward = -21.871 | State = 450|2|50 | Steps = 528889 | Walltime = 1547.808\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.979 | Episodes = 1302 | Steps = 3906 | Steps Per Second = 375.564\n",
            "[Learner] Action = -2.000 | Avg Td Error = -16.473 | Q = -10.146 | Reward = -25.035 | State = 450|2|50 | Steps = 529261 | Walltime = 1548.809\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -203.181 | Episodes = 1426 | Steps = 4278 | Steps Per Second = 372.000\n",
            "[Learner] Action = -4.000 | Avg Td Error = 5.877 | Q = -19.627 | Reward = -9.591 | State = 450|2|50 | Steps = 529626 | Walltime = 1549.810\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.198 | Episodes = 1548 | Steps = 4644 | Steps Per Second = 341.528\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.916 | Q = -0.988 | Reward = -4.904 | State = 390|5|50 | Steps = 529988 | Walltime = 1550.813\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.184 | Episodes = 1669 | Steps = 5007 | Steps Per Second = 384.704\n",
            "[Learner] Action = -2.000 | Avg Td Error = -23.341 | Q = -1.579 | Reward = -24.126 | State = 420|0|53 | Steps = 530351 | Walltime = 1551.813\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.963 | Episodes = 1791 | Steps = 5373 | Steps Per Second = 379.598\n",
            "[Learner] Action = 3.000 | Avg Td Error = -96.485 | Q = -7.278 | Reward = -103.761 | State = 420|2|51 | Steps = 530716 | Walltime = 1552.814\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.489 | Episodes = 1912 | Steps = 5736 | Steps Per Second = 344.002\n",
            "[Learner] Action = 3.000 | Avg Td Error = -45.214 | Q = -12.887 | Reward = -57.991 | State = 450|2|50 | Steps = 531082 | Walltime = 1553.817\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.592 | Episodes = 2034 | Steps = 6102 | Steps Per Second = 333.764\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.419 | Q = -15.077 | Reward = -17.858 | State = 450|2|50 | Steps = 531438 | Walltime = 1554.820\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.149 | Episodes = 2154 | Steps = 6462 | Steps Per Second = 363.952\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.331 | Q = -0.288 | Reward = 4.042 | State = 390|1|46 | Steps = 531805 | Walltime = 1555.822\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.164 | Episodes = 2277 | Steps = 6831 | Steps Per Second = 378.969\n",
            "[Learner] Action = -4.000 | Avg Td Error = -11.124 | Q = -3.982 | Reward = -13.098 | State = 420|3|50 | Steps = 532168 | Walltime = 1556.823\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.533 | Episodes = 2398 | Steps = 7194 | Steps Per Second = 384.070\n",
            "[Learner] Action = -2.000 | Avg Td Error = -1.446 | Q = -1.210 | Reward = -2.656 | State = 390|-2|54 | Steps = 532534 | Walltime = 1557.823\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.005 | Episodes = 2520 | Steps = 7560 | Steps Per Second = 367.427\n",
            "[Learner] Action = -2.000 | Avg Td Error = -0.671 | Q = -1.705 | Reward = -2.186 | State = 420|4|50 | Steps = 532897 | Walltime = 1558.823\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.727 | Episodes = 2642 | Steps = 7926 | Steps Per Second = 381.428\n",
            "[Learner] Action = -4.000 | Avg Td Error = 17.404 | Q = -19.545 | Reward = -1.033 | State = 450|2|50 | Steps = 533243 | Walltime = 1559.824\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.837 | Episodes = 2757 | Steps = 8271 | Steps Per Second = 389.202\n",
            "[Learner] Action = -1.000 | Avg Td Error = -5.717 | Q = -6.596 | Reward = -10.501 | State = 450|2|50 | Steps = 533612 | Walltime = 1560.828\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.609 | Episodes = 2879 | Steps = 8637 | Steps Per Second = 387.811\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.744 | Q = -0.040 | Reward = 2.382 | State = 420|4|56 | Steps = 533980 | Walltime = 1561.831\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.774 | Episodes = 3003 | Steps = 9009 | Steps Per Second = 386.489\n",
            "[Learner] Action = -4.000 | Avg Td Error = 9.190 | Q = -19.379 | Reward = -7.143 | State = 450|2|50 | Steps = 534325 | Walltime = 1562.833\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.117 | Episodes = 3118 | Steps = 9354 | Steps Per Second = 386.667\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.628 | Q = -0.284 | Reward = 4.344 | State = 390|1|46 | Steps = 534688 | Walltime = 1563.833\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.703 | Episodes = 3239 | Steps = 9717 | Steps Per Second = 339.226\n",
            "[Learner] Action = 4.000 | Avg Td Error = -19.501 | Q = -1.872 | Reward = -21.373 | State = 390|3|53 | Steps = 535049 | Walltime = 1564.834\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.014 | Episodes = 3359 | Steps = 10077 | Steps Per Second = 374.113\n",
            "[Learner] Action = -4.000 | Avg Td Error = -18.199 | Q = -3.405 | Reward = -20.797 | State = 420|0|50 | Steps = 535411 | Walltime = 1565.836\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.824 | Episodes = 3480 | Steps = 10440 | Steps Per Second = 378.593\n",
            "[Learner] Action = 4.000 | Avg Td Error = -1.099 | Q = -16.390 | Reward = -17.411 | State = 450|2|50 | Steps = 535781 | Walltime = 1566.837\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -55.333 | Episodes = 3603 | Steps = 10809 | Steps Per Second = 355.279\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.600 | Q = -0.951 | Reward = -8.551 | State = 390|4|46 | Steps = 536136 | Walltime = 1567.837\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.873 | Episodes = 3722 | Steps = 11166 | Steps Per Second = 365.092\n",
            "[Learner] Action = 0.000 | Avg Td Error = -41.252 | Q = -2.581 | Reward = -43.730 | State = 420|2|53 | Steps = 536485 | Walltime = 1568.839\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.669 | Episodes = 3839 | Steps = 11517 | Steps Per Second = 391.272\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.212 | Q = -1.243 | Reward = -4.275 | State = 420|4|49 | Steps = 536854 | Walltime = 1569.842\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.704 | Episodes = 3961 | Steps = 11883 | Steps Per Second = 313.867\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.188 | Q = -0.344 | Reward = -4.531 | State = 390|8|53 | Steps = 537188 | Walltime = 1570.843\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.151 | Episodes = 4074 | Steps = 12222 | Steps Per Second = 309.733\n",
            "[Learner] Action = 3.000 | Avg Td Error = -1.438 | Q = -3.364 | Reward = -4.616 | State = 420|1|50 | Steps = 537416 | Walltime = 1572.557\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.461 | Episodes = 4139 | Steps = 12417 | Steps Per Second = 2.962\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.223 | Q = -0.763 | Reward = -6.987 | State = 390|6|48 | Steps = 537718 | Walltime = 1573.558\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.685 | Episodes = 4240 | Steps = 12720 | Steps Per Second = 326.067\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.216 | Q = -1.013 | Reward = 5.232 | State = 420|0|48 | Steps = 538056 | Walltime = 1574.561\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.086 | Episodes = 4353 | Steps = 13059 | Steps Per Second = 322.879\n",
            "[Learner] Action = -4.000 | Avg Td Error = 4.872 | Q = -1.595 | Reward = 3.277 | State = 390|0|54 | Steps = 538371 | Walltime = 1575.563\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.810 | Episodes = 4458 | Steps = 13374 | Steps Per Second = 318.506\n",
            "[Learner] Action = 1.000 | Avg Td Error = 3.022 | Q = -5.731 | Reward = -1.853 | State = 450|2|50 | Steps = 538720 | Walltime = 1576.563\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.022 | Episodes = 4575 | Steps = 13725 | Steps Per Second = 378.081\n",
            "[Learner] Action = -2.000 | Avg Td Error = -2.534 | Q = -1.495 | Reward = -4.029 | State = 390|3|50 | Steps = 539031 | Walltime = 1577.564\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.747 | Episodes = 4679 | Steps = 14037 | Steps Per Second = 324.118\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.134 | Q = -0.701 | Reward = 5.433 | State = 390|-5|49 | Steps = 539390 | Walltime = 1578.565\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -83.213 | Episodes = 4799 | Steps = 14397 | Steps Per Second = 293.021\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.571 | Q = -0.825 | Reward = -2.827 | State = 420|4|49 | Steps = 539718 | Walltime = 1579.568\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.277 | Episodes = 4909 | Steps = 14727 | Steps Per Second = 385.825\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.052 | Episodes = 5103 | Steps = 15309 | Steps Per Second = 1943.307\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.628 | Episodes = 5713 | Steps = 17139 | Steps Per Second = 1874.968\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.867 | Q = -4.418 | Reward = -5.688 | State = 450|2|50 | Steps = 540000 | Walltime = 1582.061\n",
            "Check Point 36\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.142 | Q = -2.224 | Reward = 4.918 | State = 390|-2|51 | Steps = 540358 | Walltime = 1583.064\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.579 | Episodes = 123 | Steps = 369 | Steps Per Second = 386.833\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.850 | Q = -1.429 | Reward = -5.279 | State = 390|2|51 | Steps = 540697 | Walltime = 1584.067\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.322 | Episodes = 236 | Steps = 708 | Steps Per Second = 326.990\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.353 | Q = -12.895 | Reward = -8.483 | State = 450|2|50 | Steps = 541032 | Walltime = 1585.069\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.729 | Episodes = 347 | Steps = 1041 | Steps Per Second = 231.926\n",
            "[Learner] Action = -1.000 | Avg Td Error = -24.748 | Q = -0.061 | Reward = -24.809 | State = 390|1|44 | Steps = 541378 | Walltime = 1586.072\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.394 | Episodes = 465 | Steps = 1395 | Steps Per Second = 374.135\n",
            "[Learner] Action = 0.000 | Avg Td Error = -17.063 | Q = -4.562 | Reward = -19.028 | State = 450|2|50 | Steps = 541737 | Walltime = 1587.073\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.451 | Episodes = 583 | Steps = 1749 | Steps Per Second = 347.681\n",
            "[Learner] Action = 4.000 | Avg Td Error = -36.310 | Q = -5.034 | Reward = -40.355 | State = 420|-2|52 | Steps = 542088 | Walltime = 1588.075\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.697 | Episodes = 701 | Steps = 2103 | Steps Per Second = 337.181\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.049 | Q = -2.620 | Reward = 3.280 | State = 420|-1|49 | Steps = 542419 | Walltime = 1589.076\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.281 | Episodes = 812 | Steps = 2436 | Steps Per Second = 329.646\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.577 | Q = -1.334 | Reward = 6.243 | State = 390|-2|48 | Steps = 542767 | Walltime = 1590.077\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.241 | Episodes = 929 | Steps = 2787 | Steps Per Second = 384.940\n",
            "[Learner] Action = -3.000 | Avg Td Error = -29.496 | Q = -1.295 | Reward = -30.598 | State = 420|-1|54 | Steps = 543103 | Walltime = 1591.078\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.353 | Episodes = 1040 | Steps = 3120 | Steps Per Second = 317.294\n",
            "[Learner] Action = 3.000 | Avg Td Error = -57.695 | Q = -4.140 | Reward = -60.340 | State = 420|-1|50 | Steps = 543438 | Walltime = 1592.080\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.142 | Episodes = 1153 | Steps = 3459 | Steps Per Second = 327.962\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.596 | Q = -0.974 | Reward = -6.570 | State = 390|4|46 | Steps = 543790 | Walltime = 1593.081\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.415 | Episodes = 1270 | Steps = 3810 | Steps Per Second = 299.807\n",
            "[Learner] Action = 3.000 | Avg Td Error = 3.882 | Q = -12.789 | Reward = -8.841 | State = 450|2|50 | Steps = 544132 | Walltime = 1594.083\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.928 | Episodes = 1386 | Steps = 4158 | Steps Per Second = 366.731\n",
            "[Learner] Action = -3.000 | Avg Td Error = 1.620 | Q = -7.340 | Reward = -4.058 | State = 420|2|50 | Steps = 544498 | Walltime = 1595.084\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.935 | Episodes = 1508 | Steps = 4524 | Steps Per Second = 371.583\n",
            "[Learner] Action = 4.000 | Avg Td Error = -10.729 | Q = -2.161 | Reward = -12.650 | State = 420|5|48 | Steps = 544853 | Walltime = 1596.084\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -76.578 | Episodes = 1626 | Steps = 4878 | Steps Per Second = 334.750\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.377 | Q = -2.967 | Reward = 4.411 | State = 390|-1|50 | Steps = 545190 | Walltime = 1597.086\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.048 | Episodes = 1740 | Steps = 5220 | Steps Per Second = 376.036\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.266 | Q = -1.348 | Reward = 5.917 | State = 390|-2|48 | Steps = 545530 | Walltime = 1598.086\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.681 | Episodes = 1852 | Steps = 5556 | Steps Per Second = 318.740\n",
            "[Learner] Action = -1.000 | Avg Td Error = -26.158 | Q = -3.785 | Reward = -28.867 | State = 420|2|50 | Steps = 545874 | Walltime = 1599.088\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -92.493 | Episodes = 1968 | Steps = 5904 | Steps Per Second = 374.893\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.376 | Q = -0.648 | Reward = -0.187 | State = 420|4|52 | Steps = 546228 | Walltime = 1600.090\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -64.742 | Episodes = 2087 | Steps = 6261 | Steps Per Second = 375.094\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.419 | Q = -4.322 | Reward = 3.207 | State = 450|2|50 | Steps = 546584 | Walltime = 1601.091\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.536 | Episodes = 2205 | Steps = 6615 | Steps Per Second = 339.867\n",
            "[Learner] Action = -4.000 | Avg Td Error = -7.948 | Q = -3.513 | Reward = -10.468 | State = 420|6|49 | Steps = 546926 | Walltime = 1602.091\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.235 | Episodes = 2320 | Steps = 6960 | Steps Per Second = 322.193\n",
            "[Learner] Action = -4.000 | Avg Td Error = -19.610 | Q = -2.257 | Reward = -21.867 | State = 390|4|49 | Steps = 547284 | Walltime = 1603.092\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.875 | Episodes = 2440 | Steps = 7320 | Steps Per Second = 384.857\n",
            "[Learner] Action = -3.000 | Avg Td Error = 9.453 | Q = -3.986 | Reward = 6.203 | State = 420|-2|52 | Steps = 547656 | Walltime = 1604.093\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.995 | Episodes = 2565 | Steps = 7695 | Steps Per Second = 398.357\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.670 | Q = -2.667 | Reward = 3.755 | State = 420|2|53 | Steps = 548022 | Walltime = 1605.093\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.004 | Episodes = 2687 | Steps = 8061 | Steps Per Second = 377.945\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.402 | Q = -0.736 | Reward = -0.334 | State = 390|-5|50 | Steps = 548376 | Walltime = 1606.095\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.736 | Episodes = 2804 | Steps = 8412 | Steps Per Second = 335.133\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.644 | Q = -4.986 | Reward = -3.142 | State = 450|2|50 | Steps = 548728 | Walltime = 1607.096\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -53.118 | Episodes = 2924 | Steps = 8772 | Steps Per Second = 377.084\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.943 | Q = -1.875 | Reward = -9.819 | State = 390|5|52 | Steps = 549087 | Walltime = 1608.097\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.155 | Episodes = 3044 | Steps = 9132 | Steps Per Second = 369.814\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.160 | Q = -8.882 | Reward = 1.403 | State = 450|2|50 | Steps = 549451 | Walltime = 1609.098\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.036 | Episodes = 3166 | Steps = 9498 | Steps Per Second = 392.811\n",
            "[Learner] Action = -2.000 | Avg Td Error = 11.991 | Q = -9.825 | Reward = 2.780 | State = 450|2|50 | Steps = 549811 | Walltime = 1610.099\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.241 | Episodes = 3286 | Steps = 9858 | Steps Per Second = 353.125\n",
            "[Learner] Action = -3.000 | Avg Td Error = 1.153 | Q = -14.917 | Reward = -10.223 | State = 450|2|50 | Steps = 550167 | Walltime = 1611.099\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.003 | Episodes = 3405 | Steps = 10215 | Steps Per Second = 302.176\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.243 | Q = -0.703 | Reward = -0.728 | State = 420|6|50 | Steps = 550497 | Walltime = 1612.100\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.087 | Episodes = 3514 | Steps = 10542 | Steps Per Second = 328.381\n",
            "[Learner] Action = -1.000 | Avg Td Error = -5.904 | Q = -1.529 | Reward = -7.433 | State = 390|0|49 | Steps = 550846 | Walltime = 1613.102\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.433 | Episodes = 3633 | Steps = 10899 | Steps Per Second = 371.407\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.797 | Q = -4.873 | Reward = 5.542 | State = 450|2|50 | Steps = 551212 | Walltime = 1614.104\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.507 | Episodes = 3755 | Steps = 11265 | Steps Per Second = 382.099\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.563 | Q = -2.419 | Reward = 3.049 | State = 420|2|50 | Steps = 551577 | Walltime = 1615.106\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.490 | Episodes = 3877 | Steps = 11631 | Steps Per Second = 388.757\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.034 | Q = -12.411 | Reward = -8.312 | State = 450|2|50 | Steps = 551939 | Walltime = 1616.106\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.221 | Episodes = 3997 | Steps = 11991 | Steps Per Second = 322.937\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.404 | Q = -4.839 | Reward = -7.470 | State = 450|2|50 | Steps = 552272 | Walltime = 1617.109\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -82.702 | Episodes = 4107 | Steps = 12321 | Steps Per Second = 337.136\n",
            "[Learner] Action = 0.000 | Avg Td Error = -41.010 | Q = -0.342 | Reward = -41.353 | State = 390|9|51 | Steps = 552622 | Walltime = 1618.111\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.741 | Episodes = 4226 | Steps = 12678 | Steps Per Second = 378.650\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.389 | Q = -1.481 | Reward = 2.908 | State = 390|2|51 | Steps = 552970 | Walltime = 1619.112\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.514 | Episodes = 4340 | Steps = 13020 | Steps Per Second = 296.984\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.411 | Q = -1.897 | Reward = -0.486 | State = 390|-1|53 | Steps = 553302 | Walltime = 1620.112\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.347 | Episodes = 4452 | Steps = 13356 | Steps Per Second = 317.646\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.266 | Q = -0.599 | Reward = -1.865 | State = 390|4|50 | Steps = 553632 | Walltime = 1621.113\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.470 | Episodes = 4562 | Steps = 13686 | Steps Per Second = 325.906\n",
            "[Learner] Action = 1.000 | Avg Td Error = -27.283 | Q = -2.279 | Reward = -29.165 | State = 420|2|54 | Steps = 553970 | Walltime = 1622.114\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.648 | Episodes = 4678 | Steps = 14034 | Steps Per Second = 376.441\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.526 | Q = -3.755 | Reward = 5.948 | State = 420|2|52 | Steps = 554337 | Walltime = 1623.116\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.369 | Episodes = 4800 | Steps = 14400 | Steps Per Second = 377.525\n",
            "[Learner] Action = 4.000 | Avg Td Error = -10.628 | Q = -3.079 | Reward = -13.533 | State = 420|3|49 | Steps = 554690 | Walltime = 1624.117\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.758 | Episodes = 4916 | Steps = 14748 | Steps Per Second = 328.304\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.237 | Episodes = 5189 | Steps = 15567 | Steps Per Second = 1975.339\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.332 | Episodes = 5815 | Steps = 17445 | Steps Per Second = 1999.192\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.848 | Q = -6.331 | Reward = -7.707 | State = 450|2|50 | Steps = 555000 | Walltime = 1626.606\n",
            "Check Point 37\n",
            "[Learner] Action = 3.000 | Avg Td Error = -21.733 | Q = -1.430 | Reward = -22.190 | State = 420|-1|47 | Steps = 555359 | Walltime = 1627.607\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.673 | Episodes = 121 | Steps = 363 | Steps Per Second = 380.931\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.856 | Q = -1.447 | Reward = 1.409 | State = 390|1|53 | Steps = 555685 | Walltime = 1628.607\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.543 | Episodes = 230 | Steps = 690 | Steps Per Second = 359.584\n",
            "[Learner] Action = 3.000 | Avg Td Error = 5.134 | Q = -7.537 | Reward = -2.355 | State = 420|2|51 | Steps = 556048 | Walltime = 1629.609\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.253 | Episodes = 351 | Steps = 1053 | Steps Per Second = 383.953\n",
            "[Learner] Action = -4.000 | Avg Td Error = -125.708 | Q = -3.897 | Reward = -129.562 | State = 420|1|52 | Steps = 556417 | Walltime = 1630.609\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.276 | Episodes = 474 | Steps = 1422 | Steps Per Second = 381.035\n",
            "[Learner] Action = -3.000 | Avg Td Error = -10.226 | Q = -15.010 | Reward = -21.695 | State = 450|2|50 | Steps = 556786 | Walltime = 1631.613\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.149 | Episodes = 596 | Steps = 1788 | Steps Per Second = 190.008\n",
            "[Learner] Action = 2.000 | Avg Td Error = 9.715 | Q = -5.093 | Reward = 5.620 | State = 420|-2|50 | Steps = 557146 | Walltime = 1632.614\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.799 | Episodes = 717 | Steps = 2151 | Steps Per Second = 368.083\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.614 | Q = -4.906 | Reward = -2.841 | State = 450|2|50 | Steps = 557476 | Walltime = 1633.616\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.811 | Episodes = 827 | Steps = 2481 | Steps Per Second = 363.363\n",
            "[Learner] Action = -4.000 | Avg Td Error = -5.300 | Q = -1.344 | Reward = -6.645 | State = 390|3|54 | Steps = 557845 | Walltime = 1634.617\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.638 | Episodes = 950 | Steps = 2850 | Steps Per Second = 324.720\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.578 | Q = -9.338 | Reward = 1.382 | State = 450|2|50 | Steps = 558172 | Walltime = 1635.619\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.391 | Episodes = 1059 | Steps = 3177 | Steps Per Second = 290.968\n",
            "[Learner] Action = -3.000 | Avg Td Error = 1.737 | Q = -2.799 | Reward = -1.062 | State = 390|2|54 | Steps = 558501 | Walltime = 1636.621\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.369 | Episodes = 1169 | Steps = 3507 | Steps Per Second = 329.120\n",
            "[Learner] Action = -4.000 | Avg Td Error = 20.237 | Q = -19.842 | Reward = 0.790 | State = 450|2|50 | Steps = 558826 | Walltime = 1637.623\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.791 | Episodes = 1278 | Steps = 3834 | Steps Per Second = 368.061\n",
            "[Learner] Action = 3.000 | Avg Td Error = 3.999 | Q = -5.597 | Reward = -1.532 | State = 420|2|49 | Steps = 559200 | Walltime = 1638.623\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.408 | Episodes = 1403 | Steps = 4209 | Steps Per Second = 378.593\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.782 | Q = -4.738 | Reward = 4.109 | State = 450|2|50 | Steps = 559572 | Walltime = 1639.626\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.895 | Episodes = 1527 | Steps = 4581 | Steps Per Second = 267.017\n",
            "[Learner] Action = 4.000 | Avg Td Error = 7.137 | Q = -16.872 | Reward = -9.498 | State = 450|2|50 | Steps = 559935 | Walltime = 1640.626\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.757 | Episodes = 1649 | Steps = 4947 | Steps Per Second = 333.075\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.504 | Q = -15.143 | Reward = -5.080 | State = 450|2|50 | Steps = 560257 | Walltime = 1641.626\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.954 | Episodes = 1757 | Steps = 5271 | Steps Per Second = 357.205\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.316 | Q = -0.674 | Reward = 1.643 | State = 390|4|52 | Steps = 560616 | Walltime = 1642.627\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.831 | Episodes = 1876 | Steps = 5628 | Steps Per Second = 313.749\n",
            "[Learner] Action = -1.000 | Avg Td Error = 12.404 | Q = -6.442 | Reward = 5.964 | State = 450|2|50 | Steps = 560982 | Walltime = 1643.628\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.089 | Episodes = 1999 | Steps = 5997 | Steps Per Second = 327.723\n",
            "[Learner] Action = 3.000 | Avg Td Error = -50.356 | Q = -3.032 | Reward = -53.388 | State = 390|0|52 | Steps = 561303 | Walltime = 1644.629\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.548 | Episodes = 2107 | Steps = 6321 | Steps Per Second = 360.057\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.369 | Q = -1.006 | Reward = -2.375 | State = 390|2|49 | Steps = 561668 | Walltime = 1645.631\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.335 | Episodes = 2229 | Steps = 6687 | Steps Per Second = 361.536\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.794 | Q = -2.191 | Reward = -7.985 | State = 390|3|48 | Steps = 562039 | Walltime = 1646.632\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.514 | Episodes = 2353 | Steps = 7059 | Steps Per Second = 382.587\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.364 | Q = -4.495 | Reward = 2.380 | State = 450|2|50 | Steps = 562405 | Walltime = 1647.634\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.435 | Episodes = 2475 | Steps = 7425 | Steps Per Second = 369.596\n",
            "[Learner] Action = 2.000 | Avg Td Error = -21.855 | Q = -0.373 | Reward = -22.228 | State = 390|-6|48 | Steps = 562767 | Walltime = 1648.636\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -70.319 | Episodes = 2596 | Steps = 7788 | Steps Per Second = 361.682\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.481 | Q = -2.135 | Reward = 5.997 | State = 420|0|49 | Steps = 563113 | Walltime = 1649.638\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.916 | Episodes = 2711 | Steps = 8133 | Steps Per Second = 390.192\n",
            "[Learner] Action = 1.000 | Avg Td Error = -9.250 | Q = -5.956 | Reward = -14.249 | State = 450|2|50 | Steps = 563481 | Walltime = 1650.641\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.898 | Episodes = 2834 | Steps = 8502 | Steps Per Second = 380.102\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.007 | Q = -3.394 | Reward = 5.187 | State = 420|0|50 | Steps = 563818 | Walltime = 1651.641\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -70.207 | Episodes = 2947 | Steps = 8841 | Steps Per Second = 396.462\n",
            "[Learner] Action = -3.000 | Avg Td Error = 11.686 | Q = -14.886 | Reward = -1.291 | State = 450|2|50 | Steps = 564162 | Walltime = 1652.641\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.098 | Episodes = 3062 | Steps = 9186 | Steps Per Second = 384.141\n",
            "[Learner] Action = 3.000 | Avg Td Error = 7.158 | Q = -1.965 | Reward = 5.194 | State = 390|-1|54 | Steps = 564495 | Walltime = 1653.644\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.872 | Episodes = 3173 | Steps = 9519 | Steps Per Second = 387.012\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.379 | Q = -4.116 | Reward = 3.413 | State = 450|2|50 | Steps = 564858 | Walltime = 1654.645\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.430 | Episodes = 3294 | Steps = 9882 | Steps Per Second = 378.741\n",
            "[Learner] Action = 2.000 | Avg Td Error = -20.975 | Q = -3.289 | Reward = -23.832 | State = 420|2|48 | Steps = 565224 | Walltime = 1655.648\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.576 | Episodes = 3416 | Steps = 10248 | Steps Per Second = 317.590\n",
            "[Learner] Action = 0.000 | Avg Td Error = -71.405 | Q = -2.342 | Reward = -73.747 | State = 390|-2|52 | Steps = 565560 | Walltime = 1656.648\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.280 | Episodes = 3529 | Steps = 10587 | Steps Per Second = 390.253\n",
            "[Learner] Action = 4.000 | Avg Td Error = 6.268 | Q = -17.036 | Reward = -10.534 | State = 450|2|50 | Steps = 565917 | Walltime = 1657.651\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.752 | Episodes = 3649 | Steps = 10947 | Steps Per Second = 370.871\n",
            "[Learner] Action = -4.000 | Avg Td Error = 9.079 | Q = -19.818 | Reward = -7.530 | State = 450|2|50 | Steps = 566256 | Walltime = 1658.652\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -59.303 | Episodes = 3762 | Steps = 11286 | Steps Per Second = 360.387\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.753 | Q = -0.860 | Reward = -8.613 | State = 390|3|46 | Steps = 566590 | Walltime = 1659.653\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.297 | Episodes = 3873 | Steps = 11619 | Steps Per Second = 327.007\n",
            "[Learner] Action = 3.000 | Avg Td Error = 10.421 | Q = -4.422 | Reward = 6.180 | State = 420|-1|50 | Steps = 566918 | Walltime = 1660.653\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -91.328 | Episodes = 3983 | Steps = 11949 | Steps Per Second = 375.598\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.462 | Q = -4.317 | Reward = -5.020 | State = 420|-1|51 | Steps = 567291 | Walltime = 1661.654\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.032 | Episodes = 4108 | Steps = 12324 | Steps Per Second = 379.152\n",
            "[Learner] Action = -4.000 | Avg Td Error = -4.093 | Q = -0.798 | Reward = -4.904 | State = 420|1|46 | Steps = 567647 | Walltime = 1662.654\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.032 | Episodes = 4226 | Steps = 12678 | Steps Per Second = 317.462\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.371 | Q = -5.745 | Reward = 4.750 | State = 450|2|50 | Steps = 567975 | Walltime = 1663.657\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -127.013 | Episodes = 4337 | Steps = 13011 | Steps Per Second = 387.333\n",
            "[Learner] Action = -4.000 | Avg Td Error = -8.757 | Q = -2.442 | Reward = -11.198 | State = 390|4|49 | Steps = 568342 | Walltime = 1664.659\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -67.867 | Episodes = 4459 | Steps = 13377 | Steps Per Second = 385.541\n",
            "[Learner] Action = -4.000 | Avg Td Error = -16.128 | Q = -3.042 | Reward = -16.915 | State = 420|4|51 | Steps = 568678 | Walltime = 1665.662\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.360 | Episodes = 4571 | Steps = 13713 | Steps Per Second = 399.394\n",
            "[Learner] Action = 1.000 | Avg Td Error = -9.201 | Q = -0.837 | Reward = -9.819 | State = 420|4|52 | Steps = 569042 | Walltime = 1666.664\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.521 | Episodes = 4691 | Steps = 14073 | Steps Per Second = 330.581\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.943 | Q = -0.442 | Reward = -1.385 | State = 390|6|52 | Steps = 569372 | Walltime = 1667.666\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.702 | Episodes = 4802 | Steps = 14406 | Steps Per Second = 324.620\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.774 | Q = -0.937 | Reward = -8.050 | State = 420|3|52 | Steps = 569709 | Walltime = 1668.669\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -181.597 | Episodes = 4915 | Steps = 14745 | Steps Per Second = 347.374\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.164 | Episodes = 5184 | Steps = 15552 | Steps Per Second = 1950.537\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.696 | Episodes = 5780 | Steps = 17340 | Steps Per Second = 2012.300\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.819 | Q = -4.600 | Reward = 6.247 | State = 450|2|50 | Steps = 570000 | Walltime = 1671.128\n",
            "Check Point 38\n",
            "[Learner] Action = 3.000 | Avg Td Error = 1.752 | Q = -3.950 | Reward = -2.198 | State = 390|2|49 | Steps = 570356 | Walltime = 1672.128\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.579 | Episodes = 120 | Steps = 360 | Steps Per Second = 351.763\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.655 | Q = -0.214 | Reward = 3.441 | State = 390|9|52 | Steps = 570719 | Walltime = 1673.129\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.793 | Episodes = 241 | Steps = 723 | Steps Per Second = 403.053\n",
            "[Learner] Action = 2.000 | Avg Td Error = -13.848 | Q = -2.369 | Reward = -15.336 | State = 420|1|52 | Steps = 571088 | Walltime = 1674.131\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.325 | Episodes = 364 | Steps = 1092 | Steps Per Second = 384.775\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.360 | Q = -5.597 | Reward = -4.142 | State = 450|2|50 | Steps = 571449 | Walltime = 1675.132\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.786 | Episodes = 485 | Steps = 1455 | Steps Per Second = 342.094\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.798 | Q = -1.371 | Reward = 5.428 | State = 390|-1|48 | Steps = 571816 | Walltime = 1676.138\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.168 | Episodes = 607 | Steps = 1821 | Steps Per Second = 350.070\n",
            "[Learner] Action = -3.000 | Avg Td Error = -13.157 | Q = -15.170 | Reward = -24.681 | State = 450|2|50 | Steps = 572184 | Walltime = 1677.140\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.161 | Episodes = 730 | Steps = 2190 | Steps Per Second = 359.142\n",
            "[Learner] Action = 4.000 | Avg Td Error = -84.563 | Q = -0.373 | Reward = -84.936 | State = 390|-1|57 | Steps = 572552 | Walltime = 1678.142\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.171 | Episodes = 853 | Steps = 2559 | Steps Per Second = 338.178\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.899 | Q = -6.191 | Reward = -0.052 | State = 450|2|50 | Steps = 572907 | Walltime = 1679.143\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.493 | Episodes = 972 | Steps = 2916 | Steps Per Second = 370.860\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.379 | Q = -4.602 | Reward = 5.168 | State = 450|2|50 | Steps = 573272 | Walltime = 1680.145\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.879 | Episodes = 1094 | Steps = 3282 | Steps Per Second = 325.569\n",
            "[Learner] Action = -4.000 | Avg Td Error = -0.062 | Q = -3.465 | Reward = -3.527 | State = 390|2|52 | Steps = 573617 | Walltime = 1681.145\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.011 | Episodes = 1210 | Steps = 3630 | Steps Per Second = 377.627\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.947 | Q = -2.408 | Reward = -4.356 | State = 390|-1|52 | Steps = 573970 | Walltime = 1682.148\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.836 | Episodes = 1327 | Steps = 3981 | Steps Per Second = 330.659\n",
            "[Learner] Action = 2.000 | Avg Td Error = 7.003 | Q = -4.112 | Reward = 2.909 | State = 420|2|49 | Steps = 574315 | Walltime = 1683.148\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.512 | Episodes = 1443 | Steps = 4329 | Steps Per Second = 390.156\n",
            "[Learner] Action = 0.000 | Avg Td Error = -26.994 | Q = -2.696 | Reward = -29.248 | State = 420|2|53 | Steps = 574662 | Walltime = 1684.148\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -119.379 | Episodes = 1558 | Steps = 4674 | Steps Per Second = 326.371\n",
            "[Learner] Action = 1.000 | Avg Td Error = -21.342 | Q = -5.554 | Reward = -26.529 | State = 450|2|50 | Steps = 574998 | Walltime = 1685.149\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.468 | Episodes = 1671 | Steps = 5013 | Steps Per Second = 344.567\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.034 | Q = -2.156 | Reward = 3.878 | State = 390|-1|51 | Steps = 575351 | Walltime = 1686.149\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.960 | Episodes = 1790 | Steps = 5370 | Steps Per Second = 375.139\n",
            "[Learner] Action = 4.000 | Avg Td Error = -3.808 | Q = -3.491 | Reward = -7.300 | State = 390|3|51 | Steps = 575723 | Walltime = 1687.149\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.865 | Episodes = 1914 | Steps = 5742 | Steps Per Second = 355.872\n",
            "[Learner] Action = -1.000 | Avg Td Error = -4.770 | Q = -6.271 | Reward = -8.528 | State = 450|2|50 | Steps = 576086 | Walltime = 1688.150\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.336 | Episodes = 2036 | Steps = 6108 | Steps Per Second = 378.195\n",
            "[Learner] Action = 3.000 | Avg Td Error = -3.098 | Q = -1.040 | Reward = -4.138 | State = 390|3|56 | Steps = 576443 | Walltime = 1689.152\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.217 | Episodes = 2155 | Steps = 6465 | Steps Per Second = 388.134\n",
            "[Learner] Action = 2.000 | Avg Td Error = 14.547 | Q = -9.112 | Reward = 5.435 | State = 450|2|50 | Steps = 576777 | Walltime = 1690.152\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.630 | Episodes = 2265 | Steps = 6795 | Steps Per Second = 303.546\n",
            "[Learner] Action = 2.000 | Avg Td Error = -0.636 | Q = -9.076 | Reward = -9.538 | State = 450|2|50 | Steps = 577098 | Walltime = 1691.153\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -69.711 | Episodes = 2373 | Steps = 7119 | Steps Per Second = 325.064\n",
            "[Learner] Action = -3.000 | Avg Td Error = 11.528 | Q = -14.922 | Reward = -1.515 | State = 450|2|50 | Steps = 577428 | Walltime = 1692.155\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.953 | Episodes = 2484 | Steps = 7452 | Steps Per Second = 367.406\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.363 | Q = -4.824 | Reward = 1.069 | State = 450|2|50 | Steps = 577756 | Walltime = 1693.157\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.298 | Episodes = 2594 | Steps = 7782 | Steps Per Second = 334.590\n",
            "[Learner] Action = -3.000 | Avg Td Error = -38.391 | Q = -2.328 | Reward = -40.719 | State = 390|0|49 | Steps = 578093 | Walltime = 1694.158\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.744 | Episodes = 2707 | Steps = 8121 | Steps Per Second = 370.882\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.098 | Q = -0.798 | Reward = -5.896 | State = 390|6|51 | Steps = 578434 | Walltime = 1695.158\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.953 | Episodes = 2821 | Steps = 8463 | Steps Per Second = 351.037\n",
            "[Learner] Action = 0.000 | Avg Td Error = -12.152 | Q = -1.088 | Reward = -12.586 | State = 420|3|51 | Steps = 578769 | Walltime = 1696.160\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.107 | Episodes = 2931 | Steps = 8793 | Steps Per Second = 325.948\n",
            "[Learner] Action = -4.000 | Avg Td Error = -8.492 | Q = -2.028 | Reward = -10.520 | State = 390|5|51 | Steps = 579103 | Walltime = 1697.161\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.469 | Episodes = 3045 | Steps = 9135 | Steps Per Second = 362.766\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.576 | Q = -0.886 | Reward = -2.462 | State = 390|0|46 | Steps = 579425 | Walltime = 1698.163\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.008 | Episodes = 3151 | Steps = 9453 | Steps Per Second = 370.784\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.053 | Q = -2.206 | Reward = 1.847 | State = 390|-1|51 | Steps = 579764 | Walltime = 1699.163\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.945 | Episodes = 3265 | Steps = 9795 | Steps Per Second = 321.855\n",
            "[Learner] Action = 3.000 | Avg Td Error = 15.967 | Q = -12.530 | Reward = 3.442 | State = 450|2|50 | Steps = 580092 | Walltime = 1700.163\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.855 | Episodes = 3376 | Steps = 10128 | Steps Per Second = 340.880\n",
            "[Learner] Action = 4.000 | Avg Td Error = -13.026 | Q = -3.063 | Reward = -16.020 | State = 420|6|49 | Steps = 580446 | Walltime = 1701.166\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.338 | Episodes = 3495 | Steps = 10485 | Steps Per Second = 333.755\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.555 | Q = -0.051 | Reward = -7.607 | State = 390|4|43 | Steps = 580803 | Walltime = 1702.167\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.117 | Episodes = 3613 | Steps = 10839 | Steps Per Second = 337.334\n",
            "[Learner] Action = -3.000 | Avg Td Error = 11.820 | Q = -15.216 | Reward = -1.524 | State = 450|2|50 | Steps = 581144 | Walltime = 1703.169\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.405 | Episodes = 3727 | Steps = 11181 | Steps Per Second = 311.682\n",
            "[Learner] Action = 4.000 | Avg Td Error = -19.922 | Q = -2.650 | Reward = -22.572 | State = 390|-2|48 | Steps = 581493 | Walltime = 1704.170\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.546 | Episodes = 3846 | Steps = 11538 | Steps Per Second = 366.134\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.260 | Q = -0.354 | Reward = 0.906 | State = 390|7|47 | Steps = 581849 | Walltime = 1705.171\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.902 | Episodes = 3965 | Steps = 11895 | Steps Per Second = 382.925\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.760 | Q = -2.012 | Reward = 4.504 | State = 420|1|52 | Steps = 582211 | Walltime = 1706.172\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.883 | Episodes = 4086 | Steps = 12258 | Steps Per Second = 372.584\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.098 | Q = -4.763 | Reward = 1.812 | State = 450|2|50 | Steps = 582582 | Walltime = 1707.175\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.827 | Episodes = 4210 | Steps = 12630 | Steps Per Second = 389.504\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.045 | Q = -0.516 | Reward = 1.529 | State = 390|3|50 | Steps = 582918 | Walltime = 1708.178\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.436 | Episodes = 4320 | Steps = 12960 | Steps Per Second = 334.172\n",
            "[Learner] Action = 3.000 | Avg Td Error = -1.019 | Q = -12.928 | Reward = -13.784 | State = 450|2|50 | Steps = 583256 | Walltime = 1709.179\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -104.108 | Episodes = 4435 | Steps = 13305 | Steps Per Second = 385.991\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.540 | Q = -0.447 | Reward = 0.094 | State = 390|6|49 | Steps = 583630 | Walltime = 1710.181\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.549 | Episodes = 4560 | Steps = 13680 | Steps Per Second = 379.038\n",
            "[Learner] Action = 0.000 | Avg Td Error = -24.606 | Q = -2.487 | Reward = -25.983 | State = 420|2|50 | Steps = 583989 | Walltime = 1711.184\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.334 | Episodes = 4677 | Steps = 14031 | Steps Per Second = 320.127\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.779 | Q = -5.511 | Reward = -3.658 | State = 450|2|50 | Steps = 584343 | Walltime = 1712.186\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.135 | Episodes = 4797 | Steps = 14391 | Steps Per Second = 386.857\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.367 | Q = -0.219 | Reward = -4.586 | State = 390|7|53 | Steps = 584704 | Walltime = 1713.188\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.006 | Episodes = 4917 | Steps = 14751 | Steps Per Second = 396.325\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.677 | Episodes = 5217 | Steps = 15651 | Steps Per Second = 1958.735\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.903 | Episodes = 5839 | Steps = 17517 | Steps Per Second = 1979.690\n",
            "[Learner] Action = 2.000 | Avg Td Error = 4.377 | Q = -9.034 | Reward = -4.350 | State = 450|2|50 | Steps = 585000 | Walltime = 1715.583\n",
            "Check Point 39\n",
            "[Learner] Action = -4.000 | Avg Td Error = 17.847 | Q = -19.740 | Reward = -0.702 | State = 450|2|50 | Steps = 585357 | Walltime = 1716.584\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.013 | Episodes = 121 | Steps = 363 | Steps Per Second = 389.058\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.362 | Q = -0.247 | Reward = 1.216 | State = 420|4|49 | Steps = 585728 | Walltime = 1717.585\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.009 | Episodes = 245 | Steps = 735 | Steps Per Second = 357.063\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.476 | Q = -4.369 | Reward = -2.893 | State = 390|2|51 | Steps = 586096 | Walltime = 1718.586\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.172 | Episodes = 368 | Steps = 1104 | Steps Per Second = 381.670\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.431 | Q = -6.418 | Reward = -11.852 | State = 420|2|49 | Steps = 586469 | Walltime = 1719.587\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.880 | Episodes = 492 | Steps = 1476 | Steps Per Second = 331.190\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.896 | Q = -0.040 | Reward = 0.856 | State = 390|7|52 | Steps = 586822 | Walltime = 1720.590\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.680 | Episodes = 609 | Steps = 1827 | Steps Per Second = 389.263\n",
            "[Learner] Action = 1.000 | Avg Td Error = 3.297 | Q = -1.373 | Reward = 1.924 | State = 390|2|49 | Steps = 587186 | Walltime = 1721.592\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.093 | Episodes = 731 | Steps = 2193 | Steps Per Second = 355.691\n",
            "[Learner] Action = -3.000 | Avg Td Error = -37.982 | Q = -3.813 | Reward = -40.772 | State = 420|-1|52 | Steps = 587551 | Walltime = 1722.595\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.642 | Episodes = 853 | Steps = 2559 | Steps Per Second = 388.326\n",
            "[Learner] Action = 4.000 | Avg Td Error = -27.793 | Q = -1.408 | Reward = -29.201 | State = 390|-4|49 | Steps = 587915 | Walltime = 1723.596\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.735 | Episodes = 975 | Steps = 2925 | Steps Per Second = 377.774\n",
            "[Learner] Action = 2.000 | Avg Td Error = -8.014 | Q = -0.984 | Reward = -9.003 | State = 420|6|52 | Steps = 588287 | Walltime = 1724.597\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.329 | Episodes = 1099 | Steps = 3297 | Steps Per Second = 301.178\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.915 | Q = -1.811 | Reward = -4.335 | State = 420|1|49 | Steps = 588632 | Walltime = 1725.597\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.037 | Episodes = 1215 | Steps = 3645 | Steps Per Second = 379.209\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.189 | Q = -5.339 | Reward = 4.856 | State = 420|2|51 | Steps = 588988 | Walltime = 1726.599\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.211 | Episodes = 1334 | Steps = 4002 | Steps Per Second = 350.197\n",
            "[Learner] Action = 2.000 | Avg Td Error = -8.806 | Q = -2.926 | Reward = -10.836 | State = 420|0|49 | Steps = 589351 | Walltime = 1727.600\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.579 | Episodes = 1455 | Steps = 4365 | Steps Per Second = 339.978\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.082 | Q = -0.191 | Reward = 0.891 | State = 390|8|50 | Steps = 589716 | Walltime = 1728.601\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.910 | Episodes = 1577 | Steps = 4731 | Steps Per Second = 389.588\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.397 | Q = -3.123 | Reward = -9.521 | State = 390|-1|51 | Steps = 590088 | Walltime = 1729.602\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.456 | Episodes = 1701 | Steps = 5103 | Steps Per Second = 384.552\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.119 | Q = -4.782 | Reward = -3.291 | State = 450|2|50 | Steps = 590431 | Walltime = 1730.605\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -128.201 | Episodes = 1816 | Steps = 5448 | Steps Per Second = 367.041\n",
            "[Learner] Action = -1.000 | Avg Td Error = -12.250 | Q = -0.564 | Reward = -12.203 | State = 420|-2|46 | Steps = 590790 | Walltime = 1731.606\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.941 | Episodes = 1936 | Steps = 5808 | Steps Per Second = 372.838\n",
            "[Learner] Action = -4.000 | Avg Td Error = -5.501 | Q = -19.563 | Reward = -21.186 | State = 450|2|50 | Steps = 591156 | Walltime = 1732.608\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -86.450 | Episodes = 2058 | Steps = 6174 | Steps Per Second = 379.850\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.622 | Q = -4.686 | Reward = -9.469 | State = 420|2|49 | Steps = 591502 | Walltime = 1733.609\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.429 | Episodes = 2173 | Steps = 6519 | Steps Per Second = 326.067\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.200 | Q = -5.066 | Reward = 4.258 | State = 450|2|50 | Steps = 591836 | Walltime = 1734.610\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.315 | Episodes = 2285 | Steps = 6855 | Steps Per Second = 362.004\n",
            "[Learner] Action = -1.000 | Avg Td Error = -12.632 | Q = -2.215 | Reward = -14.847 | State = 390|-1|51 | Steps = 592209 | Walltime = 1735.611\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.568 | Episodes = 2409 | Steps = 7227 | Steps Per Second = 329.525\n",
            "[Learner] Action = 0.000 | Avg Td Error = -14.057 | Q = -4.964 | Reward = -16.182 | State = 450|2|50 | Steps = 592537 | Walltime = 1736.613\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.304 | Episodes = 2519 | Steps = 7557 | Steps Per Second = 377.706\n",
            "[Learner] Action = -4.000 | Avg Td Error = -7.856 | Q = -2.295 | Reward = -9.551 | State = 420|3|48 | Steps = 592872 | Walltime = 1737.616\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.754 | Episodes = 2632 | Steps = 7896 | Steps Per Second = 355.741\n",
            "[Learner] Action = 4.000 | Avg Td Error = 5.828 | Q = -16.610 | Reward = -10.529 | State = 450|2|50 | Steps = 593243 | Walltime = 1738.617\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.266 | Episodes = 2755 | Steps = 8265 | Steps Per Second = 390.156\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.496 | Q = -1.121 | Reward = 1.876 | State = 420|3|51 | Steps = 593611 | Walltime = 1739.618\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.924 | Episodes = 2878 | Steps = 8634 | Steps Per Second = 371.704\n",
            "[Learner] Action = -4.000 | Avg Td Error = 2.751 | Q = -19.539 | Reward = -12.394 | State = 450|2|50 | Steps = 593981 | Walltime = 1740.620\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.821 | Episodes = 3002 | Steps = 9006 | Steps Per Second = 363.154\n",
            "[Learner] Action = 4.000 | Avg Td Error = -11.067 | Q = -2.718 | Reward = -13.716 | State = 420|6|48 | Steps = 594349 | Walltime = 1741.621\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -64.502 | Episodes = 3125 | Steps = 9375 | Steps Per Second = 391.308\n",
            "[Learner] Action = 0.000 | Avg Td Error = -11.324 | Q = -0.262 | Reward = -11.586 | State = 390|1|57 | Steps = 594710 | Walltime = 1742.622\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.603 | Episodes = 3246 | Steps = 9738 | Steps Per Second = 373.946\n",
            "[Learner] Action = 0.000 | Avg Td Error = -102.316 | Q = -1.039 | Reward = -103.355 | State = 390|-5|51 | Steps = 595073 | Walltime = 1743.623\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.665 | Episodes = 3367 | Steps = 10101 | Steps Per Second = 397.439\n",
            "[Learner] Action = 3.000 | Avg Td Error = 5.354 | Q = -13.014 | Reward = -7.565 | State = 450|2|50 | Steps = 595445 | Walltime = 1744.624\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.612 | Episodes = 3492 | Steps = 10476 | Steps Per Second = 372.993\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.909 | Q = -1.440 | Reward = -7.349 | State = 390|4|48 | Steps = 595806 | Walltime = 1745.624\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.249 | Episodes = 3612 | Steps = 10836 | Steps Per Second = 338.779\n",
            "[Learner] Action = -4.000 | Avg Td Error = -10.731 | Q = -2.549 | Reward = -13.280 | State = 390|5|50 | Steps = 596137 | Walltime = 1746.626\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.159 | Episodes = 3723 | Steps = 11169 | Steps Per Second = 375.195\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.040 | Q = -0.610 | Reward = -2.365 | State = 420|1|47 | Steps = 596511 | Walltime = 1747.627\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.584 | Episodes = 3848 | Steps = 11544 | Steps Per Second = 383.637\n",
            "[Learner] Action = 0.000 | Avg Td Error = -51.253 | Q = -5.057 | Reward = -56.156 | State = 450|2|50 | Steps = 596843 | Walltime = 1748.628\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.820 | Episodes = 3959 | Steps = 11877 | Steps Per Second = 393.425\n",
            "[Learner] Action = -2.000 | Avg Td Error = -9.808 | Q = -1.686 | Reward = -9.811 | State = 420|3|49 | Steps = 597210 | Walltime = 1749.630\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.532 | Episodes = 4080 | Steps = 12240 | Steps Per Second = 378.468\n",
            "[Learner] Action = 3.000 | Avg Td Error = -36.678 | Q = -1.807 | Reward = -38.486 | State = 390|3|52 | Steps = 597578 | Walltime = 1750.632\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.922 | Episodes = 4202 | Steps = 12606 | Steps Per Second = 340.079\n",
            "[Learner] Action = -4.000 | Avg Td Error = -25.112 | Q = -6.011 | Reward = -30.766 | State = 420|-2|49 | Steps = 597915 | Walltime = 1751.635\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -124.578 | Episodes = 4315 | Steps = 12945 | Steps Per Second = 395.502\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.298 | Q = -0.384 | Reward = 4.914 | State = 390|-1|57 | Steps = 598284 | Walltime = 1752.637\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.300 | Episodes = 4439 | Steps = 13317 | Steps Per Second = 383.065\n",
            "[Learner] Action = -1.000 | Avg Td Error = -14.666 | Q = -0.243 | Reward = -14.909 | State = 390|7|51 | Steps = 598652 | Walltime = 1753.639\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.630 | Episodes = 4559 | Steps = 13677 | Steps Per Second = 201.459\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.420 | Q = -4.168 | Reward = -9.520 | State = 420|0|51 | Steps = 599016 | Walltime = 1754.642\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -191.270 | Episodes = 4683 | Steps = 14049 | Steps Per Second = 384.563\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.626 | Q = -0.769 | Reward = -6.395 | State = 390|2|45 | Steps = 599365 | Walltime = 1755.644\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.250 | Episodes = 4799 | Steps = 14397 | Steps Per Second = 387.835\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.820 | Q = -2.126 | Reward = 5.329 | State = 420|-2|48 | Steps = 599730 | Walltime = 1756.645\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.857 | Episodes = 4922 | Steps = 14766 | Steps Per Second = 374.113\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.725 | Episodes = 5241 | Steps = 15723 | Steps Per Second = 1692.844\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.761 | Episodes = 5862 | Steps = 17586 | Steps Per Second = 1092.267\n",
            "[Learner] Action = 0.000 | Avg Td Error = -24.506 | Q = -1.404 | Reward = -25.910 | State = 390|1|50 | Steps = 600000 | Walltime = 1758.986\n",
            "Check Point 40\n",
            "[Learner] Action = -4.000 | Avg Td Error = -18.116 | Q = -4.616 | Reward = -20.783 | State = 420|3|51 | Steps = 600363 | Walltime = 1759.988\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.692 | Episodes = 123 | Steps = 369 | Steps Per Second = 388.290\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.171 | Q = -0.013 | Reward = 1.184 | State = 420|5|49 | Steps = 600711 | Walltime = 1760.989\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.528 | Episodes = 239 | Steps = 717 | Steps Per Second = 336.928\n",
            "[Learner] Action = -2.000 | Avg Td Error = -2.592 | Q = -1.180 | Reward = -3.167 | State = 420|3|48 | Steps = 601055 | Walltime = 1761.991\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.990 | Episodes = 354 | Steps = 1062 | Steps Per Second = 358.989\n",
            "[Learner] Action = -4.000 | Avg Td Error = -20.506 | Q = -4.284 | Reward = -24.790 | State = 390|2|50 | Steps = 601420 | Walltime = 1762.993\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.646 | Episodes = 476 | Steps = 1428 | Steps Per Second = 362.798\n",
            "[Learner] Action = -3.000 | Avg Td Error = 12.143 | Q = -15.431 | Reward = -1.111 | State = 450|2|50 | Steps = 601761 | Walltime = 1764.000\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.723 | Episodes = 589 | Steps = 1767 | Steps Per Second = 384.340\n",
            "[Learner] Action = 2.000 | Avg Td Error = -0.871 | Q = -2.273 | Reward = -2.167 | State = 420|-2|48 | Steps = 602118 | Walltime = 1765.001\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.115 | Episodes = 709 | Steps = 2127 | Steps Per Second = 380.643\n",
            "[Learner] Action = -1.000 | Avg Td Error = -15.114 | Q = -6.265 | Reward = -19.970 | State = 450|2|50 | Steps = 602480 | Walltime = 1766.003\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -181.162 | Episodes = 830 | Steps = 2490 | Steps Per Second = 372.584\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.862 | Q = -1.689 | Reward = 5.173 | State = 390|1|51 | Steps = 602854 | Walltime = 1767.004\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -129.448 | Episodes = 955 | Steps = 2865 | Steps Per Second = 372.430\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.415 | Q = -5.392 | Reward = 5.663 | State = 450|2|50 | Steps = 603219 | Walltime = 1768.006\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.238 | Episodes = 1077 | Steps = 3231 | Steps Per Second = 380.770\n",
            "[Learner] Action = 0.000 | Avg Td Error = 11.158 | Q = -5.317 | Reward = 6.021 | State = 450|2|50 | Steps = 603585 | Walltime = 1769.006\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.695 | Episodes = 1199 | Steps = 3597 | Steps Per Second = 357.764\n",
            "[Learner] Action = 1.000 | Avg Td Error = 8.281 | Q = -5.172 | Reward = 3.208 | State = 450|2|50 | Steps = 603943 | Walltime = 1770.007\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.301 | Episodes = 1318 | Steps = 3954 | Steps Per Second = 308.088\n",
            "[Learner] Action = -2.000 | Avg Td Error = -42.951 | Q = -10.622 | Reward = -53.093 | State = 450|2|50 | Steps = 604290 | Walltime = 1771.008\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.972 | Episodes = 1434 | Steps = 4302 | Steps Per Second = 372.849\n",
            "[Learner] Action = 3.000 | Avg Td Error = 5.149 | Q = -12.775 | Reward = -7.531 | State = 450|2|50 | Steps = 604657 | Walltime = 1772.010\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.285 | Episodes = 1557 | Steps = 4671 | Steps Per Second = 384.328\n",
            "[Learner] Action = 0.000 | Avg Td Error = -32.839 | Q = -1.450 | Reward = -33.057 | State = 420|2|48 | Steps = 605012 | Walltime = 1773.010\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.499 | Episodes = 1676 | Steps = 5028 | Steps Per Second = 382.599\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.083 | Q = -0.519 | Reward = -2.502 | State = 420|4|51 | Steps = 605374 | Walltime = 1774.013\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.853 | Episodes = 1794 | Steps = 5382 | Steps Per Second = 267.625\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.643 | Q = -0.561 | Reward = -6.284 | State = 420|4|47 | Steps = 605713 | Walltime = 1775.013\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.231 | Episodes = 1910 | Steps = 5730 | Steps Per Second = 377.979\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.560 | Q = -0.096 | Reward = -4.655 | State = 390|4|49 | Steps = 606069 | Walltime = 1776.014\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.346 | Episodes = 2028 | Steps = 6084 | Steps Per Second = 339.464\n",
            "[Learner] Action = 2.000 | Avg Td Error = -6.638 | Q = -1.021 | Reward = -7.659 | State = 390|5|51 | Steps = 606402 | Walltime = 1777.016\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.188 | Episodes = 2139 | Steps = 6417 | Steps Per Second = 324.553\n",
            "[Learner] Action = 0.000 | Avg Td Error = -15.389 | Q = -1.924 | Reward = -17.314 | State = 390|0|50 | Steps = 606734 | Walltime = 1778.017\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -199.278 | Episodes = 2250 | Steps = 6750 | Steps Per Second = 337.796\n",
            "[Learner] Action = -3.000 | Avg Td Error = -25.822 | Q = -15.441 | Reward = -39.152 | State = 450|2|50 | Steps = 607068 | Walltime = 1779.019\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.078 | Episodes = 2362 | Steps = 7086 | Steps Per Second = 343.767\n",
            "[Learner] Action = -3.000 | Avg Td Error = -12.537 | Q = -15.516 | Reward = -24.226 | State = 450|2|50 | Steps = 607414 | Walltime = 1780.024\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.139 | Episodes = 2478 | Steps = 7434 | Steps Per Second = 385.494\n",
            "[Learner] Action = -1.000 | Avg Td Error = 8.501 | Q = -6.446 | Reward = 3.095 | State = 450|2|50 | Steps = 607767 | Walltime = 1781.026\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.617 | Episodes = 2594 | Steps = 7782 | Steps Per Second = 331.994\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.402 | Q = -5.456 | Reward = 4.043 | State = 450|2|50 | Steps = 608114 | Walltime = 1782.028\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.341 | Episodes = 2710 | Steps = 8130 | Steps Per Second = 377.163\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.685 | Q = -0.524 | Reward = 1.160 | State = 390|2|47 | Steps = 608479 | Walltime = 1783.030\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.968 | Episodes = 2832 | Steps = 8496 | Steps Per Second = 371.781\n",
            "[Learner] Action = 2.000 | Avg Td Error = -10.433 | Q = -0.252 | Reward = -10.685 | State = 390|-1|45 | Steps = 608843 | Walltime = 1784.030\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.909 | Episodes = 2953 | Steps = 8859 | Steps Per Second = 181.394\n",
            "[Learner] Action = 4.000 | Avg Td Error = -13.712 | Q = -2.651 | Reward = -16.363 | State = 390|5|51 | Steps = 609178 | Walltime = 1785.032\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.834 | Episodes = 3066 | Steps = 9198 | Steps Per Second = 215.549\n",
            "[Learner] Action = -2.000 | Avg Td Error = -7.069 | Q = -0.104 | Reward = -7.174 | State = 390|0|44 | Steps = 609535 | Walltime = 1786.035\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.840 | Episodes = 3186 | Steps = 9558 | Steps Per Second = 257.831\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.799 | Q = -1.042 | Reward = -2.842 | State = 390|6|49 | Steps = 609799 | Walltime = 1787.038\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.550 | Episodes = 3270 | Steps = 9810 | Steps Per Second = 138.694\n",
            "[Learner] Action = -3.000 | Avg Td Error = -11.736 | Q = -1.144 | Reward = -12.879 | State = 390|6|52 | Steps = 609986 | Walltime = 1788.038\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.280 | Episodes = 3335 | Steps = 10005 | Steps Per Second = 202.715\n",
            "[Learner] Action = -4.000 | Avg Td Error = -54.136 | Q = -1.094 | Reward = -55.214 | State = 420|0|55 | Steps = 610185 | Walltime = 1789.042\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.832 | Episodes = 3403 | Steps = 10209 | Steps Per Second = 257.873\n",
            "[Learner] Action = -3.000 | Avg Td Error = -11.862 | Q = -1.458 | Reward = -13.320 | State = 390|4|54 | Steps = 610419 | Walltime = 1790.042\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.394 | Episodes = 3481 | Steps = 10443 | Steps Per Second = 249.142\n",
            "[Learner] Action = -4.000 | Avg Td Error = -15.027 | Q = -3.317 | Reward = -18.344 | State = 390|3|49 | Steps = 610753 | Walltime = 1791.044\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.008 | Episodes = 3597 | Steps = 10791 | Steps Per Second = 379.942\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.632 | Q = -1.513 | Reward = -4.145 | State = 390|2|49 | Steps = 611119 | Walltime = 1792.046\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.011 | Episodes = 3719 | Steps = 11157 | Steps Per Second = 385.636\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.075 | Q = -1.705 | Reward = -7.779 | State = 390|0|48 | Steps = 611488 | Walltime = 1793.047\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.354 | Episodes = 3841 | Steps = 11523 | Steps Per Second = 362.892\n",
            "[Learner] Action = 2.000 | Avg Td Error = -17.329 | Q = -8.886 | Reward = -25.626 | State = 450|2|50 | Steps = 611837 | Walltime = 1794.049\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.754 | Episodes = 3959 | Steps = 11877 | Steps Per Second = 380.781\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.131 | Q = -1.099 | Reward = 2.992 | State = 420|0|48 | Steps = 612208 | Walltime = 1795.050\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.570 | Episodes = 4082 | Steps = 12246 | Steps Per Second = 372.419\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.530 | Q = -5.056 | Reward = 2.521 | State = 450|2|50 | Steps = 612553 | Walltime = 1796.052\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.608 | Episodes = 4198 | Steps = 12594 | Steps Per Second = 371.035\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.880 | Q = -0.160 | Reward = 0.910 | State = 420|6|53 | Steps = 612919 | Walltime = 1797.054\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.386 | Episodes = 4320 | Steps = 12960 | Steps Per Second = 366.411\n",
            "[Learner] Action = 2.000 | Avg Td Error = -20.876 | Q = -4.430 | Reward = -23.527 | State = 420|-1|51 | Steps = 613291 | Walltime = 1798.055\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.421 | Episodes = 4444 | Steps = 13332 | Steps Per Second = 336.928\n",
            "[Learner] Action = -1.000 | Avg Td Error = -23.837 | Q = -0.934 | Reward = -24.771 | State = 390|-2|55 | Steps = 613644 | Walltime = 1799.057\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.651 | Episodes = 4561 | Steps = 13683 | Steps Per Second = 337.389\n",
            "[Learner] Action = -2.000 | Avg Td Error = 2.226 | Q = -11.056 | Reward = -5.592 | State = 450|2|50 | Steps = 613995 | Walltime = 1800.058\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.211 | Episodes = 4679 | Steps = 14037 | Steps Per Second = 323.443\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.412 | Q = -0.674 | Reward = 2.513 | State = 420|4|52 | Steps = 614346 | Walltime = 1801.059\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.282 | Episodes = 4797 | Steps = 14391 | Steps Per Second = 367.556\n",
            "[Learner] Action = -2.000 | Avg Td Error = -12.461 | Q = -2.784 | Reward = -13.902 | State = 420|-1|48 | Steps = 614712 | Walltime = 1802.062\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.739 | Episodes = 4917 | Steps = 14751 | Steps Per Second = 338.833\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.688 | Episodes = 5188 | Steps = 15564 | Steps Per Second = 1958.430\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.704 | Episodes = 5821 | Steps = 17463 | Steps Per Second = 1997.288\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.165 | Q = -1.064 | Reward = 4.101 | State = 390|-5|52 | Steps = 615000 | Walltime = 1804.487\n",
            "Check Point 41\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.587 | Q = -2.364 | Reward = -1.956 | State = 420|1|50 | Steps = 615362 | Walltime = 1805.487\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.385 | Episodes = 123 | Steps = 369 | Steps Per Second = 384.023\n",
            "[Learner] Action = 4.000 | Avg Td Error = -27.811 | Q = -16.367 | Reward = -44.022 | State = 450|2|50 | Steps = 615721 | Walltime = 1806.488\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.541 | Episodes = 243 | Steps = 729 | Steps Per Second = 381.312\n",
            "[Learner] Action = -4.000 | Avg Td Error = -2.629 | Q = -18.926 | Reward = -17.421 | State = 450|2|50 | Steps = 616058 | Walltime = 1807.488\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.125 | Episodes = 356 | Steps = 1068 | Steps Per Second = 374.994\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.176 | Q = -6.727 | Reward = -5.318 | State = 450|2|50 | Steps = 616423 | Walltime = 1808.491\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.637 | Episodes = 478 | Steps = 1434 | Steps Per Second = 368.968\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.346 | Q = -3.941 | Reward = 5.713 | State = 420|-1|50 | Steps = 616772 | Walltime = 1809.493\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.713 | Episodes = 594 | Steps = 1782 | Steps Per Second = 267.204\n",
            "[Learner] Action = 1.000 | Avg Td Error = -41.183 | Q = -1.393 | Reward = -42.576 | State = 390|2|49 | Steps = 617116 | Walltime = 1810.494\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.905 | Episodes = 709 | Steps = 2127 | Steps Per Second = 359.882\n",
            "[Learner] Action = 0.000 | Avg Td Error = -19.251 | Q = -5.012 | Reward = -22.098 | State = 450|2|50 | Steps = 617486 | Walltime = 1811.494\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.021 | Episodes = 833 | Steps = 2499 | Steps Per Second = 375.789\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.634 | Q = -4.994 | Reward = 0.267 | State = 450|2|50 | Steps = 617858 | Walltime = 1812.495\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -93.672 | Episodes = 957 | Steps = 2871 | Steps Per Second = 228.216\n",
            "[Learner] Action = 2.000 | Avg Td Error = 1.419 | Q = -8.339 | Reward = -6.521 | State = 450|2|50 | Steps = 618183 | Walltime = 1813.497\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.857 | Episodes = 1066 | Steps = 3198 | Steps Per Second = 265.043\n",
            "[Learner] Action = -2.000 | Avg Td Error = -19.329 | Q = -11.125 | Reward = -28.691 | State = 450|2|50 | Steps = 618512 | Walltime = 1814.497\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.922 | Episodes = 1177 | Steps = 3531 | Steps Per Second = 386.323\n",
            "[Learner] Action = 1.000 | Avg Td Error = -3.457 | Q = -1.463 | Reward = -4.920 | State = 390|1|49 | Steps = 618882 | Walltime = 1815.499\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.519 | Episodes = 1301 | Steps = 3903 | Steps Per Second = 365.878\n",
            "[Learner] Action = 2.000 | Avg Td Error = 0.085 | Q = -3.340 | Reward = -2.880 | State = 420|1|50 | Steps = 619216 | Walltime = 1816.501\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.961 | Episodes = 1413 | Steps = 4239 | Steps Per Second = 389.950\n",
            "[Learner] Action = -4.000 | Avg Td Error = -5.130 | Q = -2.612 | Reward = -7.741 | State = 390|3|48 | Steps = 619575 | Walltime = 1817.503\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.754 | Episodes = 1533 | Steps = 4599 | Steps Per Second = 391.296\n",
            "[Learner] Action = -3.000 | Avg Td Error = 3.162 | Q = -5.606 | Reward = -0.049 | State = 420|2|53 | Steps = 619941 | Walltime = 1818.505\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.154 | Episodes = 1655 | Steps = 4965 | Steps Per Second = 376.824\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.888 | Q = -0.503 | Reward = 3.385 | State = 390|4|51 | Steps = 620312 | Walltime = 1819.507\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -117.410 | Episodes = 1779 | Steps = 5337 | Steps Per Second = 380.919\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.023 | Q = -4.032 | Reward = 5.998 | State = 420|2|52 | Steps = 620680 | Walltime = 1820.508\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.126 | Episodes = 1902 | Steps = 5706 | Steps Per Second = 381.543\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.087 | Q = -1.711 | Reward = -4.073 | State = 420|3|49 | Steps = 621013 | Walltime = 1821.509\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.842 | Episodes = 2013 | Steps = 6039 | Steps Per Second = 376.599\n",
            "[Learner] Action = 2.000 | Avg Td Error = 8.457 | Q = -8.424 | Reward = 0.390 | State = 450|2|50 | Steps = 621377 | Walltime = 1822.509\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -164.848 | Episodes = 2135 | Steps = 6405 | Steps Per Second = 394.449\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.420 | Q = -4.877 | Reward = 5.091 | State = 450|2|50 | Steps = 621741 | Walltime = 1823.511\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.853 | Episodes = 2256 | Steps = 6768 | Steps Per Second = 360.759\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.747 | Q = -3.346 | Reward = 5.461 | State = 420|1|51 | Steps = 622102 | Walltime = 1824.513\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.355 | Episodes = 2377 | Steps = 7131 | Steps Per Second = 396.162\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.411 | Q = -0.154 | Reward = -0.564 | State = 420|5|46 | Steps = 622471 | Walltime = 1825.515\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.226 | Episodes = 2500 | Steps = 7500 | Steps Per Second = 384.599\n",
            "[Learner] Action = 3.000 | Avg Td Error = 5.847 | Q = -12.241 | Reward = -6.284 | State = 450|2|50 | Steps = 622801 | Walltime = 1826.516\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -93.857 | Episodes = 2611 | Steps = 7833 | Steps Per Second = 388.974\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.643 | Q = -0.150 | Reward = -8.805 | State = 420|5|54 | Steps = 623133 | Walltime = 1827.518\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.972 | Episodes = 2722 | Steps = 8166 | Steps Per Second = 368.439\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.670 | Q = -1.057 | Reward = -6.747 | State = 420|5|48 | Steps = 623465 | Walltime = 1828.521\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.363 | Episodes = 2832 | Steps = 8496 | Steps Per Second = 369.575\n",
            "[Learner] Action = -2.000 | Avg Td Error = 14.530 | Q = -11.220 | Reward = 3.981 | State = 450|2|50 | Steps = 623814 | Walltime = 1829.521\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.039 | Episodes = 2950 | Steps = 8850 | Steps Per Second = 384.176\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.755 | Q = -4.730 | Reward = -3.911 | State = 450|2|50 | Steps = 624181 | Walltime = 1830.523\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.435 | Episodes = 3072 | Steps = 9216 | Steps Per Second = 360.790\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.159 | Q = -4.780 | Reward = 3.524 | State = 450|2|50 | Steps = 624513 | Walltime = 1831.523\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.720 | Episodes = 3182 | Steps = 9546 | Steps Per Second = 331.732\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.141 | Q = -0.241 | Reward = -0.383 | State = 390|2|47 | Steps = 624877 | Walltime = 1832.525\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.386 | Episodes = 3304 | Steps = 9912 | Steps Per Second = 318.426\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.762 | Q = -0.020 | Reward = -3.783 | State = 390|1|39 | Steps = 625238 | Walltime = 1833.525\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -182.297 | Episodes = 3426 | Steps = 10278 | Steps Per Second = 361.038\n",
            "[Learner] Action = 0.000 | Avg Td Error = -35.201 | Q = -0.682 | Reward = -35.882 | State = 390|0|47 | Steps = 625576 | Walltime = 1834.529\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.178 | Episodes = 3538 | Steps = 10614 | Steps Per Second = 371.660\n",
            "[Learner] Action = -1.000 | Avg Td Error = -50.880 | Q = -1.225 | Reward = -52.099 | State = 420|2|55 | Steps = 625944 | Walltime = 1835.529\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.245 | Episodes = 3661 | Steps = 10983 | Steps Per Second = 340.899\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.849 | Q = -0.395 | Reward = -4.244 | State = 390|5|48 | Steps = 626291 | Walltime = 1836.531\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -66.240 | Episodes = 3778 | Steps = 11334 | Steps Per Second = 353.721\n",
            "[Learner] Action = 1.000 | Avg Td Error = -10.232 | Q = -1.422 | Reward = -11.654 | State = 390|2|54 | Steps = 626650 | Walltime = 1837.533\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.678 | Episodes = 3898 | Steps = 11694 | Steps Per Second = 381.370\n",
            "[Learner] Action = -2.000 | Avg Td Error = 4.979 | Q = -5.896 | Reward = 0.642 | State = 420|2|51 | Steps = 627012 | Walltime = 1838.535\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.082 | Episodes = 4019 | Steps = 12057 | Steps Per Second = 388.541\n",
            "[Learner] Action = -1.000 | Avg Td Error = -11.279 | Q = -0.372 | Reward = -11.651 | State = 390|1|46 | Steps = 627349 | Walltime = 1839.536\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.681 | Episodes = 4132 | Steps = 12396 | Steps Per Second = 356.053\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.686 | Q = -6.695 | Reward = -1.640 | State = 450|2|50 | Steps = 627708 | Walltime = 1840.537\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.056 | Episodes = 4251 | Steps = 12753 | Steps Per Second = 345.210\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.581 | Q = -0.553 | Reward = -4.134 | State = 390|1|46 | Steps = 628076 | Walltime = 1841.538\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.592 | Episodes = 4375 | Steps = 13125 | Steps Per Second = 381.335\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.154 | Q = -8.385 | Reward = 1.929 | State = 450|2|50 | Steps = 628419 | Walltime = 1842.540\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -73.043 | Episodes = 4489 | Steps = 13467 | Steps Per Second = 382.832\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.939 | Q = -2.521 | Reward = 2.569 | State = 420|2|49 | Steps = 628784 | Walltime = 1843.542\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.398 | Episodes = 4610 | Steps = 13830 | Steps Per Second = 361.723\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.966 | Q = -2.724 | Reward = -10.625 | State = 420|6|50 | Steps = 629149 | Walltime = 1844.544\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.784 | Episodes = 4734 | Steps = 14202 | Steps Per Second = 380.171\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.053 | Q = -4.993 | Reward = -3.352 | State = 450|2|50 | Steps = 629520 | Walltime = 1845.546\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.269 | Episodes = 4858 | Steps = 14574 | Steps Per Second = 384.364\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.179 | Q = -0.290 | Reward = 1.890 | State = 390|8|51 | Steps = 629887 | Walltime = 1846.549\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.758 | Episodes = 4980 | Steps = 14940 | Steps Per Second = 341.454\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.197 | Episodes = 5475 | Steps = 16425 | Steps Per Second = 1809.970\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.049 | Q = -1.904 | Reward = -8.953 | State = 390|5|50 | Steps = 630000 | Walltime = 1848.545\n",
            "Check Point 42\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.532 | Q = -4.859 | Reward = 3.766 | State = 450|2|50 | Steps = 630350 | Walltime = 1849.546\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.548 | Episodes = 118 | Steps = 354 | Steps Per Second = 365.857\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.468 | Q = -5.553 | Reward = 1.056 | State = 420|2|51 | Steps = 630687 | Walltime = 1850.548\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.507 | Episodes = 231 | Steps = 693 | Steps Per Second = 380.562\n",
            "[Learner] Action = -1.000 | Avg Td Error = -5.473 | Q = -1.067 | Reward = -6.540 | State = 390|-2|55 | Steps = 631012 | Walltime = 1851.549\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -89.446 | Episodes = 340 | Steps = 1020 | Steps Per Second = 355.409\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.411 | Q = -1.192 | Reward = -9.602 | State = 390|5|47 | Steps = 631351 | Walltime = 1852.549\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.893 | Episodes = 453 | Steps = 1359 | Steps Per Second = 349.118\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.876 | Q = -8.099 | Reward = -6.140 | State = 420|2|48 | Steps = 631680 | Walltime = 1853.550\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.682 | Episodes = 563 | Steps = 1689 | Steps Per Second = 368.676\n",
            "[Learner] Action = -4.000 | Avg Td Error = -18.578 | Q = -3.976 | Reward = -20.203 | State = 420|4|49 | Steps = 632005 | Walltime = 1854.551\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.251 | Episodes = 672 | Steps = 2016 | Steps Per Second = 378.821\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.965 | Q = -4.779 | Reward = -6.617 | State = 450|2|50 | Steps = 632370 | Walltime = 1855.555\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -125.379 | Episodes = 794 | Steps = 2382 | Steps Per Second = 359.954\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.214 | Q = -4.742 | Reward = 2.571 | State = 450|2|50 | Steps = 632719 | Walltime = 1856.557\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.550 | Episodes = 911 | Steps = 2733 | Steps Per Second = 389.010\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.798 | Q = -2.737 | Reward = -12.535 | State = 390|5|49 | Steps = 633047 | Walltime = 1857.557\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.029 | Episodes = 1021 | Steps = 3063 | Steps Per Second = 396.887\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.093 | Q = -0.222 | Reward = 2.871 | State = 390|2|47 | Steps = 633416 | Walltime = 1858.557\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.541 | Episodes = 1145 | Steps = 3435 | Steps Per Second = 370.849\n",
            "[Learner] Action = 0.000 | Avg Td Error = -26.905 | Q = -0.397 | Reward = -27.302 | State = 390|8|49 | Steps = 633787 | Walltime = 1859.560\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.855 | Episodes = 1268 | Steps = 3804 | Steps Per Second = 346.017\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.579 | Q = -1.438 | Reward = -0.859 | State = 390|1|49 | Steps = 634153 | Walltime = 1860.560\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.272 | Episodes = 1391 | Steps = 4173 | Steps Per Second = 386.738\n",
            "[Learner] Action = 4.000 | Avg Td Error = -22.103 | Q = -1.037 | Reward = -22.781 | State = 420|0|55 | Steps = 634526 | Walltime = 1861.561\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.404 | Episodes = 1516 | Steps = 4548 | Steps Per Second = 382.855\n",
            "[Learner] Action = 3.000 | Avg Td Error = -29.487 | Q = -3.142 | Reward = -31.526 | State = 420|-1|48 | Steps = 634853 | Walltime = 1862.562\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.480 | Episodes = 1625 | Steps = 4875 | Steps Per Second = 393.118\n",
            "[Learner] Action = -4.000 | Avg Td Error = -40.807 | Q = -1.233 | Reward = -42.040 | State = 390|4|56 | Steps = 635222 | Walltime = 1863.562\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -62.124 | Episodes = 1749 | Steps = 5247 | Steps Per Second = 366.229\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.771 | Q = -3.231 | Reward = 4.129 | State = 420|0|50 | Steps = 635594 | Walltime = 1864.563\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.339 | Episodes = 1873 | Steps = 5619 | Steps Per Second = 357.743\n",
            "[Learner] Action = 0.000 | Avg Td Error = -50.825 | Q = -4.568 | Reward = -55.219 | State = 450|2|50 | Steps = 635926 | Walltime = 1865.564\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.649 | Episodes = 1984 | Steps = 5952 | Steps Per Second = 381.289\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.891 | Q = -1.106 | Reward = 1.785 | State = 390|-4|50 | Steps = 636298 | Walltime = 1866.567\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.319 | Episodes = 2108 | Steps = 6324 | Steps Per Second = 367.513\n",
            "[Learner] Action = 4.000 | Avg Td Error = -21.676 | Q = -2.525 | Reward = -24.202 | State = 390|3|53 | Steps = 636648 | Walltime = 1867.569\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.084 | Episodes = 2225 | Steps = 6675 | Steps Per Second = 378.456\n",
            "[Learner] Action = -1.000 | Avg Td Error = -18.890 | Q = -7.269 | Reward = -24.586 | State = 450|2|50 | Steps = 637005 | Walltime = 1868.571\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.259 | Episodes = 2344 | Steps = 7032 | Steps Per Second = 321.329\n",
            "[Learner] Action = 0.000 | Avg Td Error = -59.454 | Q = -0.999 | Reward = -60.439 | State = 420|2|55 | Steps = 637340 | Walltime = 1869.572\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -87.177 | Episodes = 2456 | Steps = 7368 | Steps Per Second = 301.647\n",
            "[Learner] Action = -4.000 | Avg Td Error = -11.652 | Q = -3.812 | Reward = -13.716 | State = 420|6|51 | Steps = 637700 | Walltime = 1870.573\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.731 | Episodes = 2577 | Steps = 7731 | Steps Per Second = 374.124\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.756 | Q = -0.545 | Reward = -1.302 | State = 390|4|48 | Steps = 638031 | Walltime = 1871.574\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -145.575 | Episodes = 2687 | Steps = 8061 | Steps Per Second = 384.740\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.178 | Q = -4.337 | Reward = 4.203 | State = 420|-2|49 | Steps = 638402 | Walltime = 1872.576\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.110 | Episodes = 2811 | Steps = 8433 | Steps Per Second = 365.135\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.922 | Q = -15.033 | Reward = -2.984 | State = 450|2|50 | Steps = 638739 | Walltime = 1873.577\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.530 | Episodes = 2923 | Steps = 8769 | Steps Per Second = 332.292\n",
            "[Learner] Action = -2.000 | Avg Td Error = -2.790 | Q = -0.814 | Reward = -3.603 | State = 390|5|48 | Steps = 639105 | Walltime = 1874.579\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.426 | Episodes = 3046 | Steps = 9138 | Steps Per Second = 366.389\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.864 | Q = -1.177 | Reward = -1.162 | State = 420|3|49 | Steps = 639470 | Walltime = 1875.580\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.034 | Episodes = 3169 | Steps = 9507 | Steps Per Second = 368.471\n",
            "[Learner] Action = -3.000 | Avg Td Error = -16.194 | Q = -14.973 | Reward = -27.297 | State = 450|2|50 | Steps = 639797 | Walltime = 1876.581\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -52.076 | Episodes = 3279 | Steps = 9837 | Steps Per Second = 372.044\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.925 | Q = -4.691 | Reward = 4.678 | State = 450|2|50 | Steps = 640167 | Walltime = 1877.583\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.472 | Episodes = 3402 | Steps = 10206 | Steps Per Second = 383.181\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.734 | Q = -0.276 | Reward = 2.458 | State = 390|2|46 | Steps = 640535 | Walltime = 1878.585\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.530 | Episodes = 3524 | Steps = 10572 | Steps Per Second = 305.403\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.753 | Q = -0.904 | Reward = -0.152 | State = 390|2|49 | Steps = 640842 | Walltime = 1879.586\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.184 | Episodes = 3626 | Steps = 10878 | Steps Per Second = 357.408\n",
            "[Learner] Action = 0.000 | Avg Td Error = -25.411 | Q = -1.547 | Reward = -26.958 | State = 390|1|50 | Steps = 641186 | Walltime = 1880.588\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.861 | Episodes = 3741 | Steps = 11223 | Steps Per Second = 359.522\n",
            "[Learner] Action = 0.000 | Avg Td Error = -27.121 | Q = -0.211 | Reward = -27.193 | State = 420|3|55 | Steps = 641532 | Walltime = 1881.588\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.175 | Episodes = 3857 | Steps = 11571 | Steps Per Second = 364.627\n",
            "[Learner] Action = -3.000 | Avg Td Error = -41.010 | Q = -5.516 | Reward = -45.649 | State = 420|-2|51 | Steps = 641836 | Walltime = 1882.590\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.307 | Episodes = 3956 | Steps = 11868 | Steps Per Second = 365.294\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.715 | Q = -0.452 | Reward = -5.167 | State = 390|2|48 | Steps = 642139 | Walltime = 1883.590\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.721 | Episodes = 4061 | Steps = 12183 | Steps Per Second = 362.850\n",
            "[Learner] Action = 2.000 | Avg Td Error = 9.333 | Q = -8.358 | Reward = 1.132 | State = 450|2|50 | Steps = 642483 | Walltime = 1884.591\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.917 | Episodes = 4176 | Steps = 12528 | Steps Per Second = 313.984\n",
            "[Learner] Action = 3.000 | Avg Td Error = 8.050 | Q = -3.415 | Reward = 4.636 | State = 390|-1|53 | Steps = 642808 | Walltime = 1885.593\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.480 | Episodes = 4285 | Steps = 12855 | Steps Per Second = 382.413\n",
            "[Learner] Action = 3.000 | Avg Td Error = 0.640 | Q = -1.000 | Reward = -0.360 | State = 390|7|48 | Steps = 643175 | Walltime = 1886.594\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.869 | Episodes = 4407 | Steps = 13221 | Steps Per Second = 352.038\n",
            "[Learner] Action = -2.000 | Avg Td Error = 14.206 | Q = -10.375 | Reward = 4.177 | State = 450|2|50 | Steps = 643540 | Walltime = 1887.596\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.669 | Episodes = 4529 | Steps = 13587 | Steps Per Second = 376.632\n",
            "[Learner] Action = 2.000 | Avg Td Error = -15.426 | Q = -0.556 | Reward = -15.982 | State = 390|3|56 | Steps = 643901 | Walltime = 1888.598\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.146 | Episodes = 4650 | Steps = 13950 | Steps Per Second = 349.205\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.692 | Q = -0.024 | Reward = 0.668 | State = 390|7|54 | Steps = 644270 | Walltime = 1889.599\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.918 | Episodes = 4774 | Steps = 14322 | Steps Per Second = 374.280\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.339 | Q = -8.165 | Reward = -1.457 | State = 450|2|50 | Steps = 644635 | Walltime = 1890.600\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.838 | Episodes = 4896 | Steps = 14688 | Steps Per Second = 381.300\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.412 | Episodes = 5102 | Steps = 15306 | Steps Per Second = 1952.353\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.214 | Episodes = 5730 | Steps = 17190 | Steps Per Second = 1956.299\n",
            "[Learner] Action = -4.000 | Avg Td Error = -19.146 | Q = -19.811 | Reward = -36.431 | State = 450|2|50 | Steps = 645000 | Walltime = 1893.182\n",
            "Check Point 43\n",
            "[Learner] Action = -4.000 | Avg Td Error = -6.092 | Q = -6.499 | Reward = -11.822 | State = 420|-1|49 | Steps = 645365 | Walltime = 1894.183\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.582 | Episodes = 123 | Steps = 369 | Steps Per Second = 366.379\n",
            "[Learner] Action = 2.000 | Avg Td Error = -0.010 | Q = -4.337 | Reward = -4.211 | State = 420|2|49 | Steps = 645716 | Walltime = 1895.186\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.665 | Episodes = 240 | Steps = 720 | Steps Per Second = 339.253\n",
            "[Learner] Action = -4.000 | Avg Td Error = -8.570 | Q = -1.208 | Reward = -9.777 | State = 390|2|45 | Steps = 646073 | Walltime = 1896.188\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.104 | Episodes = 359 | Steps = 1077 | Steps Per Second = 390.786\n",
            "[Learner] Action = -4.000 | Avg Td Error = 8.154 | Q = -10.434 | Reward = -0.505 | State = 420|2|51 | Steps = 646423 | Walltime = 1897.189\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.855 | Episodes = 476 | Steps = 1428 | Steps Per Second = 338.760\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.725 | Q = -0.152 | Reward = 0.572 | State = 390|6|52 | Steps = 646778 | Walltime = 1898.193\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.600 | Episodes = 594 | Steps = 1782 | Steps Per Second = 342.159\n",
            "[Learner] Action = 1.000 | Avg Td Error = -34.048 | Q = -4.536 | Reward = -36.573 | State = 420|-2|52 | Steps = 647117 | Walltime = 1899.195\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -73.547 | Episodes = 707 | Steps = 2121 | Steps Per Second = 348.345\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.490 | Q = -4.556 | Reward = 6.102 | State = 450|2|50 | Steps = 647471 | Walltime = 1900.195\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -108.779 | Episodes = 826 | Steps = 2478 | Steps Per Second = 382.076\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.246 | Q = -4.525 | Reward = 1.398 | State = 450|2|50 | Steps = 647830 | Walltime = 1901.197\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.964 | Episodes = 946 | Steps = 2838 | Steps Per Second = 334.216\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.621 | Q = -1.000 | Reward = -4.621 | State = 390|-4|51 | Steps = 648179 | Walltime = 1902.197\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.142 | Episodes = 1063 | Steps = 3189 | Steps Per Second = 392.456\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.552 | Q = -0.501 | Reward = -2.053 | State = 390|6|49 | Steps = 648523 | Walltime = 1903.200\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.932 | Episodes = 1178 | Steps = 3534 | Steps Per Second = 307.380\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.212 | Q = -0.807 | Reward = 3.405 | State = 390|1|48 | Steps = 648868 | Walltime = 1904.203\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.418 | Episodes = 1293 | Steps = 3879 | Steps Per Second = 340.143\n",
            "[Learner] Action = 4.000 | Avg Td Error = 13.827 | Q = -17.205 | Reward = -2.994 | State = 450|2|50 | Steps = 649227 | Walltime = 1905.203\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.358 | Episodes = 1414 | Steps = 4242 | Steps Per Second = 370.075\n",
            "[Learner] Action = -2.000 | Avg Td Error = -45.907 | Q = -3.367 | Reward = -49.274 | State = 390|-1|53 | Steps = 649574 | Walltime = 1906.204\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -126.885 | Episodes = 1530 | Steps = 4590 | Steps Per Second = 336.343\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.730 | Q = -1.991 | Reward = -9.721 | State = 390|6|49 | Steps = 649933 | Walltime = 1907.207\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.018 | Episodes = 1650 | Steps = 4950 | Steps Per Second = 374.648\n",
            "[Learner] Action = 3.000 | Avg Td Error = -4.054 | Q = -1.841 | Reward = -5.895 | State = 390|4|50 | Steps = 650292 | Walltime = 1908.210\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -78.620 | Episodes = 1769 | Steps = 5307 | Steps Per Second = 344.841\n",
            "[Learner] Action = -3.000 | Avg Td Error = 0.967 | Q = -2.406 | Reward = -0.628 | State = 420|4|52 | Steps = 650651 | Walltime = 1909.210\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -143.217 | Episodes = 1890 | Steps = 5670 | Steps Per Second = 376.993\n",
            "[Learner] Action = -3.000 | Avg Td Error = 9.678 | Q = -15.185 | Reward = -2.274 | State = 450|2|50 | Steps = 651020 | Walltime = 1910.212\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.577 | Episodes = 2013 | Steps = 6039 | Steps Per Second = 393.413\n",
            "[Learner] Action = 3.000 | Avg Td Error = 10.715 | Q = -4.710 | Reward = 6.241 | State = 420|-1|50 | Steps = 651370 | Walltime = 1911.213\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -86.714 | Episodes = 2129 | Steps = 6387 | Steps Per Second = 340.576\n",
            "[Learner] Action = 3.000 | Avg Td Error = -10.144 | Q = -2.109 | Reward = -12.252 | State = 390|2|47 | Steps = 651729 | Walltime = 1912.214\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.565 | Episodes = 2250 | Steps = 6750 | Steps Per Second = 374.113\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.641 | Q = -0.231 | Reward = 1.461 | State = 420|5|51 | Steps = 652098 | Walltime = 1913.216\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -101.395 | Episodes = 2373 | Steps = 7119 | Steps Per Second = 384.305\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.345 | Q = -0.426 | Reward = -1.771 | State = 390|5|48 | Steps = 652456 | Walltime = 1914.217\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.022 | Episodes = 2492 | Steps = 7476 | Steps Per Second = 335.258\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.801 | Q = -0.298 | Reward = -2.099 | State = 390|2|46 | Steps = 652812 | Walltime = 1915.217\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.790 | Episodes = 2612 | Steps = 7836 | Steps Per Second = 374.503\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.652 | Q = -8.124 | Reward = -12.413 | State = 450|2|50 | Steps = 653176 | Walltime = 1916.217\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.809 | Episodes = 2734 | Steps = 8202 | Steps Per Second = 374.091\n",
            "[Learner] Action = -3.000 | Avg Td Error = -9.435 | Q = -3.390 | Reward = -10.214 | State = 420|1|51 | Steps = 653541 | Walltime = 1917.217\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.246 | Episodes = 2856 | Steps = 8568 | Steps Per Second = 378.354\n",
            "[Learner] Action = -1.000 | Avg Td Error = -15.455 | Q = -3.680 | Reward = -16.690 | State = 420|0|51 | Steps = 653909 | Walltime = 1918.218\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.258 | Episodes = 2979 | Steps = 8937 | Steps Per Second = 395.291\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.240 | Q = -1.276 | Reward = 3.031 | State = 420|-2|47 | Steps = 654281 | Walltime = 1919.219\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.494 | Episodes = 3104 | Steps = 9312 | Steps Per Second = 382.483\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.226 | Q = -0.800 | Reward = -6.994 | State = 420|6|54 | Steps = 654613 | Walltime = 1920.220\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.511 | Episodes = 3214 | Steps = 9642 | Steps Per Second = 336.037\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.878 | Q = -0.679 | Reward = 1.198 | State = 390|2|47 | Steps = 654975 | Walltime = 1921.222\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.059 | Episodes = 3336 | Steps = 10008 | Steps Per Second = 372.595\n",
            "[Learner] Action = -3.000 | Avg Td Error = -9.181 | Q = -1.875 | Reward = -9.420 | State = 420|3|48 | Steps = 655318 | Walltime = 1922.222\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.010 | Episodes = 3450 | Steps = 10350 | Steps Per Second = 340.290\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.892 | Q = -2.905 | Reward = -12.700 | State = 420|6|48 | Steps = 655677 | Walltime = 1923.223\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.981 | Episodes = 3571 | Steps = 10713 | Steps Per Second = 392.872\n",
            "[Learner] Action = 4.000 | Avg Td Error = -1.902 | Q = -17.198 | Reward = -18.977 | State = 450|2|50 | Steps = 656047 | Walltime = 1924.225\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.312 | Episodes = 3695 | Steps = 11085 | Steps Per Second = 393.462\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.277 | Q = -3.210 | Reward = -4.487 | State = 390|-1|51 | Steps = 656423 | Walltime = 1925.226\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.836 | Episodes = 3820 | Steps = 11460 | Steps Per Second = 340.686\n",
            "[Learner] Action = 4.000 | Avg Td Error = 0.070 | Q = -3.968 | Reward = -3.866 | State = 420|1|49 | Steps = 656790 | Walltime = 1926.227\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -107.846 | Episodes = 3943 | Steps = 11829 | Steps Per Second = 368.979\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.119 | Q = -4.225 | Reward = 5.550 | State = 450|2|50 | Steps = 657154 | Walltime = 1927.229\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.178 | Episodes = 4065 | Steps = 12195 | Steps Per Second = 384.223\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.650 | Q = -10.707 | Reward = -11.631 | State = 450|2|50 | Steps = 657518 | Walltime = 1928.230\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -134.638 | Episodes = 4187 | Steps = 12561 | Steps Per Second = 387.000\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.140 | Q = -0.706 | Reward = 4.434 | State = 390|3|56 | Steps = 657888 | Walltime = 1929.231\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -70.808 | Episodes = 4311 | Steps = 12933 | Steps Per Second = 378.433\n",
            "[Learner] Action = -3.000 | Avg Td Error = 12.487 | Q = -15.024 | Reward = -0.245 | State = 450|2|50 | Steps = 658246 | Walltime = 1930.234\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.341 | Episodes = 4429 | Steps = 13287 | Steps Per Second = 338.023\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.660 | Q = -14.873 | Reward = -3.958 | State = 450|2|50 | Steps = 658599 | Walltime = 1931.235\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.837 | Episodes = 4547 | Steps = 13641 | Steps Per Second = 320.576\n",
            "[Learner] Action = -1.000 | Avg Td Error = 9.642 | Q = -7.015 | Reward = 3.896 | State = 450|2|50 | Steps = 658937 | Walltime = 1932.235\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.223 | Episodes = 4660 | Steps = 13980 | Steps Per Second = 330.798\n",
            "[Learner] Action = -4.000 | Avg Td Error = -11.511 | Q = -0.672 | Reward = -12.183 | State = 390|1|44 | Steps = 659296 | Walltime = 1933.238\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.252 | Episodes = 4783 | Steps = 14349 | Steps Per Second = 398.433\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.507 | Q = -4.397 | Reward = -2.200 | State = 450|2|50 | Steps = 659664 | Walltime = 1934.239\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.694 | Episodes = 4903 | Steps = 14709 | Steps Per Second = 340.797\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.116 | Episodes = 5095 | Steps = 15285 | Steps Per Second = 1797.816\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.610 | Episodes = 5725 | Steps = 17175 | Steps Per Second = 1948.121\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.190 | Q = -0.611 | Reward = -0.422 | State = 390|5|51 | Steps = 660000 | Walltime = 1936.824\n",
            "Check Point 44\n",
            "[Learner] Action = -2.000 | Avg Td Error = 2.761 | Q = -10.564 | Reward = -4.357 | State = 450|2|50 | Steps = 660363 | Walltime = 1937.825\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.759 | Episodes = 123 | Steps = 369 | Steps Per Second = 333.782\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.662 | Q = -1.262 | Reward = 2.854 | State = 420|3|51 | Steps = 660695 | Walltime = 1938.826\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.875 | Episodes = 234 | Steps = 702 | Steps Per Second = 321.509\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.383 | Q = -0.787 | Reward = -4.170 | State = 390|6|53 | Steps = 661032 | Walltime = 1939.827\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.519 | Episodes = 347 | Steps = 1041 | Steps Per Second = 359.245\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.267 | Q = -2.968 | Reward = -8.126 | State = 420|5|50 | Steps = 661393 | Walltime = 1940.827\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.851 | Episodes = 468 | Steps = 1404 | Steps Per Second = 335.804\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.300 | Q = -0.642 | Reward = -0.942 | State = 390|4|51 | Steps = 661729 | Walltime = 1941.831\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.596 | Episodes = 580 | Steps = 1740 | Steps Per Second = 364.205\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.028 | Q = -4.300 | Reward = 2.820 | State = 450|2|50 | Steps = 662080 | Walltime = 1942.832\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.792 | Episodes = 698 | Steps = 2094 | Steps Per Second = 332.740\n",
            "[Learner] Action = -2.000 | Avg Td Error = 8.578 | Q = -4.913 | Reward = 5.294 | State = 420|2|52 | Steps = 662415 | Walltime = 1943.832\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.291 | Episodes = 811 | Steps = 2433 | Steps Per Second = 379.598\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.813 | Q = -7.089 | Reward = -5.078 | State = 450|2|50 | Steps = 662784 | Walltime = 1944.834\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.766 | Episodes = 934 | Steps = 2802 | Steps Per Second = 366.219\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.111 | Q = -0.776 | Reward = 2.817 | State = 420|4|52 | Steps = 663155 | Walltime = 1945.836\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.762 | Episodes = 1058 | Steps = 3174 | Steps Per Second = 363.112\n",
            "[Learner] Action = -2.000 | Avg Td Error = 6.818 | Q = -10.577 | Reward = -1.275 | State = 450|2|50 | Steps = 663511 | Walltime = 1946.836\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.742 | Episodes = 1176 | Steps = 3528 | Steps Per Second = 300.208\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.004 | Q = -4.010 | Reward = 4.410 | State = 450|2|50 | Steps = 663860 | Walltime = 1947.837\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.127 | Episodes = 1294 | Steps = 3882 | Steps Per Second = 325.645\n",
            "[Learner] Action = 1.000 | Avg Td Error = -7.591 | Q = -0.622 | Reward = -8.213 | State = 390|-2|46 | Steps = 664217 | Walltime = 1948.839\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.143 | Episodes = 1412 | Steps = 4236 | Steps Per Second = 281.208\n",
            "[Learner] Action = 0.000 | Avg Td Error = -20.676 | Q = -3.942 | Reward = -22.383 | State = 450|2|50 | Steps = 664576 | Walltime = 1949.840\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -204.630 | Episodes = 1533 | Steps = 4599 | Steps Per Second = 391.394\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.233 | Q = -1.712 | Reward = -2.046 | State = 420|1|53 | Steps = 664949 | Walltime = 1950.841\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.924 | Episodes = 1658 | Steps = 4974 | Steps Per Second = 385.211\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.424 | Q = -4.144 | Reward = 4.694 | State = 450|2|50 | Steps = 665301 | Walltime = 1951.843\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.829 | Episodes = 1775 | Steps = 5325 | Steps Per Second = 394.103\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.036 | Q = -0.275 | Reward = -0.117 | State = 420|6|50 | Steps = 665674 | Walltime = 1952.846\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.256 | Episodes = 1899 | Steps = 5697 | Steps Per Second = 376.802\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.157 | Q = -0.028 | Reward = -0.005 | State = 420|5|49 | Steps = 666038 | Walltime = 1953.846\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.308 | Episodes = 2021 | Steps = 6063 | Steps Per Second = 378.024\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.566 | Q = -0.105 | Reward = 0.461 | State = 390|4|49 | Steps = 666404 | Walltime = 1954.847\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.331 | Episodes = 2144 | Steps = 6432 | Steps Per Second = 382.169\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.885 | Q = -0.157 | Reward = 0.757 | State = 420|3|47 | Steps = 666761 | Walltime = 1955.849\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.529 | Episodes = 2263 | Steps = 6789 | Steps Per Second = 396.687\n",
            "[Learner] Action = 2.000 | Avg Td Error = 3.448 | Q = -8.929 | Reward = -5.084 | State = 450|2|50 | Steps = 667126 | Walltime = 1956.851\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.251 | Episodes = 2385 | Steps = 7155 | Steps Per Second = 367.846\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.481 | Q = -0.430 | Reward = -2.911 | State = 390|6|49 | Steps = 667494 | Walltime = 1957.853\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.695 | Episodes = 2508 | Steps = 7524 | Steps Per Second = 383.777\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.248 | Q = -2.089 | Reward = 6.160 | State = 390|-1|50 | Steps = 667856 | Walltime = 1958.855\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -132.633 | Episodes = 2629 | Steps = 7887 | Steps Per Second = 390.652\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.893 | Q = -4.009 | Reward = -16.476 | State = 420|6|51 | Steps = 668212 | Walltime = 1959.857\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.787 | Episodes = 2748 | Steps = 8244 | Steps Per Second = 386.394\n",
            "[Learner] Action = 4.000 | Avg Td Error = 8.542 | Q = -16.979 | Reward = -8.204 | State = 450|2|50 | Steps = 668586 | Walltime = 1960.858\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.070 | Episodes = 2873 | Steps = 8619 | Steps Per Second = 390.023\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.459 | Q = -4.422 | Reward = -0.132 | State = 450|2|50 | Steps = 668954 | Walltime = 1961.860\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.597 | Episodes = 2996 | Steps = 8988 | Steps Per Second = 387.214\n",
            "[Learner] Action = 1.000 | Avg Td Error = 3.749 | Q = -1.558 | Reward = 2.191 | State = 390|1|49 | Steps = 669301 | Walltime = 1962.861\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.506 | Episodes = 3112 | Steps = 9336 | Steps Per Second = 377.831\n",
            "[Learner] Action = -3.000 | Avg Td Error = -44.963 | Q = -14.914 | Reward = -59.281 | State = 450|2|50 | Steps = 669669 | Walltime = 1963.861\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -217.457 | Episodes = 3235 | Steps = 9705 | Steps Per Second = 400.985\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.845 | Q = -4.588 | Reward = -7.489 | State = 450|2|50 | Steps = 670014 | Walltime = 1964.862\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.476 | Episodes = 3350 | Steps = 10050 | Steps Per Second = 380.689\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.418 | Q = -3.585 | Reward = -10.763 | State = 420|4|51 | Steps = 670387 | Walltime = 1965.865\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -251.292 | Episodes = 3474 | Steps = 10422 | Steps Per Second = 383.590\n",
            "[Learner] Action = 4.000 | Avg Td Error = -99.769 | Q = -8.080 | Reward = -107.801 | State = 420|2|52 | Steps = 670753 | Walltime = 1966.866\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -146.035 | Episodes = 3596 | Steps = 10788 | Steps Per Second = 365.814\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.006 | Q = -1.661 | Reward = 4.345 | State = 390|1|54 | Steps = 671107 | Walltime = 1967.868\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.319 | Episodes = 3714 | Steps = 11142 | Steps Per Second = 382.227\n",
            "[Learner] Action = 4.000 | Avg Td Error = -10.280 | Q = -3.775 | Reward = -13.953 | State = 420|6|51 | Steps = 671474 | Walltime = 1968.868\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.498 | Episodes = 3835 | Steps = 11505 | Steps Per Second = 334.394\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.111 | Q = 0.016 | Reward = 0.127 | State = 390|3|46 | Steps = 671824 | Walltime = 1969.870\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.612 | Episodes = 3951 | Steps = 11853 | Steps Per Second = 377.367\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.690 | Q = -3.257 | Reward = 1.432 | State = 390|-2|51 | Steps = 672187 | Walltime = 1970.872\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.469 | Episodes = 4074 | Steps = 12222 | Steps Per Second = 377.468\n",
            "[Learner] Action = 3.000 | Avg Td Error = 2.637 | Q = -4.399 | Reward = -1.297 | State = 420|1|52 | Steps = 672551 | Walltime = 1971.873\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.864 | Episodes = 4196 | Steps = 12588 | Steps Per Second = 353.582\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.042 | Q = -15.086 | Reward = -16.635 | State = 450|2|50 | Steps = 672912 | Walltime = 1972.873\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.714 | Episodes = 4315 | Steps = 12945 | Steps Per Second = 341.195\n",
            "[Learner] Action = 2.000 | Avg Td Error = 11.910 | Q = -8.318 | Reward = 3.614 | State = 450|2|50 | Steps = 673255 | Walltime = 1973.874\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.044 | Episodes = 4431 | Steps = 13293 | Steps Per Second = 372.342\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.895 | Q = -0.297 | Reward = 2.267 | State = 420|4|54 | Steps = 673628 | Walltime = 1974.874\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.917 | Episodes = 4556 | Steps = 13668 | Steps Per Second = 328.845\n",
            "[Learner] Action = 3.000 | Avg Td Error = -34.498 | Q = -3.701 | Reward = -37.087 | State = 420|0|50 | Steps = 673972 | Walltime = 1975.874\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.508 | Episodes = 4671 | Steps = 14013 | Steps Per Second = 377.299\n",
            "[Learner] Action = -4.000 | Avg Td Error = -8.891 | Q = -4.420 | Reward = -11.417 | State = 420|6|49 | Steps = 674338 | Walltime = 1976.876\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.365 | Episodes = 4793 | Steps = 14379 | Steps Per Second = 379.781\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.121 | Q = -1.600 | Reward = 2.522 | State = 390|-2|54 | Steps = 674701 | Walltime = 1977.877\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -93.441 | Episodes = 4915 | Steps = 14745 | Steps Per Second = 393.462\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.363 | Episodes = 5180 | Steps = 15540 | Steps Per Second = 2003.968\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.055 | Episodes = 5808 | Steps = 17424 | Steps Per Second = 1853.153\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.378 | Q = -15.173 | Reward = -3.386 | State = 450|2|50 | Steps = 675000 | Walltime = 1980.309\n",
            "Check Point 45\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.133 | Q = -1.267 | Reward = 1.897 | State = 420|2|47 | Steps = 675336 | Walltime = 1981.312\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -182.938 | Episodes = 113 | Steps = 339 | Steps Per Second = 290.424\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.652 | Q = -0.909 | Reward = -6.560 | State = 390|6|51 | Steps = 675672 | Walltime = 1982.314\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.745 | Episodes = 225 | Steps = 675 | Steps Per Second = 338.432\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.256 | Q = -0.030 | Reward = 1.350 | State = 420|4|46 | Steps = 676029 | Walltime = 1983.316\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.943 | Episodes = 344 | Steps = 1032 | Steps Per Second = 330.382\n",
            "[Learner] Action = -2.000 | Avg Td Error = 13.805 | Q = -10.875 | Reward = 3.719 | State = 450|2|50 | Steps = 676399 | Walltime = 1984.318\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.610 | Episodes = 468 | Steps = 1404 | Steps Per Second = 367.954\n",
            "[Learner] Action = -3.000 | Avg Td Error = 18.156 | Q = -15.076 | Reward = 3.359 | State = 450|2|50 | Steps = 676765 | Walltime = 1985.318\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.498 | Episodes = 591 | Steps = 1773 | Steps Per Second = 373.413\n",
            "[Learner] Action = 4.000 | Avg Td Error = -23.580 | Q = -16.149 | Reward = -39.515 | State = 450|2|50 | Steps = 677138 | Walltime = 1986.320\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.826 | Episodes = 716 | Steps = 2148 | Steps Per Second = 381.277\n",
            "[Learner] Action = 0.000 | Avg Td Error = -17.141 | Q = -4.506 | Reward = -18.325 | State = 450|2|50 | Steps = 677490 | Walltime = 1987.321\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.184 | Episodes = 833 | Steps = 2499 | Steps Per Second = 318.821\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.587 | Q = -0.087 | Reward = -0.673 | State = 390|5|50 | Steps = 677823 | Walltime = 1988.323\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.387 | Episodes = 944 | Steps = 2832 | Steps Per Second = 318.902\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.512 | Q = -1.733 | Reward = -1.221 | State = 390|0|49 | Steps = 678178 | Walltime = 1989.324\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.488 | Episodes = 1063 | Steps = 3189 | Steps Per Second = 381.555\n",
            "[Learner] Action = 4.000 | Avg Td Error = 10.621 | Q = -16.176 | Reward = -5.334 | State = 450|2|50 | Steps = 678534 | Walltime = 1990.326\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.091 | Episodes = 1181 | Steps = 3543 | Steps Per Second = 322.226\n",
            "[Learner] Action = 1.000 | Avg Td Error = -5.346 | Q = -5.314 | Reward = -9.448 | State = 450|2|50 | Steps = 678881 | Walltime = 1991.328\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.070 | Episodes = 1297 | Steps = 3891 | Steps Per Second = 376.013\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.573 | Q = -2.031 | Reward = -4.604 | State = 390|2|51 | Steps = 679238 | Walltime = 1992.329\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.294 | Episodes = 1417 | Steps = 4251 | Steps Per Second = 389.419\n",
            "[Learner] Action = -4.000 | Avg Td Error = 9.192 | Q = -20.442 | Reward = -7.227 | State = 450|2|50 | Steps = 679613 | Walltime = 1993.330\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -80.610 | Episodes = 1542 | Steps = 4626 | Steps Per Second = 374.413\n",
            "[Learner] Action = 4.000 | Avg Td Error = -34.768 | Q = -5.480 | Reward = -39.800 | State = 420|0|49 | Steps = 679979 | Walltime = 1994.332\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.582 | Episodes = 1664 | Steps = 4992 | Steps Per Second = 349.157\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.166 | Q = -0.124 | Reward = -4.180 | State = 420|6|51 | Steps = 680338 | Walltime = 1995.334\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.245 | Episodes = 1784 | Steps = 5352 | Steps Per Second = 385.069\n",
            "[Learner] Action = 2.000 | Avg Td Error = -15.726 | Q = -8.493 | Reward = -23.606 | State = 450|2|50 | Steps = 680707 | Walltime = 1996.336\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.591 | Episodes = 1907 | Steps = 5721 | Steps Per Second = 398.698\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.960 | Q = -9.892 | Reward = -7.684 | State = 420|2|51 | Steps = 681070 | Walltime = 1997.338\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.957 | Episodes = 2028 | Steps = 6084 | Steps Per Second = 371.802\n",
            "[Learner] Action = -1.000 | Avg Td Error = -12.377 | Q = -4.470 | Reward = -14.893 | State = 420|2|51 | Steps = 681433 | Walltime = 1998.340\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.900 | Episodes = 2149 | Steps = 6447 | Steps Per Second = 389.986\n",
            "[Learner] Action = -3.000 | Avg Td Error = 10.880 | Q = -14.752 | Reward = -1.505 | State = 450|2|50 | Steps = 681763 | Walltime = 1999.340\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.990 | Episodes = 2259 | Steps = 6777 | Steps Per Second = 331.443\n",
            "[Learner] Action = -4.000 | Avg Td Error = -14.996 | Q = -3.955 | Reward = -16.545 | State = 420|4|50 | Steps = 682123 | Walltime = 2000.342\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.759 | Episodes = 2380 | Steps = 7140 | Steps Per Second = 365.432\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.171 | Q = -16.163 | Reward = -14.720 | State = 450|2|50 | Steps = 682453 | Walltime = 2001.343\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.816 | Episodes = 2489 | Steps = 7467 | Steps Per Second = 305.945\n",
            "[Learner] Action = -4.000 | Avg Td Error = 3.102 | Q = -4.974 | Reward = -1.022 | State = 420|1|50 | Steps = 682781 | Walltime = 2002.343\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.654 | Episodes = 2600 | Steps = 7800 | Steps Per Second = 359.091\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.272 | Q = -1.540 | Reward = 2.733 | State = 390|2|49 | Steps = 683116 | Walltime = 2003.344\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.021 | Episodes = 2712 | Steps = 8136 | Steps Per Second = 356.891\n",
            "[Learner] Action = 3.000 | Avg Td Error = -8.727 | Q = -8.347 | Reward = -16.902 | State = 420|2|50 | Steps = 683424 | Walltime = 2004.347\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.201 | Episodes = 2814 | Steps = 8442 | Steps Per Second = 321.904\n",
            "[Learner] Action = -1.000 | Avg Td Error = -12.798 | Q = -1.606 | Reward = -12.436 | State = 420|1|48 | Steps = 683742 | Walltime = 2005.348\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.375 | Episodes = 2921 | Steps = 8763 | Steps Per Second = 356.366\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.741 | Q = -4.616 | Reward = 4.532 | State = 450|2|50 | Steps = 684048 | Walltime = 2006.350\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.548 | Episodes = 3022 | Steps = 9066 | Steps Per Second = 327.245\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.044 | Q = -0.126 | Reward = 0.918 | State = 390|4|48 | Steps = 684374 | Walltime = 2007.352\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.407 | Episodes = 3131 | Steps = 9393 | Steps Per Second = 335.500\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.910 | Q = -0.269 | Reward = -0.815 | State = 420|6|50 | Steps = 684678 | Walltime = 2008.354\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.853 | Episodes = 3232 | Steps = 9696 | Steps Per Second = 322.110\n",
            "[Learner] Action = 1.000 | Avg Td Error = -24.181 | Q = -2.376 | Reward = -24.604 | State = 420|-1|48 | Steps = 685014 | Walltime = 2009.354\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.343 | Episodes = 3344 | Steps = 10032 | Steps Per Second = 342.821\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.280 | Q = -0.068 | Reward = -3.242 | State = 420|6|52 | Steps = 685346 | Walltime = 2010.355\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.299 | Episodes = 3456 | Steps = 10368 | Steps Per Second = 358.488\n",
            "[Learner] Action = 2.000 | Avg Td Error = -7.536 | Q = -1.045 | Reward = -8.581 | State = 390|4|48 | Steps = 685692 | Walltime = 2011.357\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.540 | Episodes = 3572 | Steps = 10716 | Steps Per Second = 365.453\n",
            "[Learner] Action = 2.000 | Avg Td Error = -8.153 | Q = -1.453 | Reward = -9.581 | State = 420|5|51 | Steps = 686051 | Walltime = 2012.357\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.621 | Episodes = 3693 | Steps = 11079 | Steps Per Second = 388.553\n",
            "[Learner] Action = -1.000 | Avg Td Error = -16.971 | Q = -5.151 | Reward = -21.051 | State = 420|2|52 | Steps = 686383 | Walltime = 2013.359\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.460 | Episodes = 3803 | Steps = 11409 | Steps Per Second = 343.214\n",
            "[Learner] Action = 2.000 | Avg Td Error = -11.083 | Q = -8.272 | Reward = -18.473 | State = 450|2|50 | Steps = 686721 | Walltime = 2014.362\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.154 | Episodes = 3916 | Steps = 11748 | Steps Per Second = 307.373\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.648 | Q = -0.740 | Reward = 3.492 | State = 420|0|47 | Steps = 687057 | Walltime = 2015.363\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -140.551 | Episodes = 4028 | Steps = 12084 | Steps Per Second = 328.793\n",
            "[Learner] Action = 1.000 | Avg Td Error = -6.994 | Q = -4.997 | Reward = -10.767 | State = 450|2|50 | Steps = 687416 | Walltime = 2016.365\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.806 | Episodes = 4149 | Steps = 12447 | Steps Per Second = 326.058\n",
            "[Learner] Action = -3.000 | Avg Td Error = -15.049 | Q = -14.970 | Reward = -27.657 | State = 450|2|50 | Steps = 687777 | Walltime = 2017.368\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.914 | Episodes = 4270 | Steps = 12810 | Steps Per Second = 308.246\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.028 | Q = -2.719 | Reward = 3.501 | State = 420|2|49 | Steps = 688139 | Walltime = 2018.370\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.096 | Episodes = 4391 | Steps = 13173 | Steps Per Second = 365.464\n",
            "[Learner] Action = -1.000 | Avg Td Error = -37.044 | Q = -2.354 | Reward = -39.398 | State = 390|-1|50 | Steps = 688504 | Walltime = 2019.370\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -44.970 | Episodes = 4513 | Steps = 13539 | Steps Per Second = 375.811\n",
            "[Learner] Action = -1.000 | Avg Td Error = 8.092 | Q = -2.673 | Reward = 5.742 | State = 420|-1|48 | Steps = 688863 | Walltime = 2020.371\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.426 | Episodes = 4633 | Steps = 13899 | Steps Per Second = 361.142\n",
            "[Learner] Action = -2.000 | Avg Td Error = 2.281 | Q = -2.672 | Reward = 1.159 | State = 420|3|51 | Steps = 689235 | Walltime = 2021.372\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.672 | Episodes = 4758 | Steps = 14274 | Steps Per Second = 370.238\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.733 | Q = -5.209 | Reward = -13.571 | State = 420|2|47 | Steps = 689606 | Walltime = 2022.373\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.356 | Episodes = 4880 | Steps = 14640 | Steps Per Second = 331.094\n",
            "[Learner] Action = 3.000 | Avg Td Error = -20.951 | Q = -0.270 | Reward = -21.229 | State = 420|5|55 | Steps = 689938 | Walltime = 2023.374\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.861 | Episodes = 4991 | Steps = 14973 | Steps Per Second = 338.560\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.757 | Episodes = 5572 | Steps = 16716 | Steps Per Second = 1999.827\n",
            "[Learner] Action = 3.000 | Avg Td Error = -36.103 | Q = -0.944 | Reward = -37.047 | State = 390|6|46 | Steps = 690000 | Walltime = 2025.184\n",
            "Check Point 46\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.138 | Q = -10.905 | Reward = -6.997 | State = 450|2|50 | Steps = 690342 | Walltime = 2026.185\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.945 | Episodes = 115 | Steps = 345 | Steps Per Second = 285.204\n",
            "[Learner] Action = -1.000 | Avg Td Error = -42.425 | Q = -2.431 | Reward = -44.856 | State = 390|-1|51 | Steps = 690695 | Walltime = 2027.185\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.086 | Episodes = 233 | Steps = 699 | Steps Per Second = 386.988\n",
            "[Learner] Action = -4.000 | Avg Td Error = -41.419 | Q = -3.091 | Reward = -44.510 | State = 390|-2|52 | Steps = 691065 | Walltime = 2028.186\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.695 | Episodes = 357 | Steps = 1071 | Steps Per Second = 394.684\n",
            "[Learner] Action = -2.000 | Avg Td Error = 8.393 | Q = -11.096 | Reward = -0.285 | State = 450|2|50 | Steps = 691423 | Walltime = 2029.187\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.292 | Episodes = 476 | Steps = 1428 | Steps Per Second = 328.948\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.191 | Q = -1.944 | Reward = 6.247 | State = 390|-2|49 | Steps = 691782 | Walltime = 2030.188\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.110 | Episodes = 596 | Steps = 1788 | Steps Per Second = 384.869\n",
            "[Learner] Action = 4.000 | Avg Td Error = -25.354 | Q = -6.635 | Reward = -29.902 | State = 420|-2|50 | Steps = 692130 | Walltime = 2031.188\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.250 | Episodes = 713 | Steps = 2139 | Steps Per Second = 322.457\n",
            "[Learner] Action = 1.000 | Avg Td Error = -8.804 | Q = -0.342 | Reward = -9.146 | State = 390|7|51 | Steps = 692477 | Walltime = 2032.189\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.264 | Episodes = 829 | Steps = 2487 | Steps Per Second = 343.120\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.108 | Q = -1.248 | Reward = 5.334 | State = 420|3|52 | Steps = 692820 | Walltime = 2033.191\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.759 | Episodes = 944 | Steps = 2832 | Steps Per Second = 332.925\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.291 | Q = -4.611 | Reward = -2.045 | State = 450|2|50 | Steps = 693178 | Walltime = 2034.192\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.259 | Episodes = 1064 | Steps = 3192 | Steps Per Second = 376.013\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.212 | Q = -1.373 | Reward = 5.839 | State = 390|0|55 | Steps = 693542 | Walltime = 2035.193\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.816 | Episodes = 1186 | Steps = 3558 | Steps Per Second = 369.141\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.770 | Q = -3.784 | Reward = -14.043 | State = 420|4|51 | Steps = 693894 | Walltime = 2036.194\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.379 | Episodes = 1304 | Steps = 3912 | Steps Per Second = 378.775\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.508 | Q = -0.978 | Reward = -5.485 | State = 390|2|55 | Steps = 694260 | Walltime = 2037.196\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.556 | Episodes = 1426 | Steps = 4278 | Steps Per Second = 372.672\n",
            "[Learner] Action = 4.000 | Avg Td Error = 12.675 | Q = -16.616 | Reward = -3.478 | State = 450|2|50 | Steps = 694620 | Walltime = 2038.196\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.502 | Episodes = 1547 | Steps = 4641 | Steps Per Second = 393.425\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.054 | Q = -1.962 | Reward = -10.016 | State = 390|-1|49 | Steps = 694988 | Walltime = 2039.198\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.936 | Episodes = 1670 | Steps = 5010 | Steps Per Second = 372.077\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.455 | Q = -1.647 | Reward = -10.843 | State = 420|3|47 | Steps = 695358 | Walltime = 2040.200\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.469 | Episodes = 1794 | Steps = 5382 | Steps Per Second = 381.428\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.121 | Q = -4.807 | Reward = 0.250 | State = 450|2|50 | Steps = 695725 | Walltime = 2041.201\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.117 | Episodes = 1916 | Steps = 5748 | Steps Per Second = 367.631\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.616 | Q = -0.557 | Reward = 1.080 | State = 420|5|53 | Steps = 696087 | Walltime = 2042.201\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.755 | Episodes = 2037 | Steps = 6111 | Steps Per Second = 385.730\n",
            "[Learner] Action = -2.000 | Avg Td Error = -10.098 | Q = -0.406 | Reward = -10.505 | State = 390|-2|45 | Steps = 696451 | Walltime = 2043.202\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.897 | Episodes = 2159 | Steps = 6477 | Steps Per Second = 387.895\n",
            "[Learner] Action = 2.000 | Avg Td Error = 9.929 | Q = -8.500 | Reward = 1.584 | State = 450|2|50 | Steps = 696822 | Walltime = 2044.202\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.457 | Episodes = 2282 | Steps = 6846 | Steps Per Second = 334.803\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.360 | Q = -6.991 | Reward = -5.298 | State = 450|2|50 | Steps = 697157 | Walltime = 2045.202\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.832 | Episodes = 2394 | Steps = 7182 | Steps Per Second = 336.307\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.673 | Q = -4.838 | Reward = 5.247 | State = 450|2|50 | Steps = 697504 | Walltime = 2046.205\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.157 | Episodes = 2511 | Steps = 7533 | Steps Per Second = 394.510\n",
            "[Learner] Action = 4.000 | Avg Td Error = 10.853 | Q = -16.889 | Reward = -5.805 | State = 450|2|50 | Steps = 697873 | Walltime = 2047.205\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.974 | Episodes = 2634 | Steps = 7902 | Steps Per Second = 371.418\n",
            "[Learner] Action = -3.000 | Avg Td Error = -59.203 | Q = -3.024 | Reward = -61.339 | State = 420|0|52 | Steps = 698231 | Walltime = 2048.207\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.066 | Episodes = 2753 | Steps = 8259 | Steps Per Second = 349.603\n",
            "[Learner] Action = 3.000 | Avg Td Error = 10.543 | Q = -4.919 | Reward = 6.000 | State = 420|-1|51 | Steps = 698586 | Walltime = 2049.209\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.746 | Episodes = 2873 | Steps = 8619 | Steps Per Second = 381.092\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.574 | Q = -0.084 | Reward = -3.658 | State = 390|4|45 | Steps = 698950 | Walltime = 2050.212\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.038 | Episodes = 2995 | Steps = 8985 | Steps Per Second = 384.622\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.447 | Q = -2.248 | Reward = -1.515 | State = 420|4|50 | Steps = 699302 | Walltime = 2051.213\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.252 | Episodes = 3111 | Steps = 9333 | Steps Per Second = 347.096\n",
            "[Learner] Action = 4.000 | Avg Td Error = -39.617 | Q = -17.117 | Reward = -56.712 | State = 450|2|50 | Steps = 699633 | Walltime = 2052.213\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.195 | Episodes = 3221 | Steps = 9663 | Steps Per Second = 322.102\n",
            "[Learner] Action = -4.000 | Avg Td Error = -59.064 | Q = -7.109 | Reward = -65.501 | State = 420|-2|49 | Steps = 699984 | Walltime = 2053.215\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.189 | Episodes = 3340 | Steps = 10020 | Steps Per Second = 382.762\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.198 | Q = -0.232 | Reward = -1.283 | State = 420|6|53 | Steps = 700338 | Walltime = 2054.215\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.758 | Episodes = 3459 | Steps = 10377 | Steps Per Second = 295.964\n",
            "[Learner] Action = -4.000 | Avg Td Error = 14.999 | Q = -20.358 | Reward = -2.803 | State = 450|2|50 | Steps = 700703 | Walltime = 2055.217\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.148 | Episodes = 3581 | Steps = 10743 | Steps Per Second = 376.779\n",
            "[Learner] Action = 4.000 | Avg Td Error = -13.152 | Q = -0.302 | Reward = -12.823 | State = 420|-1|56 | Steps = 701070 | Walltime = 2056.219\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -62.857 | Episodes = 3703 | Steps = 11109 | Steps Per Second = 368.708\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.099 | Q = -0.305 | Reward = 0.962 | State = 420|6|50 | Steps = 701421 | Walltime = 2057.220\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.573 | Episodes = 3820 | Steps = 11460 | Steps Per Second = 333.950\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.799 | Q = -6.904 | Reward = -3.238 | State = 450|2|50 | Steps = 701778 | Walltime = 2058.220\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -127.252 | Episodes = 3940 | Steps = 11820 | Steps Per Second = 338.897\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.567 | Q = -6.838 | Reward = -2.404 | State = 450|2|50 | Steps = 702141 | Walltime = 2059.221\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.774 | Episodes = 4062 | Steps = 12186 | Steps Per Second = 360.490\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.254 | Q = -5.447 | Reward = 1.448 | State = 450|2|50 | Steps = 702493 | Walltime = 2060.222\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.799 | Episodes = 4180 | Steps = 12540 | Steps Per Second = 360.573\n",
            "[Learner] Action = -3.000 | Avg Td Error = 12.846 | Q = -15.521 | Reward = -0.215 | State = 450|2|50 | Steps = 702857 | Walltime = 2061.222\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.856 | Episodes = 4302 | Steps = 12906 | Steps Per Second = 373.137\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.408 | Q = -11.042 | Reward = -5.877 | State = 450|2|50 | Steps = 703225 | Walltime = 2062.224\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.651 | Episodes = 4424 | Steps = 13272 | Steps Per Second = 350.880\n",
            "[Learner] Action = 4.000 | Avg Td Error = -14.828 | Q = -17.050 | Reward = -31.810 | State = 450|2|50 | Steps = 703590 | Walltime = 2063.224\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.545 | Episodes = 4546 | Steps = 13638 | Steps Per Second = 383.766\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.703 | Q = -0.134 | Reward = 4.569 | State = 390|6|46 | Steps = 703955 | Walltime = 2064.226\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.644 | Episodes = 4668 | Steps = 14004 | Steps Per Second = 357.286\n",
            "[Learner] Action = 0.000 | Avg Td Error = -63.238 | Q = -1.611 | Reward = -64.849 | State = 390|1|54 | Steps = 704295 | Walltime = 2065.229\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.520 | Episodes = 4782 | Steps = 14346 | Steps Per Second = 380.643\n",
            "[Learner] Action = 4.000 | Avg Td Error = 11.941 | Q = -17.119 | Reward = -4.943 | State = 450|2|50 | Steps = 704661 | Walltime = 2066.231\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.380 | Episodes = 4902 | Steps = 14706 | Steps Per Second = 325.123\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.705 | Episodes = 5110 | Steps = 15330 | Steps Per Second = 1960.260\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.610 | Episodes = 5725 | Steps = 17175 | Steps Per Second = 1944.508\n",
            "[Learner] Action = -3.000 | Avg Td Error = -5.362 | Q = -2.960 | Reward = -7.675 | State = 420|6|50 | Steps = 705000 | Walltime = 2068.823\n",
            "Check Point 47\n",
            "[Learner] Action = 4.000 | Avg Td Error = 3.573 | Q = -5.628 | Reward = -1.930 | State = 420|0|49 | Steps = 705339 | Walltime = 2069.823\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -142.255 | Episodes = 115 | Steps = 345 | Steps Per Second = 337.262\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.477 | Q = -1.350 | Reward = -5.827 | State = 390|4|50 | Steps = 705679 | Walltime = 2070.823\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.809 | Episodes = 229 | Steps = 687 | Steps Per Second = 375.766\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.010 | Q = -1.644 | Reward = 1.367 | State = 390|1|50 | Steps = 706041 | Walltime = 2071.823\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.036 | Episodes = 350 | Steps = 1050 | Steps Per Second = 361.526\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.059 | Q = -12.650 | Reward = -8.451 | State = 450|2|50 | Steps = 706406 | Walltime = 2072.824\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.315 | Episodes = 472 | Steps = 1416 | Steps Per Second = 362.913\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.848 | Q = -4.935 | Reward = 3.979 | State = 450|2|50 | Steps = 706762 | Walltime = 2073.826\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.261 | Episodes = 591 | Steps = 1773 | Steps Per Second = 348.818\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.646 | Q = -1.123 | Reward = 5.524 | State = 390|2|50 | Steps = 707077 | Walltime = 2074.827\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.102 | Episodes = 697 | Steps = 2091 | Steps Per Second = 364.733\n",
            "[Learner] Action = 2.000 | Avg Td Error = 8.028 | Q = -2.693 | Reward = 5.334 | State = 390|-3|51 | Steps = 707404 | Walltime = 2075.828\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.611 | Episodes = 806 | Steps = 2418 | Steps Per Second = 258.498\n",
            "[Learner] Action = -2.000 | Avg Td Error = -2.125 | Q = -10.997 | Reward = -9.185 | State = 450|2|50 | Steps = 707713 | Walltime = 2076.829\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.185 | Episodes = 910 | Steps = 2730 | Steps Per Second = 359.430\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.242 | Q = -0.523 | Reward = -3.765 | State = 390|6|55 | Steps = 708075 | Walltime = 2077.830\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.723 | Episodes = 1031 | Steps = 3093 | Steps Per Second = 372.595\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.381 | Q = -4.525 | Reward = 3.904 | State = 450|2|50 | Steps = 708431 | Walltime = 2078.830\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.255 | Episodes = 1149 | Steps = 3447 | Steps Per Second = 377.514\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.589 | Q = -0.722 | Reward = -2.276 | State = 420|6|51 | Steps = 708796 | Walltime = 2079.832\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.747 | Episodes = 1271 | Steps = 3813 | Steps Per Second = 372.551\n",
            "[Learner] Action = -2.000 | Avg Td Error = 8.208 | Q = -11.047 | Reward = -0.306 | State = 450|2|50 | Steps = 709162 | Walltime = 2080.833\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.413 | Episodes = 1394 | Steps = 4182 | Steps Per Second = 382.227\n",
            "[Learner] Action = -4.000 | Avg Td Error = -7.132 | Q = -2.959 | Reward = -9.373 | State = 420|3|48 | Steps = 709518 | Walltime = 2081.836\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.672 | Episodes = 1512 | Steps = 4536 | Steps Per Second = 333.897\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.401 | Q = -4.660 | Reward = 5.424 | State = 450|2|50 | Steps = 709854 | Walltime = 2082.837\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.292 | Episodes = 1625 | Steps = 4875 | Steps Per Second = 332.108\n",
            "[Learner] Action = 2.000 | Avg Td Error = -10.654 | Q = -4.939 | Reward = -13.589 | State = 420|-1|52 | Steps = 710207 | Walltime = 2083.838\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.026 | Episodes = 1744 | Steps = 5232 | Steps Per Second = 319.680\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.572 | Q = -2.969 | Reward = 6.190 | State = 420|1|50 | Steps = 710576 | Walltime = 2084.839\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.887 | Episodes = 1867 | Steps = 5601 | Steps Per Second = 362.453\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.901 | Q = -4.807 | Reward = 4.535 | State = 450|2|50 | Steps = 710944 | Walltime = 2085.840\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.657 | Episodes = 1989 | Steps = 5967 | Steps Per Second = 297.722\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.736 | Q = -1.075 | Reward = -5.728 | State = 420|3|50 | Steps = 711284 | Walltime = 2086.841\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.195 | Episodes = 2103 | Steps = 6309 | Steps Per Second = 343.251\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.617 | Q = -4.794 | Reward = 5.497 | State = 450|2|50 | Steps = 711615 | Walltime = 2087.842\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.243 | Episodes = 2213 | Steps = 6639 | Steps Per Second = 290.921\n",
            "[Learner] Action = -4.000 | Avg Td Error = -2.365 | Q = -4.532 | Reward = -6.897 | State = 390|0|53 | Steps = 711946 | Walltime = 2088.845\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -168.158 | Episodes = 2324 | Steps = 6972 | Steps Per Second = 369.705\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.759 | Q = -4.708 | Reward = 3.084 | State = 450|2|50 | Steps = 712284 | Walltime = 2089.848\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.780 | Episodes = 2436 | Steps = 7308 | Steps Per Second = 330.277\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.141 | Q = -0.660 | Reward = 5.758 | State = 420|-2|55 | Steps = 712618 | Walltime = 2090.850\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -65.307 | Episodes = 2548 | Steps = 7644 | Steps Per Second = 300.222\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.919 | Q = -1.332 | Reward = 0.075 | State = 420|-1|47 | Steps = 712957 | Walltime = 2091.852\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.085 | Episodes = 2663 | Steps = 7989 | Steps Per Second = 397.075\n",
            "[Learner] Action = -2.000 | Avg Td Error = 4.279 | Q = -10.995 | Reward = -2.926 | State = 450|2|50 | Steps = 713321 | Walltime = 2092.854\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.995 | Episodes = 2783 | Steps = 8349 | Steps Per Second = 344.841\n",
            "[Learner] Action = 4.000 | Avg Td Error = -25.167 | Q = -5.203 | Reward = -30.238 | State = 420|1|50 | Steps = 713658 | Walltime = 2093.856\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.662 | Episodes = 2896 | Steps = 8688 | Steps Per Second = 335.607\n",
            "[Learner] Action = 4.000 | Avg Td Error = 16.156 | Q = -16.683 | Reward = -0.122 | State = 450|2|50 | Steps = 713997 | Walltime = 2094.858\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.708 | Episodes = 3010 | Steps = 9030 | Steps Per Second = 373.247\n",
            "[Learner] Action = 2.000 | Avg Td Error = -0.469 | Q = -0.634 | Reward = -1.103 | State = 390|5|53 | Steps = 714364 | Walltime = 2095.858\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.504 | Episodes = 3133 | Steps = 9399 | Steps Per Second = 380.413\n",
            "[Learner] Action = 3.000 | Avg Td Error = 8.249 | Q = -12.498 | Reward = -4.211 | State = 450|2|50 | Steps = 714730 | Walltime = 2096.860\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -114.339 | Episodes = 3254 | Steps = 9762 | Steps Per Second = 378.752\n",
            "[Learner] Action = -4.000 | Avg Td Error = -10.178 | Q = -1.928 | Reward = -11.406 | State = 420|3|47 | Steps = 715096 | Walltime = 2097.862\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.822 | Episodes = 3377 | Steps = 10131 | Steps Per Second = 376.373\n",
            "[Learner] Action = 3.000 | Avg Td Error = -1.124 | Q = -2.554 | Reward = -3.678 | State = 390|3|51 | Steps = 715459 | Walltime = 2098.864\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.138 | Episodes = 3499 | Steps = 10497 | Steps Per Second = 383.240\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.655 | Q = -0.070 | Reward = 0.811 | State = 420|6|52 | Steps = 715810 | Walltime = 2099.865\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -105.458 | Episodes = 3616 | Steps = 10848 | Steps Per Second = 387.083\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.505 | Q = -4.449 | Reward = -1.108 | State = 450|2|50 | Steps = 716177 | Walltime = 2100.866\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -98.277 | Episodes = 3738 | Steps = 11214 | Steps Per Second = 379.816\n",
            "[Learner] Action = 2.000 | Avg Td Error = 13.256 | Q = -8.497 | Reward = 4.777 | State = 450|2|50 | Steps = 716548 | Walltime = 2101.868\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.460 | Episodes = 3862 | Steps = 11586 | Steps Per Second = 381.983\n",
            "[Learner] Action = 4.000 | Avg Td Error = -10.769 | Q = -1.075 | Reward = -11.637 | State = 420|1|46 | Steps = 716916 | Walltime = 2102.870\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.047 | Episodes = 3984 | Steps = 11952 | Steps Per Second = 323.685\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.577 | Q = -4.445 | Reward = 6.195 | State = 450|2|50 | Steps = 717277 | Walltime = 2103.871\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.892 | Episodes = 4106 | Steps = 12318 | Steps Per Second = 387.119\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.318 | Q = -1.253 | Reward = -3.441 | State = 420|3|51 | Steps = 717647 | Walltime = 2104.872\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.279 | Episodes = 4230 | Steps = 12690 | Steps Per Second = 383.590\n",
            "[Learner] Action = 0.000 | Avg Td Error = -22.019 | Q = -4.458 | Reward = -23.977 | State = 450|2|50 | Steps = 718018 | Walltime = 2105.874\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.525 | Episodes = 4354 | Steps = 13062 | Steps Per Second = 386.489\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.506 | Q = -2.092 | Reward = -9.598 | State = 390|5|53 | Steps = 718388 | Walltime = 2106.875\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.987 | Episodes = 4478 | Steps = 13434 | Steps Per Second = 380.770\n",
            "[Learner] Action = 1.000 | Avg Td Error = -4.552 | Q = -0.604 | Reward = -5.156 | State = 390|6|49 | Steps = 718756 | Walltime = 2107.876\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.441 | Episodes = 4599 | Steps = 13797 | Steps Per Second = 323.993\n",
            "[Learner] Action = -4.000 | Avg Td Error = -65.927 | Q = -3.027 | Reward = -68.954 | State = 390|-3|51 | Steps = 719096 | Walltime = 2108.876\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.160 | Episodes = 4715 | Steps = 14145 | Steps Per Second = 367.374\n",
            "[Learner] Action = 3.000 | Avg Td Error = -16.444 | Q = -0.153 | Reward = -16.596 | State = 390|-3|44 | Steps = 719458 | Walltime = 2109.878\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.680 | Episodes = 4836 | Steps = 14508 | Steps Per Second = 374.949\n",
            "[Learner] Action = -1.000 | Avg Td Error = 8.830 | Q = -6.914 | Reward = 3.460 | State = 450|2|50 | Steps = 719829 | Walltime = 2110.879\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -66.631 | Episodes = 4957 | Steps = 14871 | Steps Per Second = 332.380\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.621 | Episodes = 5383 | Steps = 16149 | Steps Per Second = 1904.194\n",
            "[Learner] Action = 2.000 | Avg Td Error = 0.619 | Q = -2.785 | Reward = -2.165 | State = 390|2|52 | Steps = 720000 | Walltime = 2113.007\n",
            "Check Point 48\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.255 | Q = -2.902 | Reward = -1.646 | State = 390|1|53 | Steps = 720364 | Walltime = 2114.008\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.170 | Episodes = 123 | Steps = 369 | Steps Per Second = 373.779\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.112 | Q = -3.119 | Reward = -6.713 | State = 420|3|51 | Steps = 720727 | Walltime = 2115.009\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.427 | Episodes = 244 | Steps = 732 | Steps Per Second = 384.399\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.560 | Q = -1.511 | Reward = -8.071 | State = 390|1|49 | Steps = 721092 | Walltime = 2116.012\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.196 | Episodes = 366 | Steps = 1098 | Steps Per Second = 318.297\n",
            "[Learner] Action = 3.000 | Avg Td Error = 10.844 | Q = -6.769 | Reward = 4.684 | State = 420|-2|50 | Steps = 721423 | Walltime = 2117.013\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.657 | Episodes = 477 | Steps = 1431 | Steps Per Second = 336.495\n",
            "[Learner] Action = -3.000 | Avg Td Error = -10.703 | Q = -2.964 | Reward = -11.168 | State = 420|3|52 | Steps = 721766 | Walltime = 2118.015\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.120 | Episodes = 591 | Steps = 1773 | Steps Per Second = 398.837\n",
            "[Learner] Action = -1.000 | Avg Td Error = -31.121 | Q = -6.885 | Reward = -36.901 | State = 450|2|50 | Steps = 722108 | Walltime = 2119.016\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.068 | Episodes = 705 | Steps = 2115 | Steps Per Second = 303.766\n",
            "[Learner] Action = 1.000 | Avg Td Error = 2.669 | Q = -4.349 | Reward = -0.552 | State = 420|2|52 | Steps = 722443 | Walltime = 2120.017\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.972 | Episodes = 817 | Steps = 2451 | Steps Per Second = 348.944\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.229 | Q = -5.904 | Reward = 1.994 | State = 450|2|50 | Steps = 722788 | Walltime = 2121.019\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.953 | Episodes = 933 | Steps = 2799 | Steps Per Second = 369.379\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.267 | Q = -2.637 | Reward = -1.310 | State = 420|2|47 | Steps = 723149 | Walltime = 2122.022\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.420 | Episodes = 1054 | Steps = 3162 | Steps Per Second = 380.424\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.603 | Q = -4.303 | Reward = 3.453 | State = 450|2|50 | Steps = 723506 | Walltime = 2123.024\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.038 | Episodes = 1173 | Steps = 3519 | Steps Per Second = 342.457\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.032 | Q = -0.159 | Reward = -0.189 | State = 420|5|50 | Steps = 723850 | Walltime = 2124.024\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.535 | Episodes = 1289 | Steps = 3867 | Steps Per Second = 372.353\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.119 | Q = -1.185 | Reward = -0.066 | State = 390|0|48 | Steps = 724220 | Walltime = 2125.035\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.589 | Episodes = 1412 | Steps = 4236 | Steps Per Second = 373.691\n",
            "[Learner] Action = -4.000 | Avg Td Error = 0.786 | Q = -7.139 | Reward = -5.470 | State = 420|-1|49 | Steps = 724574 | Walltime = 2126.038\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -117.017 | Episodes = 1530 | Steps = 4590 | Steps Per Second = 335.930\n",
            "[Learner] Action = 3.000 | Avg Td Error = -4.857 | Q = -12.115 | Reward = -16.724 | State = 450|2|50 | Steps = 724927 | Walltime = 2127.039\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.855 | Episodes = 1649 | Steps = 4947 | Steps Per Second = 383.462\n",
            "[Learner] Action = 4.000 | Avg Td Error = 3.183 | Q = -5.550 | Reward = -2.367 | State = 390|2|50 | Steps = 725273 | Walltime = 2128.040\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.025 | Episodes = 1764 | Steps = 5292 | Steps Per Second = 291.872\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.286 | Q = -0.447 | Reward = -0.606 | State = 420|5|53 | Steps = 725617 | Walltime = 2129.043\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.696 | Episodes = 1879 | Steps = 5637 | Steps Per Second = 378.070\n",
            "[Learner] Action = -1.000 | Avg Td Error = -13.240 | Q = -3.649 | Reward = -15.163 | State = 420|2|53 | Steps = 725984 | Walltime = 2130.045\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.663 | Episodes = 2002 | Steps = 6006 | Steps Per Second = 373.103\n",
            "[Learner] Action = -2.000 | Avg Td Error = 10.587 | Q = -6.935 | Reward = 4.665 | State = 420|-2|50 | Steps = 726358 | Walltime = 2131.047\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.988 | Episodes = 2127 | Steps = 6381 | Steps Per Second = 391.540\n",
            "[Learner] Action = -3.000 | Avg Td Error = -10.420 | Q = -2.319 | Reward = -12.739 | State = 390|-1|48 | Steps = 726731 | Walltime = 2132.048\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.860 | Episodes = 2252 | Steps = 6756 | Steps Per Second = 369.358\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.076 | Q = -0.274 | Reward = 0.544 | State = 420|5|54 | Steps = 727090 | Walltime = 2133.048\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.197 | Episodes = 2372 | Steps = 7116 | Steps Per Second = 383.088\n",
            "[Learner] Action = 1.000 | Avg Td Error = -21.153 | Q = -5.160 | Reward = -23.817 | State = 420|-2|51 | Steps = 727464 | Walltime = 2134.050\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.318 | Episodes = 2497 | Steps = 7491 | Steps Per Second = 393.105\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.444 | Q = -4.538 | Reward = 4.333 | State = 450|2|50 | Steps = 727830 | Walltime = 2135.050\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.693 | Episodes = 2619 | Steps = 7857 | Steps Per Second = 378.752\n",
            "[Learner] Action = 0.000 | Avg Td Error = -14.418 | Q = -4.580 | Reward = -15.355 | State = 450|2|50 | Steps = 728201 | Walltime = 2136.050\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.996 | Episodes = 2743 | Steps = 8229 | Steps Per Second = 389.697\n",
            "[Learner] Action = 0.000 | Avg Td Error = -11.071 | Q = -4.553 | Reward = -11.976 | State = 450|2|50 | Steps = 728569 | Walltime = 2137.051\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -110.624 | Episodes = 2866 | Steps = 8598 | Steps Per Second = 374.246\n",
            "[Learner] Action = 1.000 | Avg Td Error = -8.925 | Q = -0.179 | Reward = -9.104 | State = 390|6|44 | Steps = 728931 | Walltime = 2138.052\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.715 | Episodes = 2987 | Steps = 8961 | Steps Per Second = 381.682\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.882 | Q = -0.471 | Reward = -10.353 | State = 390|5|45 | Steps = 729298 | Walltime = 2139.052\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.037 | Episodes = 3110 | Steps = 9330 | Steps Per Second = 374.826\n",
            "[Learner] Action = -1.000 | Avg Td Error = -37.012 | Q = -0.402 | Reward = -37.415 | State = 390|4|55 | Steps = 729647 | Walltime = 2140.053\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.357 | Episodes = 3225 | Steps = 9675 | Steps Per Second = 339.565\n",
            "[Learner] Action = -3.000 | Avg Td Error = -11.838 | Q = -0.831 | Reward = -12.669 | State = 390|7|48 | Steps = 730002 | Walltime = 2141.053\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.528 | Episodes = 3345 | Steps = 10035 | Steps Per Second = 386.121\n",
            "[Learner] Action = -2.000 | Avg Td Error = -8.814 | Q = -1.066 | Reward = -9.880 | State = 390|5|53 | Steps = 730351 | Walltime = 2142.054\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.454 | Episodes = 3461 | Steps = 10383 | Steps Per Second = 325.106\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.976 | Q = -1.012 | Reward = 4.964 | State = 390|-4|48 | Steps = 730698 | Walltime = 2143.056\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.143 | Episodes = 3578 | Steps = 10734 | Steps Per Second = 370.445\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.166 | Q = -0.627 | Reward = 0.539 | State = 390|1|47 | Steps = 731048 | Walltime = 2144.057\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.826 | Episodes = 3695 | Steps = 11085 | Steps Per Second = 380.493\n",
            "[Learner] Action = 0.000 | Avg Td Error = -9.124 | Q = -4.725 | Reward = -9.601 | State = 450|2|50 | Steps = 731418 | Walltime = 2145.058\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.037 | Episodes = 3819 | Steps = 11457 | Steps Per Second = 386.881\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.126 | Q = -2.149 | Reward = 5.977 | State = 390|1|53 | Steps = 731780 | Walltime = 2146.059\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.219 | Episodes = 3940 | Steps = 11820 | Steps Per Second = 380.194\n",
            "[Learner] Action = 1.000 | Avg Td Error = -21.145 | Q = -4.371 | Reward = -22.860 | State = 420|-2|49 | Steps = 732137 | Walltime = 2147.060\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.525 | Episodes = 4058 | Steps = 12174 | Steps Per Second = 328.356\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.804 | Q = -4.342 | Reward = -9.717 | State = 420|4|50 | Steps = 732484 | Walltime = 2148.061\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.835 | Episodes = 4176 | Steps = 12528 | Steps Per Second = 382.401\n",
            "[Learner] Action = -3.000 | Avg Td Error = -27.163 | Q = -3.256 | Reward = -30.420 | State = 390|0|52 | Steps = 732850 | Walltime = 2149.063\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.247 | Episodes = 4298 | Steps = 12894 | Steps Per Second = 377.446\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.107 | Q = -0.443 | Reward = 2.665 | State = 390|6|48 | Steps = 733214 | Walltime = 2150.064\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.846 | Episodes = 4420 | Steps = 13260 | Steps Per Second = 368.492\n",
            "[Learner] Action = -4.000 | Avg Td Error = -48.704 | Q = -2.438 | Reward = -51.142 | State = 390|-1|53 | Steps = 733578 | Walltime = 2151.065\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.065 | Episodes = 4542 | Steps = 13626 | Steps Per Second = 380.436\n",
            "[Learner] Action = 3.000 | Avg Td Error = -0.508 | Q = -3.428 | Reward = -3.936 | State = 390|2|53 | Steps = 733945 | Walltime = 2152.068\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.225 | Episodes = 4665 | Steps = 13995 | Steps Per Second = 385.258\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.365 | Q = -1.278 | Reward = 6.087 | State = 390|1|55 | Steps = 734311 | Walltime = 2153.068\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.690 | Episodes = 4787 | Steps = 14361 | Steps Per Second = 369.271\n",
            "[Learner] Action = 4.000 | Avg Td Error = -49.445 | Q = -11.465 | Reward = -60.768 | State = 420|2|50 | Steps = 734660 | Walltime = 2154.070\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.510 | Episodes = 4901 | Steps = 14703 | Steps Per Second = 332.468\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.996 | Episodes = 5083 | Steps = 15249 | Steps Per Second = 1773.490\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -83.836 | Episodes = 5699 | Steps = 17097 | Steps Per Second = 2003.011\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.334 | Q = -0.827 | Reward = 0.271 | State = 420|4|52 | Steps = 735000 | Walltime = 2156.696\n",
            "Check Point 49\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.523 | Q = -1.867 | Reward = -5.297 | State = 420|3|51 | Steps = 735365 | Walltime = 2157.696\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.087 | Episodes = 123 | Steps = 369 | Steps Per Second = 385.813\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.719 | Q = -3.832 | Reward = 0.887 | State = 390|2|50 | Steps = 735733 | Walltime = 2158.698\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.012 | Episodes = 246 | Steps = 738 | Steps Per Second = 373.159\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.356 | Q = -16.053 | Reward = -12.346 | State = 450|2|50 | Steps = 736102 | Walltime = 2159.700\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -94.342 | Episodes = 369 | Steps = 1107 | Steps Per Second = 388.529\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.539 | Q = -3.926 | Reward = 2.755 | State = 420|0|50 | Steps = 736473 | Walltime = 2160.703\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.688 | Episodes = 493 | Steps = 1479 | Steps Per Second = 345.030\n",
            "[Learner] Action = 4.000 | Avg Td Error = 1.294 | Q = -4.256 | Reward = -2.875 | State = 420|1|52 | Steps = 736816 | Walltime = 2161.706\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.183 | Episodes = 607 | Steps = 1821 | Steps Per Second = 338.323\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.012 | Q = -1.218 | Reward = -4.091 | State = 420|4|52 | Steps = 737151 | Walltime = 2162.708\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.674 | Episodes = 719 | Steps = 2157 | Steps Per Second = 341.250\n",
            "[Learner] Action = 3.000 | Avg Td Error = -8.468 | Q = -2.865 | Reward = -11.286 | State = 420|4|51 | Steps = 737480 | Walltime = 2163.709\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.522 | Episodes = 829 | Steps = 2487 | Steps Per Second = 354.219\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.996 | Q = 0.021 | Reward = 1.017 | State = 390|3|46 | Steps = 737830 | Walltime = 2164.711\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.971 | Episodes = 946 | Steps = 2838 | Steps Per Second = 367.685\n",
            "[Learner] Action = -2.000 | Avg Td Error = 3.960 | Q = -0.314 | Reward = 3.646 | State = 390|2|57 | Steps = 738154 | Walltime = 2165.712\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.162 | Episodes = 1054 | Steps = 3162 | Steps Per Second = 364.258\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.849 | Q = -0.861 | Reward = -9.710 | State = 390|5|45 | Steps = 738492 | Walltime = 2166.712\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.584 | Episodes = 1167 | Steps = 3501 | Steps Per Second = 387.644\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.539 | Q = -1.246 | Reward = 0.293 | State = 390|2|48 | Steps = 738859 | Walltime = 2167.714\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -314.241 | Episodes = 1290 | Steps = 3870 | Steps Per Second = 379.988\n",
            "[Learner] Action = 2.000 | Avg Td Error = 5.454 | Q = -3.100 | Reward = 4.622 | State = 420|-2|53 | Steps = 739226 | Walltime = 2168.715\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.509 | Episodes = 1413 | Steps = 4239 | Steps Per Second = 385.518\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.109 | Q = -16.042 | Reward = -15.107 | State = 450|2|50 | Steps = 739590 | Walltime = 2169.718\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.799 | Episodes = 1534 | Steps = 4602 | Steps Per Second = 384.728\n",
            "[Learner] Action = 4.000 | Avg Td Error = 12.496 | Q = -16.786 | Reward = -3.848 | State = 450|2|50 | Steps = 739926 | Walltime = 2170.719\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.761 | Episodes = 1647 | Steps = 4941 | Steps Per Second = 383.637\n",
            "[Learner] Action = 3.000 | Avg Td Error = 13.938 | Q = -12.689 | Reward = 1.437 | State = 450|2|50 | Steps = 740263 | Walltime = 2171.721\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.883 | Episodes = 1759 | Steps = 5277 | Steps Per Second = 337.688\n",
            "[Learner] Action = -3.000 | Avg Td Error = -2.823 | Q = -15.976 | Reward = -13.724 | State = 450|2|50 | Steps = 740592 | Walltime = 2172.723\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.420 | Episodes = 1869 | Steps = 5607 | Steps Per Second = 329.715\n",
            "[Learner] Action = -1.000 | Avg Td Error = -28.664 | Q = -2.587 | Reward = -31.251 | State = 390|0|51 | Steps = 740959 | Walltime = 2173.724\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -70.685 | Episodes = 1993 | Steps = 5979 | Steps Per Second = 381.636\n",
            "[Learner] Action = 3.000 | Avg Td Error = 8.382 | Q = -6.352 | Reward = 2.675 | State = 420|-2|51 | Steps = 741326 | Walltime = 2174.724\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.966 | Episodes = 2116 | Steps = 6348 | Steps Per Second = 374.781\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.977 | Q = -0.422 | Reward = -1.400 | State = 390|6|52 | Steps = 741685 | Walltime = 2175.725\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.266 | Episodes = 2236 | Steps = 6708 | Steps Per Second = 378.104\n",
            "[Learner] Action = 2.000 | Avg Td Error = -14.192 | Q = -3.013 | Reward = -17.206 | State = 390|0|52 | Steps = 742048 | Walltime = 2176.727\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.980 | Episodes = 2356 | Steps = 7068 | Steps Per Second = 329.879\n",
            "[Learner] Action = 2.000 | Avg Td Error = -18.735 | Q = -8.031 | Reward = -26.188 | State = 450|2|50 | Steps = 742380 | Walltime = 2177.729\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.601 | Episodes = 2468 | Steps = 7404 | Steps Per Second = 379.724\n",
            "[Learner] Action = -1.000 | Avg Td Error = -9.058 | Q = -0.848 | Reward = -7.734 | State = 420|0|47 | Steps = 742748 | Walltime = 2178.730\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -103.477 | Episodes = 2591 | Steps = 7773 | Steps Per Second = 391.735\n",
            "[Learner] Action = 0.000 | Avg Td Error = -18.854 | Q = -5.210 | Reward = -20.208 | State = 450|2|50 | Steps = 743118 | Walltime = 2179.732\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.762 | Episodes = 2715 | Steps = 8145 | Steps Per Second = 388.890\n",
            "[Learner] Action = 2.000 | Avg Td Error = -7.222 | Q = -8.238 | Reward = -14.626 | State = 450|2|50 | Steps = 743466 | Walltime = 2180.735\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.102 | Episodes = 2831 | Steps = 8493 | Steps Per Second = 381.323\n",
            "[Learner] Action = -3.000 | Avg Td Error = -12.995 | Q = -2.650 | Reward = -13.530 | State = 420|1|48 | Steps = 743830 | Walltime = 2181.735\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.793 | Episodes = 2952 | Steps = 8856 | Steps Per Second = 328.639\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.908 | Q = -0.094 | Reward = -2.761 | State = 420|5|48 | Steps = 744154 | Walltime = 2182.737\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.008 | Episodes = 3062 | Steps = 9186 | Steps Per Second = 383.731\n",
            "[Learner] Action = -2.000 | Avg Td Error = 7.266 | Q = -11.135 | Reward = -1.166 | State = 450|2|50 | Steps = 744515 | Walltime = 2183.738\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -53.909 | Episodes = 3182 | Steps = 9546 | Steps Per Second = 369.857\n",
            "[Learner] Action = -4.000 | Avg Td Error = -23.491 | Q = -5.741 | Reward = -29.232 | State = 390|0|50 | Steps = 744880 | Walltime = 2184.741\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.645 | Episodes = 3304 | Steps = 9912 | Steps Per Second = 338.532\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.377 | Q = -0.039 | Reward = -2.416 | State = 390|8|55 | Steps = 745224 | Walltime = 2185.742\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.449 | Episodes = 3419 | Steps = 10257 | Steps Per Second = 373.092\n",
            "[Learner] Action = -3.000 | Avg Td Error = -12.849 | Q = -3.481 | Reward = -16.329 | State = 390|2|54 | Steps = 745570 | Walltime = 2186.743\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -60.663 | Episodes = 3535 | Steps = 10605 | Steps Per Second = 375.385\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.776 | Q = -4.890 | Reward = -0.272 | State = 450|2|50 | Steps = 745910 | Walltime = 2187.744\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.353 | Episodes = 3648 | Steps = 10944 | Steps Per Second = 314.998\n",
            "[Learner] Action = -4.000 | Avg Td Error = -19.062 | Q = -1.357 | Reward = -20.420 | State = 390|4|55 | Steps = 746215 | Walltime = 2188.746\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.380 | Episodes = 3751 | Steps = 11253 | Steps Per Second = 240.008\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.040 | Q = -4.000 | Reward = -7.102 | State = 420|2|51 | Steps = 746541 | Walltime = 2189.747\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.004 | Episodes = 3859 | Steps = 11577 | Steps Per Second = 323.668\n",
            "[Learner] Action = 0.000 | Avg Td Error = -9.615 | Q = -1.862 | Reward = -9.601 | State = 420|1|53 | Steps = 746860 | Walltime = 2190.748\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.029 | Episodes = 3964 | Steps = 11892 | Steps Per Second = 224.639\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.320 | Q = -4.992 | Reward = 0.135 | State = 450|2|50 | Steps = 747204 | Walltime = 2191.748\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.391 | Episodes = 4083 | Steps = 12249 | Steps Per Second = 396.612\n",
            "[Learner] Action = 3.000 | Avg Td Error = 9.329 | Q = -3.080 | Reward = 6.249 | State = 390|-1|49 | Steps = 747568 | Walltime = 2192.749\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -82.062 | Episodes = 4204 | Steps = 12612 | Steps Per Second = 362.808\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.137 | Q = -4.973 | Reward = 4.661 | State = 450|2|50 | Steps = 747926 | Walltime = 2193.750\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.631 | Episodes = 4324 | Steps = 12972 | Steps Per Second = 377.220\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.652 | Q = -0.431 | Reward = 1.457 | State = 420|4|50 | Steps = 748294 | Walltime = 2194.750\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.816 | Episodes = 4447 | Steps = 13341 | Steps Per Second = 357.581\n",
            "[Learner] Action = -3.000 | Avg Td Error = 0.388 | Q = -15.626 | Reward = -10.473 | State = 450|2|50 | Steps = 748635 | Walltime = 2195.752\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.248 | Episodes = 4559 | Steps = 13677 | Steps Per Second = 343.364\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.405 | Q = -2.327 | Reward = -4.594 | State = 420|0|53 | Steps = 748966 | Walltime = 2196.755\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.457 | Episodes = 4672 | Steps = 14016 | Steps Per Second = 371.726\n",
            "[Learner] Action = 1.000 | Avg Td Error = -3.064 | Q = -0.051 | Reward = -3.115 | State = 390|4|43 | Steps = 749318 | Walltime = 2197.757\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.432 | Episodes = 4789 | Steps = 14367 | Steps Per Second = 366.038\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.000 | Q = -5.725 | Reward = 1.989 | State = 450|2|50 | Steps = 749654 | Walltime = 2198.757\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.780 | Episodes = 4900 | Steps = 14700 | Steps Per Second = 335.679\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.578 | Q = -1.608 | Reward = 5.970 | State = 390|-1|48 | Steps = 749983 | Walltime = 2199.758\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.799 | Episodes = 5062 | Steps = 15186 | Steps Per Second = 2006.524\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.363 | Episodes = 5686 | Steps = 17058 | Steps Per Second = 1989.708\n",
            "[Learner] Action = 2.000 | Avg Td Error = 1.116 | Q = -8.125 | Reward = -6.588 | State = 450|2|50 | Steps = 750000 | Walltime = 2201.402\n",
            "Check Point 50\n",
            "[Learner] Action = 2.000 | Avg Td Error = -14.096 | Q = -3.940 | Reward = -16.340 | State = 420|-1|49 | Steps = 750352 | Walltime = 2202.405\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.628 | Episodes = 119 | Steps = 357 | Steps Per Second = 385.211\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.575 | Q = -1.856 | Reward = 5.719 | State = 390|1|49 | Steps = 750716 | Walltime = 2203.405\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.932 | Episodes = 241 | Steps = 723 | Steps Per Second = 380.125\n",
            "[Learner] Action = -4.000 | Avg Td Error = 18.123 | Q = -21.180 | Reward = -1.641 | State = 450|2|50 | Steps = 751070 | Walltime = 2204.405\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.815 | Episodes = 360 | Steps = 1080 | Steps Per Second = 360.243\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.823 | Q = -15.719 | Reward = -8.160 | State = 450|2|50 | Steps = 751429 | Walltime = 2205.408\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.528 | Episodes = 480 | Steps = 1440 | Steps Per Second = 364.923\n",
            "[Learner] Action = -3.000 | Avg Td Error = 16.409 | Q = -15.702 | Reward = 1.899 | State = 450|2|50 | Steps = 751771 | Walltime = 2206.408\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.066 | Episodes = 593 | Steps = 1779 | Steps Per Second = 272.907\n",
            "[Learner] Action = 4.000 | Avg Td Error = -28.982 | Q = -4.467 | Reward = -33.339 | State = 420|1|52 | Steps = 752128 | Walltime = 2207.410\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.062 | Episodes = 714 | Steps = 2142 | Steps Per Second = 372.938\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.464 | Q = -0.218 | Reward = 6.247 | State = 390|-1|60 | Steps = 752484 | Walltime = 2208.411\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -42.313 | Episodes = 833 | Steps = 2499 | Steps Per Second = 387.548\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.461 | Q = -5.112 | Reward = 4.789 | State = 450|2|50 | Steps = 752833 | Walltime = 2209.415\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.674 | Episodes = 949 | Steps = 2847 | Steps Per Second = 323.410\n",
            "[Learner] Action = -4.000 | Avg Td Error = -24.238 | Q = -1.003 | Reward = -25.241 | State = 390|-4|46 | Steps = 753185 | Walltime = 2210.415\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.122 | Episodes = 1068 | Steps = 3204 | Steps Per Second = 374.737\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.138 | Q = -5.169 | Reward = 3.175 | State = 450|2|50 | Steps = 753557 | Walltime = 2211.416\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.166 | Episodes = 1192 | Steps = 3576 | Steps Per Second = 370.347\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.113 | Q = -0.119 | Reward = 1.994 | State = 390|6|53 | Steps = 753919 | Walltime = 2212.417\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.096 | Episodes = 1313 | Steps = 3939 | Steps Per Second = 378.581\n",
            "[Learner] Action = -2.000 | Avg Td Error = -56.478 | Q = -2.444 | Reward = -58.922 | State = 390|-1|48 | Steps = 754279 | Walltime = 2213.418\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.836 | Episodes = 1433 | Steps = 4299 | Steps Per Second = 384.094\n",
            "[Learner] Action = 3.000 | Avg Td Error = -8.682 | Q = -3.090 | Reward = -11.799 | State = 420|4|49 | Steps = 754606 | Walltime = 2214.422\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.109 | Episodes = 1542 | Steps = 4626 | Steps Per Second = 328.948\n",
            "[Learner] Action = 0.000 | Avg Td Error = -10.084 | Q = -2.681 | Reward = -10.783 | State = 420|2|50 | Steps = 754968 | Walltime = 2215.422\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.027 | Episodes = 1663 | Steps = 4989 | Steps Per Second = 332.187\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.909 | Q = -4.972 | Reward = 0.547 | State = 420|2|50 | Steps = 755304 | Walltime = 2216.424\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -98.223 | Episodes = 1776 | Steps = 5328 | Steps Per Second = 374.670\n",
            "[Learner] Action = -3.000 | Avg Td Error = 1.109 | Q = -0.113 | Reward = 0.996 | State = 390|10|45 | Steps = 755667 | Walltime = 2217.424\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.541 | Episodes = 1897 | Steps = 5691 | Steps Per Second = 346.599\n",
            "[Learner] Action = 0.000 | Avg Td Error = -17.920 | Q = -1.407 | Reward = -19.326 | State = 390|0|55 | Steps = 756012 | Walltime = 2218.424\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.456 | Episodes = 2014 | Steps = 6042 | Steps Per Second = 363.112\n",
            "[Learner] Action = 4.000 | Avg Td Error = -46.840 | Q = -2.909 | Reward = -49.750 | State = 390|3|48 | Steps = 756334 | Walltime = 2219.427\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.695 | Episodes = 2120 | Steps = 6360 | Steps Per Second = 352.818\n",
            "[Learner] Action = 4.000 | Avg Td Error = -24.192 | Q = -16.770 | Reward = -40.722 | State = 450|2|50 | Steps = 756692 | Walltime = 2220.429\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.971 | Episodes = 2240 | Steps = 6720 | Steps Per Second = 348.557\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.903 | Q = -0.666 | Reward = -1.175 | State = 420|3|48 | Steps = 757017 | Walltime = 2221.429\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.921 | Episodes = 2348 | Steps = 7044 | Steps Per Second = 390.265\n",
            "[Learner] Action = -1.000 | Avg Td Error = 9.206 | Q = -4.953 | Reward = 4.672 | State = 420|2|50 | Steps = 757375 | Walltime = 2222.430\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.340 | Episodes = 2469 | Steps = 7407 | Steps Per Second = 366.027\n",
            "[Learner] Action = -4.000 | Avg Td Error = 1.604 | Q = -4.175 | Reward = -2.571 | State = 390|-2|50 | Steps = 757723 | Walltime = 2223.430\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.565 | Episodes = 2585 | Steps = 7755 | Steps Per Second = 366.197\n",
            "[Learner] Action = 2.000 | Avg Td Error = 8.213 | Q = -2.949 | Reward = 5.265 | State = 390|2|51 | Steps = 758085 | Walltime = 2224.430\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.377 | Episodes = 2706 | Steps = 8118 | Steps Per Second = 354.219\n",
            "[Learner] Action = 0.000 | Avg Td Error = -17.018 | Q = -1.614 | Reward = -16.357 | State = 420|1|48 | Steps = 758445 | Walltime = 2225.432\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.134 | Episodes = 2827 | Steps = 8481 | Steps Per Second = 343.777\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.519 | Q = -1.427 | Reward = -7.945 | State = 390|4|47 | Steps = 758775 | Walltime = 2226.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.737 | Episodes = 2937 | Steps = 8811 | Steps Per Second = 391.029\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.612 | Q = -1.395 | Reward = 0.218 | State = 390|-1|48 | Steps = 759102 | Walltime = 2227.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.827 | Episodes = 3047 | Steps = 9141 | Steps Per Second = 385.341\n",
            "[Learner] Action = -4.000 | Avg Td Error = 6.678 | Q = -0.645 | Reward = 6.033 | State = 390|-4|58 | Steps = 759468 | Walltime = 2228.434\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -78.685 | Episodes = 3170 | Steps = 9510 | Steps Per Second = 374.469\n",
            "[Learner] Action = 1.000 | Avg Td Error = -7.736 | Q = -5.978 | Reward = -12.449 | State = 450|2|50 | Steps = 759815 | Walltime = 2229.435\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.896 | Episodes = 3285 | Steps = 9855 | Steps Per Second = 328.210\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.071 | Q = -1.083 | Reward = -6.154 | State = 390|2|49 | Steps = 760164 | Walltime = 2230.437\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.613 | Episodes = 3403 | Steps = 10209 | Steps Per Second = 378.912\n",
            "[Learner] Action = 3.000 | Avg Td Error = -8.748 | Q = -12.664 | Reward = -21.183 | State = 450|2|50 | Steps = 760526 | Walltime = 2231.438\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.691 | Episodes = 3524 | Steps = 10572 | Steps Per Second = 368.255\n",
            "[Learner] Action = -1.000 | Avg Td Error = 7.121 | Q = -6.551 | Reward = 1.559 | State = 420|-2|51 | Steps = 760874 | Walltime = 2232.438\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 11.201 | Episodes = 3639 | Steps = 10917 | Steps Per Second = 367.921\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.110 | Q = -0.988 | Reward = 3.122 | State = 390|-1|46 | Steps = 761233 | Walltime = 2233.440\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.620 | Episodes = 3760 | Steps = 11280 | Steps Per Second = 361.516\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.560 | Q = -4.306 | Reward = -11.633 | State = 420|5|50 | Steps = 761603 | Walltime = 2234.442\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.840 | Episodes = 3884 | Steps = 11652 | Steps Per Second = 357.957\n",
            "[Learner] Action = 4.000 | Avg Td Error = -50.620 | Q = -16.704 | Reward = -67.311 | State = 450|2|50 | Steps = 761969 | Walltime = 2235.444\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.268 | Episodes = 4006 | Steps = 12018 | Steps Per Second = 379.083\n",
            "[Learner] Action = -4.000 | Avg Td Error = 13.700 | Q = -20.719 | Reward = -4.266 | State = 450|2|50 | Steps = 762294 | Walltime = 2236.446\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.616 | Episodes = 4114 | Steps = 12342 | Steps Per Second = 368.892\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.338 | Q = -0.397 | Reward = 0.941 | State = 390|2|47 | Steps = 762646 | Walltime = 2237.447\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.222 | Episodes = 4232 | Steps = 12696 | Steps Per Second = 373.480\n",
            "[Learner] Action = -1.000 | Avg Td Error = 9.064 | Q = -4.129 | Reward = 4.987 | State = 420|2|49 | Steps = 763008 | Walltime = 2238.449\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.088 | Episodes = 4353 | Steps = 13059 | Steps Per Second = 340.686\n",
            "[Learner] Action = 3.000 | Avg Td Error = -8.326 | Q = -1.688 | Reward = -10.014 | State = 390|6|53 | Steps = 763338 | Walltime = 2239.450\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.188 | Episodes = 4463 | Steps = 13389 | Steps Per Second = 377.084\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.029 | Q = -8.150 | Reward = -8.695 | State = 450|2|50 | Steps = 763667 | Walltime = 2240.453\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.721 | Episodes = 4573 | Steps = 13719 | Steps Per Second = 378.901\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.044 | Q = -8.159 | Reward = 2.074 | State = 450|2|50 | Steps = 763992 | Walltime = 2241.454\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.855 | Episodes = 4682 | Steps = 14046 | Steps Per Second = 371.342\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.034 | Q = -2.100 | Reward = 3.934 | State = 390|-2|49 | Steps = 764356 | Walltime = 2242.455\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.926 | Episodes = 4804 | Steps = 14412 | Steps Per Second = 371.967\n",
            "[Learner] Action = -2.000 | Avg Td Error = 7.589 | Q = -6.673 | Reward = 2.072 | State = 420|2|50 | Steps = 764697 | Walltime = 2243.455\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.083 | Episodes = 4917 | Steps = 14751 | Steps Per Second = 328.107\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.363 | Episodes = 5180 | Steps = 15540 | Steps Per Second = 1983.122\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.273 | Episodes = 5752 | Steps = 17256 | Steps Per Second = 1962.400\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.292 | Q = -0.376 | Reward = -4.668 | State = 390|5|45 | Steps = 765000 | Walltime = 2246.050\n",
            "Check Point 51\n",
            "[Learner] Action = -2.000 | Avg Td Error = 9.026 | Q = -11.118 | Reward = 0.602 | State = 450|2|50 | Steps = 765359 | Walltime = 2247.051\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.251 | Episodes = 121 | Steps = 363 | Steps Per Second = 386.358\n",
            "[Learner] Action = 3.000 | Avg Td Error = -2.616 | Q = -3.128 | Reward = -5.721 | State = 420|3|52 | Steps = 765712 | Walltime = 2248.054\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.504 | Episodes = 239 | Steps = 717 | Steps Per Second = 329.732\n",
            "[Learner] Action = 0.000 | Avg Td Error = -11.766 | Q = -0.428 | Reward = -12.194 | State = 390|3|50 | Steps = 766045 | Walltime = 2249.056\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.642 | Episodes = 350 | Steps = 1050 | Steps Per Second = 345.722\n",
            "[Learner] Action = -3.000 | Avg Td Error = 2.049 | Q = -15.412 | Reward = -8.468 | State = 450|2|50 | Steps = 766402 | Walltime = 2250.056\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -75.797 | Episodes = 469 | Steps = 1407 | Steps Per Second = 373.292\n",
            "[Learner] Action = 3.000 | Avg Td Error = -3.215 | Q = -12.833 | Reward = -15.775 | State = 450|2|50 | Steps = 766754 | Walltime = 2251.065\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.250 | Episodes = 586 | Steps = 1758 | Steps Per Second = 349.060\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.468 | Q = -0.760 | Reward = -4.228 | State = 390|4|51 | Steps = 767100 | Walltime = 2252.066\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.744 | Episodes = 702 | Steps = 2106 | Steps Per Second = 367.256\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.417 | Q = -0.749 | Reward = -7.166 | State = 390|3|45 | Steps = 767451 | Walltime = 2253.067\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.981 | Episodes = 819 | Steps = 2457 | Steps Per Second = 306.713\n",
            "[Learner] Action = 3.000 | Avg Td Error = 14.943 | Q = -12.828 | Reward = 2.277 | State = 450|2|50 | Steps = 767792 | Walltime = 2254.069\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.851 | Episodes = 933 | Steps = 2799 | Steps Per Second = 375.755\n",
            "[Learner] Action = -4.000 | Avg Td Error = -0.578 | Q = -1.334 | Reward = 0.873 | State = 420|3|54 | Steps = 768129 | Walltime = 2255.069\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.572 | Episodes = 1045 | Steps = 3135 | Steps Per Second = 339.208\n",
            "[Learner] Action = -1.000 | Avg Td Error = 8.702 | Q = -6.702 | Reward = 3.649 | State = 450|2|50 | Steps = 768467 | Walltime = 2256.070\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.821 | Episodes = 1158 | Steps = 3474 | Steps Per Second = 317.502\n",
            "[Learner] Action = -2.000 | Avg Td Error = -88.270 | Q = -3.018 | Reward = -91.289 | State = 390|-1|50 | Steps = 768795 | Walltime = 2257.073\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.199 | Episodes = 1268 | Steps = 3804 | Steps Per Second = 377.412\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.361 | Q = -0.375 | Reward = 0.986 | State = 390|5|46 | Steps = 769155 | Walltime = 2258.073\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.813 | Episodes = 1388 | Steps = 4164 | Steps Per Second = 360.925\n",
            "[Learner] Action = -4.000 | Avg Td Error = 21.005 | Q = -20.285 | Reward = 1.355 | State = 450|2|50 | Steps = 769519 | Walltime = 2259.074\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.474 | Episodes = 1510 | Steps = 4530 | Steps Per Second = 327.911\n",
            "[Learner] Action = 1.000 | Avg Td Error = 8.596 | Q = -2.349 | Reward = 6.247 | State = 390|2|53 | Steps = 769845 | Walltime = 2260.076\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -56.375 | Episodes = 1619 | Steps = 4857 | Steps Per Second = 338.287\n",
            "[Learner] Action = 3.000 | Avg Td Error = -10.967 | Q = -2.697 | Reward = -13.664 | State = 390|3|51 | Steps = 770188 | Walltime = 2261.078\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.168 | Episodes = 1734 | Steps = 5202 | Steps Per Second = 385.896\n",
            "[Learner] Action = 3.000 | Avg Td Error = -3.962 | Q = -12.505 | Reward = -16.200 | State = 450|2|50 | Steps = 770539 | Walltime = 2262.078\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.299 | Episodes = 1851 | Steps = 5553 | Steps Per Second = 334.367\n",
            "[Learner] Action = 3.000 | Avg Td Error = -15.793 | Q = -12.519 | Reward = -27.841 | State = 450|2|50 | Steps = 770876 | Walltime = 2263.080\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.575 | Episodes = 1964 | Steps = 5892 | Steps Per Second = 324.544\n",
            "[Learner] Action = -4.000 | Avg Td Error = 4.198 | Q = -3.469 | Reward = 0.729 | State = 390|-1|48 | Steps = 771220 | Walltime = 2264.083\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.747 | Episodes = 2079 | Steps = 6237 | Steps Per Second = 328.664\n",
            "[Learner] Action = -4.000 | Avg Td Error = -8.167 | Q = -2.756 | Reward = -10.923 | State = 390|4|48 | Steps = 771563 | Walltime = 2265.084\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.611 | Episodes = 2194 | Steps = 6582 | Steps Per Second = 335.795\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.392 | Q = -4.383 | Reward = 6.160 | State = 450|2|50 | Steps = 771913 | Walltime = 2266.085\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.180 | Episodes = 2312 | Steps = 6936 | Steps Per Second = 367.298\n",
            "[Learner] Action = -4.000 | Avg Td Error = -5.045 | Q = -3.171 | Reward = -7.503 | State = 420|3|48 | Steps = 772280 | Walltime = 2267.087\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.439 | Episodes = 2435 | Steps = 7305 | Steps Per Second = 382.902\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.671 | Q = -0.263 | Reward = -3.933 | State = 390|2|44 | Steps = 772628 | Walltime = 2268.088\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.392 | Episodes = 2550 | Steps = 7650 | Steps Per Second = 331.295\n",
            "[Learner] Action = -2.000 | Avg Td Error = 0.622 | Q = -0.669 | Reward = -0.047 | State = 390|6|47 | Steps = 772962 | Walltime = 2269.091\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.417 | Episodes = 2662 | Steps = 7986 | Steps Per Second = 298.301\n",
            "[Learner] Action = -4.000 | Avg Td Error = -7.688 | Q = -2.544 | Reward = -10.232 | State = 390|5|51 | Steps = 773303 | Walltime = 2270.092\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.269 | Episodes = 2776 | Steps = 8328 | Steps Per Second = 324.369\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.995 | Q = -3.267 | Reward = -4.634 | State = 420|4|50 | Steps = 773654 | Walltime = 2271.093\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.309 | Episodes = 2894 | Steps = 8682 | Steps Per Second = 370.336\n",
            "[Learner] Action = -2.000 | Avg Td Error = -10.766 | Q = -2.543 | Reward = -13.309 | State = 390|-1|48 | Steps = 774024 | Walltime = 2272.093\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.475 | Episodes = 3018 | Steps = 9054 | Steps Per Second = 377.945\n",
            "[Learner] Action = -3.000 | Avg Td Error = -0.816 | Q = -15.234 | Reward = -10.922 | State = 450|2|50 | Steps = 774397 | Walltime = 2273.094\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -115.162 | Episodes = 3143 | Steps = 9429 | Steps Per Second = 381.891\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.414 | Q = -2.355 | Reward = -6.616 | State = 420|6|48 | Steps = 774757 | Walltime = 2274.094\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.721 | Episodes = 3264 | Steps = 9792 | Steps Per Second = 379.621\n",
            "[Learner] Action = -2.000 | Avg Td Error = 4.316 | Q = -10.843 | Reward = -2.617 | State = 450|2|50 | Steps = 775135 | Walltime = 2275.097\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.068 | Episodes = 3390 | Steps = 10170 | Steps Per Second = 382.157\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.174 | Q = -1.823 | Reward = -4.980 | State = 420|5|49 | Steps = 775505 | Walltime = 2276.098\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -144.494 | Episodes = 3514 | Steps = 10542 | Steps Per Second = 372.088\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.203 | Q = -0.179 | Reward = 1.040 | State = 420|6|51 | Steps = 775874 | Walltime = 2277.101\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.613 | Episodes = 3637 | Steps = 10911 | Steps Per Second = 376.903\n",
            "[Learner] Action = 4.000 | Avg Td Error = 2.643 | Q = -16.782 | Reward = -13.809 | State = 450|2|50 | Steps = 776239 | Walltime = 2278.101\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.688 | Episodes = 3759 | Steps = 11277 | Steps Per Second = 390.737\n",
            "[Learner] Action = 4.000 | Avg Td Error = -2.010 | Q = -0.545 | Reward = -2.556 | State = 390|9|52 | Steps = 776594 | Walltime = 2279.103\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.127 | Episodes = 3878 | Steps = 11634 | Steps Per Second = 380.516\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.551 | Q = -4.628 | Reward = 6.072 | State = 450|2|50 | Steps = 776954 | Walltime = 2280.104\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.605 | Episodes = 3998 | Steps = 11994 | Steps Per Second = 373.081\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.525 | Q = -4.565 | Reward = 5.656 | State = 450|2|50 | Steps = 777326 | Walltime = 2281.104\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.442 | Episodes = 4123 | Steps = 12369 | Steps Per Second = 396.000\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.049 | Q = -3.922 | Reward = -7.366 | State = 420|0|50 | Steps = 777697 | Walltime = 2282.107\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.910 | Episodes = 4247 | Steps = 12741 | Steps Per Second = 389.323\n",
            "[Learner] Action = 1.000 | Avg Td Error = -34.965 | Q = -3.916 | Reward = -36.290 | State = 420|-1|49 | Steps = 778057 | Walltime = 2283.108\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.815 | Episodes = 4367 | Steps = 13101 | Steps Per Second = 393.241\n",
            "[Learner] Action = 3.000 | Avg Td Error = -20.345 | Q = -12.254 | Reward = -32.114 | State = 450|2|50 | Steps = 778418 | Walltime = 2284.108\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.177 | Episodes = 4488 | Steps = 13464 | Steps Per Second = 362.213\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.528 | Q = -4.510 | Reward = -5.130 | State = 450|2|50 | Steps = 778768 | Walltime = 2285.109\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.483 | Episodes = 4603 | Steps = 13809 | Steps Per Second = 302.561\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.255 | Q = -1.030 | Reward = -4.451 | State = 420|6|53 | Steps = 779115 | Walltime = 2286.110\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.904 | Episodes = 4722 | Steps = 14166 | Steps Per Second = 378.081\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.653 | Q = -0.115 | Reward = 5.538 | State = 390|6|46 | Steps = 779482 | Walltime = 2287.112\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -70.749 | Episodes = 4843 | Steps = 14529 | Steps Per Second = 315.156\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.539 | Q = -4.436 | Reward = 6.169 | State = 450|2|50 | Steps = 779831 | Walltime = 2288.114\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.938 | Episodes = 4961 | Steps = 14883 | Steps Per Second = 395.764\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.217 | Episodes = 5414 | Steps = 16242 | Steps Per Second = 1773.740\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.788 | Q = -0.181 | Reward = 1.607 | State = 390|3|47 | Steps = 780000 | Walltime = 2290.221\n",
            "Check Point 52\n",
            "[Learner] Action = -2.000 | Avg Td Error = 2.719 | Q = -5.349 | Reward = 0.091 | State = 420|-1|52 | Steps = 780353 | Walltime = 2291.223\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.228 | Episodes = 119 | Steps = 357 | Steps Per Second = 339.318\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.654 | Q = -0.142 | Reward = -4.795 | State = 390|5|51 | Steps = 780714 | Walltime = 2292.225\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.483 | Episodes = 240 | Steps = 720 | Steps Per Second = 376.599\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.214 | Q = -1.686 | Reward = 6.185 | State = 420|0|48 | Steps = 781073 | Walltime = 2293.227\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.145 | Episodes = 360 | Steps = 1080 | Steps Per Second = 371.901\n",
            "[Learner] Action = -1.000 | Avg Td Error = -17.654 | Q = -2.539 | Reward = -20.194 | State = 390|-1|53 | Steps = 781426 | Walltime = 2294.229\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -49.223 | Episodes = 478 | Steps = 1434 | Steps Per Second = 385.105\n",
            "[Learner] Action = -4.000 | Avg Td Error = -17.971 | Q = -4.313 | Reward = -22.284 | State = 390|2|48 | Steps = 781796 | Walltime = 2295.231\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.982 | Episodes = 601 | Steps = 1803 | Steps Per Second = 353.830\n",
            "[Learner] Action = -4.000 | Avg Td Error = 16.547 | Q = -21.178 | Reward = -1.878 | State = 450|2|50 | Steps = 782151 | Walltime = 2296.233\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.569 | Episodes = 720 | Steps = 2160 | Steps Per Second = 367.835\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.452 | Q = -0.060 | Reward = -3.398 | State = 420|6|52 | Steps = 782495 | Walltime = 2297.233\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -61.713 | Episodes = 835 | Steps = 2505 | Steps Per Second = 377.661\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.385 | Q = -1.287 | Reward = -6.413 | State = 420|6|48 | Steps = 782853 | Walltime = 2298.235\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.353 | Episodes = 954 | Steps = 2862 | Steps Per Second = 303.708\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.339 | Q = -4.335 | Reward = 4.185 | State = 450|2|50 | Steps = 783198 | Walltime = 2299.236\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.021 | Episodes = 1070 | Steps = 3210 | Steps Per Second = 367.997\n",
            "[Learner] Action = 3.000 | Avg Td Error = 13.363 | Q = -12.256 | Reward = 1.260 | State = 450|2|50 | Steps = 783568 | Walltime = 2300.237\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.335 | Episodes = 1194 | Steps = 3582 | Steps Per Second = 377.321\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.781 | Q = -2.091 | Reward = 2.690 | State = 390|-1|55 | Steps = 783937 | Walltime = 2301.238\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.206 | Episodes = 1317 | Steps = 3951 | Steps Per Second = 391.321\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.618 | Q = -3.087 | Reward = -10.705 | State = 390|3|48 | Steps = 784301 | Walltime = 2302.239\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.891 | Episodes = 1439 | Steps = 4317 | Steps Per Second = 383.637\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.276 | Q = 0.029 | Reward = 0.305 | State = 390|3|46 | Steps = 784672 | Walltime = 2303.240\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -132.690 | Episodes = 1563 | Steps = 4689 | Steps Per Second = 383.895\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.007 | Q = -1.875 | Reward = -4.734 | State = 420|5|50 | Steps = 785008 | Walltime = 2304.242\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.503 | Episodes = 1674 | Steps = 5022 | Steps Per Second = 321.386\n",
            "[Learner] Action = -1.000 | Avg Td Error = -4.662 | Q = -6.954 | Reward = -8.420 | State = 450|2|50 | Steps = 785336 | Walltime = 2305.243\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.098 | Episodes = 1784 | Steps = 5352 | Steps Per Second = 337.271\n",
            "[Learner] Action = 2.000 | Avg Td Error = 8.003 | Q = -8.281 | Reward = 0.081 | State = 450|2|50 | Steps = 785671 | Walltime = 2306.245\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.348 | Episodes = 1896 | Steps = 5688 | Steps Per Second = 336.271\n",
            "[Learner] Action = -2.000 | Avg Td Error = -4.122 | Q = -1.035 | Reward = -4.322 | State = 420|6|53 | Steps = 786005 | Walltime = 2307.245\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.162 | Episodes = 2008 | Steps = 6024 | Steps Per Second = 358.620\n",
            "[Learner] Action = 1.000 | Avg Td Error = 3.370 | Q = -6.162 | Reward = -1.513 | State = 450|2|50 | Steps = 786353 | Walltime = 2308.247\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.097 | Episodes = 2125 | Steps = 6375 | Steps Per Second = 391.979\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.268 | Q = -2.719 | Reward = 5.569 | State = 420|-1|48 | Steps = 786699 | Walltime = 2309.248\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.447 | Episodes = 2240 | Steps = 6720 | Steps Per Second = 305.225\n",
            "[Learner] Action = 1.000 | Avg Td Error = -21.496 | Q = -6.125 | Reward = -27.152 | State = 450|2|50 | Steps = 787051 | Walltime = 2310.249\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.153 | Episodes = 2359 | Steps = 7077 | Steps Per Second = 348.944\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.203 | Q = -0.411 | Reward = -12.615 | State = 390|9|46 | Steps = 787424 | Walltime = 2311.251\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.495 | Episodes = 2484 | Steps = 7452 | Steps Per Second = 338.105\n",
            "[Learner] Action = 1.000 | Avg Td Error = -6.711 | Q = -0.149 | Reward = -6.208 | State = 420|-1|45 | Steps = 787792 | Walltime = 2312.251\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.021 | Episodes = 2607 | Steps = 7821 | Steps Per Second = 369.488\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.134 | Q = -4.750 | Reward = 5.699 | State = 420|2|52 | Steps = 788136 | Walltime = 2313.254\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.063 | Episodes = 2722 | Steps = 8166 | Steps Per Second = 357.743\n",
            "[Learner] Action = 3.000 | Avg Td Error = -4.583 | Q = -4.155 | Reward = -8.536 | State = 420|3|50 | Steps = 788464 | Walltime = 2314.255\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.198 | Episodes = 2832 | Steps = 8496 | Steps Per Second = 305.292\n",
            "[Learner] Action = 0.000 | Avg Td Error = -5.344 | Q = -4.000 | Reward = -7.422 | State = 420|2|51 | Steps = 788790 | Walltime = 2315.257\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.015 | Episodes = 2941 | Steps = 8823 | Steps Per Second = 317.270\n",
            "[Learner] Action = -2.000 | Avg Td Error = 1.379 | Q = -11.071 | Reward = -5.763 | State = 450|2|50 | Steps = 789107 | Walltime = 2316.257\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.960 | Episodes = 3046 | Steps = 9138 | Steps Per Second = 303.539\n",
            "[Learner] Action = 1.000 | Avg Td Error = 4.718 | Q = -6.162 | Reward = -0.166 | State = 450|2|50 | Steps = 789441 | Walltime = 2317.260\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.212 | Episodes = 3159 | Steps = 9477 | Steps Per Second = 340.318\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.286 | Q = -4.588 | Reward = 5.144 | State = 450|2|50 | Steps = 789771 | Walltime = 2318.260\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.046 | Episodes = 3270 | Steps = 9810 | Steps Per Second = 377.344\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.142 | Q = -0.045 | Reward = -4.188 | State = 390|9|55 | Steps = 790091 | Walltime = 2319.262\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.590 | Episodes = 3376 | Steps = 10128 | Steps Per Second = 365.782\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.937 | Q = -1.947 | Reward = -0.010 | State = 390|1|54 | Steps = 790402 | Walltime = 2320.263\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -129.768 | Episodes = 3480 | Steps = 10440 | Steps Per Second = 303.334\n",
            "[Learner] Action = -1.000 | Avg Td Error = -11.287 | Q = -6.897 | Reward = -15.367 | State = 450|2|50 | Steps = 790742 | Walltime = 2321.264\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.527 | Episodes = 3596 | Steps = 10788 | Steps Per Second = 367.159\n",
            "[Learner] Action = -1.000 | Avg Td Error = -8.010 | Q = -2.264 | Reward = -10.273 | State = 390|2|50 | Steps = 791090 | Walltime = 2322.266\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.035 | Episodes = 3712 | Steps = 11136 | Steps Per Second = 358.518\n",
            "[Learner] Action = 0.000 | Avg Td Error = -13.625 | Q = -4.538 | Reward = -14.067 | State = 450|2|50 | Steps = 791444 | Walltime = 2323.267\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.766 | Episodes = 3832 | Steps = 11496 | Steps Per Second = 384.434\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.711 | Q = -0.018 | Reward = 0.693 | State = 390|9|54 | Steps = 791812 | Walltime = 2324.269\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.305 | Episodes = 3954 | Steps = 11862 | Steps Per Second = 378.741\n",
            "[Learner] Action = 1.000 | Avg Td Error = 8.487 | Q = -4.642 | Reward = 5.347 | State = 420|-2|49 | Steps = 792180 | Walltime = 2325.270\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -248.325 | Episodes = 4077 | Steps = 12231 | Steps Per Second = 370.293\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.679 | Q = -4.783 | Reward = 6.075 | State = 450|2|50 | Steps = 792535 | Walltime = 2326.275\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.093 | Episodes = 4193 | Steps = 12579 | Steps Per Second = 332.477\n",
            "[Learner] Action = 2.000 | Avg Td Error = -10.821 | Q = -1.016 | Reward = -11.837 | State = 390|2|55 | Steps = 792896 | Walltime = 2327.277\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.116 | Episodes = 4315 | Steps = 12945 | Steps Per Second = 384.493\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.627 | Q = -4.712 | Reward = -7.611 | State = 450|2|50 | Steps = 793244 | Walltime = 2328.277\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.634 | Episodes = 4430 | Steps = 13290 | Steps Per Second = 330.494\n",
            "[Learner] Action = 0.000 | Avg Td Error = -10.519 | Q = -0.077 | Reward = -10.596 | State = 390|6|44 | Steps = 793595 | Walltime = 2329.278\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.917 | Episodes = 4550 | Steps = 13650 | Steps Per Second = 363.847\n",
            "[Learner] Action = 4.000 | Avg Td Error = 6.030 | Q = -1.332 | Reward = 4.698 | State = 390|1|55 | Steps = 793939 | Walltime = 2330.280\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.628 | Episodes = 4662 | Steps = 13986 | Steps Per Second = 320.845\n",
            "[Learner] Action = 4.000 | Avg Td Error = -5.074 | Q = -9.872 | Reward = -14.719 | State = 420|2|49 | Steps = 794283 | Walltime = 2331.281\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.112 | Episodes = 4777 | Steps = 14331 | Steps Per Second = 337.479\n",
            "[Learner] Action = 2.000 | Avg Td Error = 9.759 | Q = -3.780 | Reward = 6.150 | State = 420|1|51 | Steps = 794650 | Walltime = 2332.282\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.398 | Episodes = 4903 | Steps = 14709 | Steps Per Second = 396.100\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.182 | Episodes = 5116 | Steps = 15348 | Steps Per Second = 1795.763\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.273 | Episodes = 5752 | Steps = 17256 | Steps Per Second = 1975.029\n",
            "[Learner] Action = 4.000 | Avg Td Error = 14.932 | Q = -16.578 | Reward = -1.181 | State = 450|2|50 | Steps = 795000 | Walltime = 2334.846\n",
            "Check Point 53\n",
            "[Learner] Action = -3.000 | Avg Td Error = -3.308 | Q = -14.963 | Reward = -12.927 | State = 450|2|50 | Steps = 795332 | Walltime = 2335.847\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -77.145 | Episodes = 112 | Steps = 336 | Steps Per Second = 381.728\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.963 | Q = -4.526 | Reward = 1.278 | State = 450|2|50 | Steps = 795687 | Walltime = 2336.848\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.578 | Episodes = 231 | Steps = 693 | Steps Per Second = 391.491\n",
            "[Learner] Action = 4.000 | Avg Td Error = -3.777 | Q = -4.130 | Reward = -7.907 | State = 390|3|49 | Steps = 796052 | Walltime = 2337.851\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.811 | Episodes = 353 | Steps = 1059 | Steps Per Second = 386.714\n",
            "[Learner] Action = 1.000 | Avg Td Error = -19.004 | Q = -2.128 | Reward = -21.132 | State = 390|-3|52 | Steps = 796401 | Walltime = 2338.853\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.471 | Episodes = 470 | Steps = 1410 | Steps Per Second = 208.565\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.734 | Q = -5.041 | Reward = 1.680 | State = 420|2|49 | Steps = 796756 | Walltime = 2339.856\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.447 | Episodes = 589 | Steps = 1767 | Steps Per Second = 380.344\n",
            "[Learner] Action = -3.000 | Avg Td Error = -2.641 | Q = -15.139 | Reward = -12.437 | State = 450|2|50 | Steps = 797113 | Walltime = 2340.857\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.949 | Episodes = 708 | Steps = 2124 | Steps Per Second = 332.978\n",
            "[Learner] Action = -4.000 | Avg Td Error = -4.061 | Q = -4.040 | Reward = -8.101 | State = 390|3|50 | Steps = 797448 | Walltime = 2341.858\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -41.526 | Episodes = 820 | Steps = 2460 | Steps Per Second = 378.798\n",
            "[Learner] Action = 3.000 | Avg Td Error = 0.064 | Q = -3.085 | Reward = -2.958 | State = 420|1|48 | Steps = 797811 | Walltime = 2342.859\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.111 | Episodes = 941 | Steps = 2823 | Steps Per Second = 382.424\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.127 | Q = -0.063 | Reward = -1.190 | State = 390|4|47 | Steps = 798165 | Walltime = 2343.861\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.949 | Episodes = 1059 | Steps = 3177 | Steps Per Second = 345.352\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.671 | Q = -1.398 | Reward = 1.043 | State = 420|3|51 | Steps = 798504 | Walltime = 2344.862\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.072 | Episodes = 1173 | Steps = 3519 | Steps Per Second = 391.808\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.101 | Q = -1.587 | Reward = 4.514 | State = 390|-1|48 | Steps = 798847 | Walltime = 2345.864\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.107 | Episodes = 1288 | Steps = 3864 | Steps Per Second = 379.495\n",
            "[Learner] Action = -3.000 | Avg Td Error = 1.034 | Q = -0.993 | Reward = 2.044 | State = 420|4|54 | Steps = 799203 | Walltime = 2346.866\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.675 | Episodes = 1406 | Steps = 4218 | Steps Per Second = 333.234\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.688 | Q = -6.267 | Reward = -6.559 | State = 450|2|50 | Steps = 799546 | Walltime = 2347.867\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -193.221 | Episodes = 1521 | Steps = 4563 | Steps Per Second = 395.242\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.838 | Q = -0.297 | Reward = 5.542 | State = 390|2|57 | Steps = 799917 | Walltime = 2348.869\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.034 | Episodes = 1645 | Steps = 4935 | Steps Per Second = 400.832\n",
            "[Learner] Action = -1.000 | Avg Td Error = -16.743 | Q = -1.417 | Reward = -18.160 | State = 390|3|53 | Steps = 800278 | Walltime = 2349.872\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.786 | Episodes = 1764 | Steps = 5292 | Steps Per Second = 363.742\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.298 | Q = -0.107 | Reward = 1.192 | State = 390|5|50 | Steps = 800642 | Walltime = 2350.873\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.944 | Episodes = 1887 | Steps = 5661 | Steps Per Second = 387.691\n",
            "[Learner] Action = -2.000 | Avg Td Error = -12.464 | Q = -1.496 | Reward = -13.879 | State = 420|6|48 | Steps = 801001 | Walltime = 2351.875\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.402 | Episodes = 2007 | Steps = 6021 | Steps Per Second = 378.183\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.525 | Q = -4.076 | Reward = -7.515 | State = 420|2|48 | Steps = 801346 | Walltime = 2352.877\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.547 | Episodes = 2122 | Steps = 6366 | Steps Per Second = 362.808\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.694 | Q = -1.730 | Reward = 0.327 | State = 420|1|48 | Steps = 801715 | Walltime = 2353.877\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -99.064 | Episodes = 2245 | Steps = 6735 | Steps Per Second = 386.026\n",
            "[Learner] Action = 2.000 | Avg Td Error = -4.621 | Q = -1.216 | Reward = -5.836 | State = 390|5|48 | Steps = 802089 | Walltime = 2354.878\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.402 | Episodes = 2370 | Steps = 7110 | Steps Per Second = 366.528\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.351 | Q = -2.138 | Reward = 1.384 | State = 420|2|49 | Steps = 802460 | Walltime = 2355.878\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.137 | Episodes = 2494 | Steps = 7482 | Steps Per Second = 373.779\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.478 | Q = -5.125 | Reward = 1.309 | State = 450|2|50 | Steps = 802817 | Walltime = 2356.878\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.369 | Episodes = 2613 | Steps = 7839 | Steps Per Second = 382.459\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.865 | Q = -1.684 | Reward = 6.181 | State = 390|1|49 | Steps = 803177 | Walltime = 2357.881\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.150 | Episodes = 2733 | Steps = 8199 | Steps Per Second = 358.692\n",
            "[Learner] Action = -2.000 | Avg Td Error = -10.947 | Q = -7.269 | Reward = -15.599 | State = 420|2|51 | Steps = 803548 | Walltime = 2358.882\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.510 | Episodes = 2857 | Steps = 8571 | Steps Per Second = 395.714\n",
            "[Learner] Action = 1.000 | Avg Td Error = -0.990 | Q = -1.676 | Reward = -2.665 | State = 390|1|49 | Steps = 803919 | Walltime = 2359.884\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.176 | Episodes = 2981 | Steps = 8943 | Steps Per Second = 393.044\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.716 | Q = -5.173 | Reward = -0.514 | State = 450|2|50 | Steps = 804280 | Walltime = 2360.884\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.595 | Episodes = 3102 | Steps = 9306 | Steps Per Second = 380.712\n",
            "[Learner] Action = 2.000 | Avg Td Error = -29.811 | Q = -8.252 | Reward = -37.776 | State = 450|2|50 | Steps = 804651 | Walltime = 2361.886\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.780 | Episodes = 3226 | Steps = 9678 | Steps Per Second = 381.127\n",
            "[Learner] Action = -3.000 | Avg Td Error = 15.424 | Q = -15.104 | Reward = 1.597 | State = 450|2|50 | Steps = 805020 | Walltime = 2362.886\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.248 | Episodes = 3349 | Steps = 10047 | Steps Per Second = 327.254\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.503 | Q = -5.217 | Reward = -4.600 | State = 450|2|50 | Steps = 805362 | Walltime = 2363.889\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.196 | Episodes = 3464 | Steps = 10392 | Steps Per Second = 374.124\n",
            "[Learner] Action = 4.000 | Avg Td Error = -2.446 | Q = -7.889 | Reward = -10.199 | State = 420|2|53 | Steps = 805730 | Walltime = 2364.890\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -55.733 | Episodes = 3587 | Steps = 10761 | Steps Per Second = 366.571\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.421 | Q = -21.040 | Reward = -28.113 | State = 450|2|50 | Steps = 806098 | Walltime = 2365.893\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.723 | Episodes = 3710 | Steps = 11130 | Steps Per Second = 383.100\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.698 | Q = -1.448 | Reward = -2.955 | State = 420|2|47 | Steps = 806467 | Walltime = 2366.895\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.703 | Episodes = 3833 | Steps = 11499 | Steps Per Second = 397.589\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.633 | Q = -5.437 | Reward = -13.071 | State = 390|-2|50 | Steps = 806809 | Walltime = 2367.897\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.399 | Episodes = 3947 | Steps = 11841 | Steps Per Second = 385.648\n",
            "[Learner] Action = 4.000 | Avg Td Error = 0.635 | Q = -16.492 | Reward = -15.653 | State = 450|2|50 | Steps = 807180 | Walltime = 2368.898\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -58.393 | Episodes = 4071 | Steps = 12213 | Steps Per Second = 378.490\n",
            "[Learner] Action = -2.000 | Avg Td Error = -2.099 | Q = -0.096 | Reward = -1.163 | State = 420|4|56 | Steps = 807540 | Walltime = 2369.899\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.496 | Episodes = 4190 | Steps = 12570 | Steps Per Second = 332.864\n",
            "[Learner] Action = -2.000 | Avg Td Error = 3.741 | Q = -3.032 | Reward = 3.543 | State = 420|1|52 | Steps = 807897 | Walltime = 2370.899\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.213 | Episodes = 4309 | Steps = 12927 | Steps Per Second = 347.902\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.032 | Q = -1.288 | Reward = 5.805 | State = 420|-1|47 | Steps = 808232 | Walltime = 2371.901\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.772 | Episodes = 4422 | Steps = 13266 | Steps Per Second = 380.574\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.257 | Q = -12.273 | Reward = -7.836 | State = 450|2|50 | Steps = 808607 | Walltime = 2372.903\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.524 | Episodes = 4547 | Steps = 13641 | Steps Per Second = 311.813\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.942 | Q = -0.976 | Reward = 2.966 | State = 390|1|48 | Steps = 808974 | Walltime = 2373.905\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -152.523 | Episodes = 4670 | Steps = 14010 | Steps Per Second = 386.584\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.348 | Q = -15.221 | Reward = -14.257 | State = 450|2|50 | Steps = 809350 | Walltime = 2374.910\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.063 | Episodes = 4795 | Steps = 14385 | Steps Per Second = 387.620\n",
            "[Learner] Action = -4.000 | Avg Td Error = 15.950 | Q = -21.033 | Reward = -2.223 | State = 450|2|50 | Steps = 809722 | Walltime = 2375.911\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.898 | Episodes = 4919 | Steps = 14757 | Steps Per Second = 376.070\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.677 | Episodes = 5217 | Steps = 15651 | Steps Per Second = 2015.523\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 9.254 | Episodes = 5844 | Steps = 17532 | Steps Per Second = 1957.516\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.780 | Q = -5.851 | Reward = -0.965 | State = 450|2|50 | Steps = 810000 | Walltime = 2378.264\n",
            "Check Point 54\n",
            "[Learner] Action = 3.000 | Avg Td Error = 15.675 | Q = -12.239 | Reward = 3.478 | State = 450|2|50 | Steps = 810368 | Walltime = 2379.266\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.997 | Episodes = 124 | Steps = 372 | Steps Per Second = 383.310\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.399 | Q = -8.146 | Reward = 2.428 | State = 450|2|50 | Steps = 810704 | Walltime = 2380.269\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -85.759 | Episodes = 236 | Steps = 708 | Steps Per Second = 337.669\n",
            "[Learner] Action = 0.000 | Avg Td Error = -12.399 | Q = -5.916 | Reward = -14.277 | State = 450|2|50 | Steps = 811037 | Walltime = 2381.269\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.656 | Episodes = 347 | Steps = 1041 | Steps Per Second = 338.551\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.894 | Q = -3.372 | Reward = -2.674 | State = 420|1|50 | Steps = 811380 | Walltime = 2382.270\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.329 | Episodes = 462 | Steps = 1386 | Steps Per Second = 334.199\n",
            "[Learner] Action = -4.000 | Avg Td Error = 5.889 | Q = -2.816 | Reward = 3.074 | State = 390|-1|47 | Steps = 811708 | Walltime = 2383.270\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.304 | Episodes = 571 | Steps = 1713 | Steps Per Second = 310.743\n",
            "[Learner] Action = 3.000 | Avg Td Error = 12.172 | Q = -12.231 | Reward = 0.033 | State = 450|2|50 | Steps = 812041 | Walltime = 2384.273\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.924 | Episodes = 683 | Steps = 2049 | Steps Per Second = 337.923\n",
            "[Learner] Action = -3.000 | Avg Td Error = -3.572 | Q = -1.244 | Reward = -4.816 | State = 390|6|46 | Steps = 812392 | Walltime = 2385.275\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -52.683 | Episodes = 800 | Steps = 2400 | Steps Per Second = 324.645\n",
            "[Learner] Action = -3.000 | Avg Td Error = 3.990 | Q = -3.906 | Reward = 0.084 | State = 390|2|52 | Steps = 812756 | Walltime = 2386.276\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.879 | Episodes = 922 | Steps = 2766 | Steps Per Second = 365.294\n",
            "[Learner] Action = 4.000 | Avg Td Error = 5.249 | Q = -6.612 | Reward = -1.265 | State = 420|0|51 | Steps = 813122 | Walltime = 2387.277\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.469 | Episodes = 1045 | Steps = 3135 | Steps Per Second = 368.752\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.311 | Q = -0.849 | Reward = -6.098 | State = 420|0|47 | Steps = 813495 | Walltime = 2388.279\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.888 | Episodes = 1170 | Steps = 3510 | Steps Per Second = 382.076\n",
            "[Learner] Action = -3.000 | Avg Td Error = 13.139 | Q = -15.257 | Reward = 0.634 | State = 450|2|50 | Steps = 813862 | Walltime = 2389.280\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.857 | Episodes = 1292 | Steps = 3876 | Steps Per Second = 371.550\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.864 | Q = -1.233 | Reward = 4.631 | State = 390|3|52 | Steps = 814218 | Walltime = 2390.281\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.641 | Episodes = 1411 | Steps = 4233 | Steps Per Second = 345.684\n",
            "[Learner] Action = 0.000 | Avg Td Error = -39.435 | Q = -5.581 | Reward = -43.459 | State = 450|2|50 | Steps = 814563 | Walltime = 2391.282\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.915 | Episodes = 1526 | Steps = 4578 | Steps Per Second = 322.176\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.185 | Q = -5.557 | Reward = -1.587 | State = 450|2|50 | Steps = 814917 | Walltime = 2392.285\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.994 | Episodes = 1645 | Steps = 4935 | Steps Per Second = 389.721\n",
            "[Learner] Action = -4.000 | Avg Td Error = -4.273 | Q = -2.134 | Reward = -6.407 | State = 390|5|53 | Steps = 815266 | Walltime = 2393.286\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.322 | Episodes = 1761 | Steps = 5283 | Steps Per Second = 277.750\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.592 | Q = -1.340 | Reward = 1.066 | State = 420|3|51 | Steps = 815593 | Walltime = 2394.286\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.408 | Episodes = 1871 | Steps = 5613 | Steps Per Second = 301.474\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.185 | Q = -2.003 | Reward = -8.188 | State = 390|4|48 | Steps = 815950 | Walltime = 2395.287\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.970 | Episodes = 1992 | Steps = 5976 | Steps Per Second = 375.464\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.222 | Q = -5.573 | Reward = 0.897 | State = 450|2|50 | Steps = 816328 | Walltime = 2396.288\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.668 | Episodes = 2118 | Steps = 6354 | Steps Per Second = 378.172\n",
            "[Learner] Action = 3.000 | Avg Td Error = -18.607 | Q = -3.158 | Reward = -21.765 | State = 390|4|51 | Steps = 816589 | Walltime = 2397.289\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.710 | Episodes = 2202 | Steps = 6606 | Steps Per Second = 237.517\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.647 | Q = -5.658 | Reward = 5.660 | State = 450|2|50 | Steps = 816832 | Walltime = 2398.293\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.137 | Episodes = 2283 | Steps = 6849 | Steps Per Second = 259.265\n",
            "[Learner] Action = 3.000 | Avg Td Error = 1.625 | Q = -7.161 | Reward = -5.484 | State = 420|2|49 | Steps = 817075 | Walltime = 2399.297\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.220 | Episodes = 2364 | Steps = 7092 | Steps Per Second = 177.562\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.047 | Q = -0.828 | Reward = -1.773 | State = 420|6|51 | Steps = 817352 | Walltime = 2400.297\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.244 | Episodes = 2460 | Steps = 7380 | Steps Per Second = 378.206\n",
            "[Learner] Action = 2.000 | Avg Td Error = 11.142 | Q = -8.242 | Reward = 2.939 | State = 450|2|50 | Steps = 817719 | Walltime = 2401.298\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.606 | Episodes = 2583 | Steps = 7749 | Steps Per Second = 373.968\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.167 | Q = -3.012 | Reward = -9.162 | State = 420|5|51 | Steps = 818094 | Walltime = 2402.300\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.426 | Episodes = 2708 | Steps = 8124 | Steps Per Second = 381.035\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.205 | Q = -2.788 | Reward = -8.993 | State = 390|1|51 | Steps = 818464 | Walltime = 2403.301\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.363 | Episodes = 2831 | Steps = 8493 | Steps Per Second = 386.548\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.925 | Q = -0.055 | Reward = 1.005 | State = 420|6|52 | Steps = 818804 | Walltime = 2404.302\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.514 | Episodes = 2944 | Steps = 8832 | Steps Per Second = 332.266\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.973 | Q = -1.941 | Reward = -3.781 | State = 420|3|51 | Steps = 819153 | Walltime = 2405.303\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -63.637 | Episodes = 3061 | Steps = 9183 | Steps Per Second = 392.517\n",
            "[Learner] Action = 1.000 | Avg Td Error = -20.002 | Q = -6.311 | Reward = -25.298 | State = 450|2|50 | Steps = 819517 | Walltime = 2406.306\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.459 | Episodes = 3183 | Steps = 9549 | Steps Per Second = 401.164\n",
            "[Learner] Action = 4.000 | Avg Td Error = -8.347 | Q = -1.802 | Reward = -9.797 | State = 420|5|47 | Steps = 819889 | Walltime = 2407.308\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.559 | Episodes = 3307 | Steps = 9921 | Steps Per Second = 383.030\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.032 | Q = -0.811 | Reward = 1.221 | State = 390|3|51 | Steps = 820237 | Walltime = 2408.309\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.894 | Episodes = 3423 | Steps = 10269 | Steps Per Second = 383.251\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.313 | Q = -6.199 | Reward = 3.392 | State = 450|2|50 | Steps = 820593 | Walltime = 2409.312\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.934 | Episodes = 3540 | Steps = 10620 | Steps Per Second = 296.711\n",
            "[Learner] Action = 3.000 | Avg Td Error = -16.991 | Q = -4.515 | Reward = -20.309 | State = 420|0|50 | Steps = 820957 | Walltime = 2410.312\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.865 | Episodes = 3664 | Steps = 10992 | Steps Per Second = 248.699\n",
            "[Learner] Action = -3.000 | Avg Td Error = -9.197 | Q = -15.583 | Reward = -19.805 | State = 450|2|50 | Steps = 821305 | Walltime = 2411.312\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.498 | Episodes = 3779 | Steps = 11337 | Steps Per Second = 168.532\n",
            "[Learner] Action = -4.000 | Avg Td Error = 10.421 | Q = -21.055 | Reward = -5.732 | State = 450|2|50 | Steps = 821642 | Walltime = 2412.314\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -226.184 | Episodes = 3893 | Steps = 11679 | Steps Per Second = 205.512\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.440 | Q = -0.176 | Reward = 2.265 | State = 390|3|48 | Steps = 822006 | Walltime = 2413.314\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.071 | Episodes = 4017 | Steps = 12051 | Steps Per Second = 280.149\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.030 | Q = -7.499 | Reward = -4.124 | State = 450|2|50 | Steps = 822379 | Walltime = 2414.314\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.324 | Episodes = 4142 | Steps = 12426 | Steps Per Second = 378.297\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.768 | Q = -5.512 | Reward = 4.720 | State = 450|2|50 | Steps = 822746 | Walltime = 2415.317\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -18.219 | Episodes = 4264 | Steps = 12792 | Steps Per Second = 379.598\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.281 | Q = -5.442 | Reward = -4.504 | State = 450|2|50 | Steps = 823088 | Walltime = 2416.319\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.487 | Episodes = 4377 | Steps = 13131 | Steps Per Second = 340.456\n",
            "[Learner] Action = 1.000 | Avg Td Error = -1.993 | Q = -5.535 | Reward = -6.312 | State = 420|2|51 | Steps = 823451 | Walltime = 2417.320\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.991 | Episodes = 4498 | Steps = 13494 | Steps Per Second = 333.172\n",
            "[Learner] Action = 1.000 | Avg Td Error = -3.222 | Q = -6.128 | Reward = -7.987 | State = 450|2|50 | Steps = 823816 | Walltime = 2418.323\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.458 | Episodes = 4622 | Steps = 13866 | Steps Per Second = 371.792\n",
            "[Learner] Action = 3.000 | Avg Td Error = 4.182 | Q = -0.986 | Reward = 3.196 | State = 390|-4|55 | Steps = 824146 | Walltime = 2419.325\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.625 | Episodes = 4730 | Steps = 14190 | Steps Per Second = 333.022\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.405 | Q = -0.918 | Reward = 0.004 | State = 420|0|47 | Steps = 824501 | Walltime = 2420.326\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -55.873 | Episodes = 4850 | Steps = 14550 | Steps Per Second = 330.277\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.250 | Q = -3.111 | Reward = 0.139 | State = 390|-2|50 | Steps = 824867 | Walltime = 2421.328\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.136 | Episodes = 4973 | Steps = 14919 | Steps Per Second = 391.296\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.377 | Episodes = 5495 | Steps = 16485 | Steps Per Second = 1986.566\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.228 | Q = -1.285 | Reward = 1.943 | State = 390|2|49 | Steps = 825000 | Walltime = 2423.281\n",
            "Check Point 55\n",
            "[Learner] Action = -1.000 | Avg Td Error = 0.186 | Q = -4.845 | Reward = -2.016 | State = 420|2|51 | Steps = 825364 | Walltime = 2424.283\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.374 | Episodes = 123 | Steps = 369 | Steps Per Second = 390.749\n",
            "[Learner] Action = -1.000 | Avg Td Error = -24.834 | Q = -1.324 | Reward = -26.158 | State = 390|-2|55 | Steps = 825715 | Walltime = 2425.285\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.450 | Episodes = 240 | Steps = 720 | Steps Per Second = 335.196\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.677 | Q = -0.794 | Reward = -1.337 | State = 420|6|52 | Steps = 826086 | Walltime = 2426.285\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.246 | Episodes = 364 | Steps = 1092 | Steps Per Second = 381.474\n",
            "[Learner] Action = 2.000 | Avg Td Error = -5.445 | Q = -1.827 | Reward = -7.085 | State = 420|4|49 | Steps = 826456 | Walltime = 2427.286\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.339 | Episodes = 488 | Steps = 1464 | Steps Per Second = 377.911\n",
            "[Learner] Action = -3.000 | Avg Td Error = 16.952 | Q = -16.012 | Reward = 2.280 | State = 450|2|50 | Steps = 826801 | Walltime = 2428.290\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.965 | Episodes = 602 | Steps = 1806 | Steps Per Second = 327.050\n",
            "[Learner] Action = 0.000 | Avg Td Error = -14.583 | Q = -1.354 | Reward = -13.535 | State = 420|-1|47 | Steps = 827145 | Walltime = 2429.291\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.430 | Episodes = 717 | Steps = 2151 | Steps Per Second = 389.491\n",
            "[Learner] Action = 1.000 | Avg Td Error = 5.048 | Q = -4.643 | Reward = 0.965 | State = 420|2|50 | Steps = 827519 | Walltime = 2430.291\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -85.098 | Episodes = 842 | Steps = 2526 | Steps Per Second = 388.218\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.081 | Q = -4.370 | Reward = -0.142 | State = 420|2|51 | Steps = 827862 | Walltime = 2431.292\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.550 | Episodes = 956 | Steps = 2868 | Steps Per Second = 336.352\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.504 | Q = 0.024 | Reward = 0.528 | State = 390|4|46 | Steps = 828219 | Walltime = 2432.293\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.316 | Episodes = 1076 | Steps = 3228 | Steps Per Second = 382.273\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.497 | Q = -3.038 | Reward = -1.541 | State = 390|0|51 | Steps = 828588 | Walltime = 2433.296\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -73.353 | Episodes = 1199 | Steps = 3597 | Steps Per Second = 344.294\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.622 | Q = -3.905 | Reward = 1.717 | State = 390|2|52 | Steps = 828954 | Walltime = 2434.298\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.119 | Episodes = 1322 | Steps = 3966 | Steps Per Second = 364.775\n",
            "[Learner] Action = -2.000 | Avg Td Error = -13.437 | Q = -7.194 | Reward = -18.021 | State = 420|2|51 | Steps = 829318 | Walltime = 2435.299\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.437 | Episodes = 1444 | Steps = 4332 | Steps Per Second = 377.435\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.463 | Q = -6.400 | Reward = 0.806 | State = 450|2|50 | Steps = 829690 | Walltime = 2436.300\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -54.893 | Episodes = 1568 | Steps = 4704 | Steps Per Second = 368.665\n",
            "[Learner] Action = 3.000 | Avg Td Error = -13.845 | Q = -12.173 | Reward = -25.824 | State = 450|2|50 | Steps = 830061 | Walltime = 2437.301\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.483 | Episodes = 1692 | Steps = 5076 | Steps Per Second = 383.965\n",
            "[Learner] Action = 1.000 | Avg Td Error = -7.266 | Q = -1.474 | Reward = -8.181 | State = 420|2|47 | Steps = 830429 | Walltime = 2438.302\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -47.974 | Episodes = 1815 | Steps = 5445 | Steps Per Second = 395.403\n",
            "[Learner] Action = -3.000 | Avg Td Error = 4.697 | Q = -1.631 | Reward = 3.066 | State = 390|2|55 | Steps = 830788 | Walltime = 2439.304\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.233 | Episodes = 1934 | Steps = 5802 | Steps Per Second = 328.330\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.434 | Q = -16.091 | Reward = -12.009 | State = 450|2|50 | Steps = 831129 | Walltime = 2440.305\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -74.584 | Episodes = 2048 | Steps = 6144 | Steps Per Second = 338.187\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.570 | Q = -5.330 | Reward = 3.569 | State = 450|2|50 | Steps = 831496 | Walltime = 2441.306\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.955 | Episodes = 2171 | Steps = 6513 | Steps Per Second = 377.276\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.457 | Q = -0.064 | Reward = -5.522 | State = 390|9|54 | Steps = 831853 | Walltime = 2442.308\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.261 | Episodes = 2291 | Steps = 6873 | Steps Per Second = 373.613\n",
            "[Learner] Action = 1.000 | Avg Td Error = -5.110 | Q = -6.319 | Reward = -10.180 | State = 450|2|50 | Steps = 832198 | Walltime = 2443.308\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.197 | Episodes = 2406 | Steps = 7218 | Steps Per Second = 338.196\n",
            "[Learner] Action = -3.000 | Avg Td Error = 8.928 | Q = -4.977 | Reward = 3.951 | State = 390|-1|50 | Steps = 832557 | Walltime = 2444.309\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.887 | Episodes = 2526 | Steps = 7578 | Steps Per Second = 326.625\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.311 | Q = -0.050 | Reward = -1.223 | State = 420|5|45 | Steps = 832901 | Walltime = 2445.312\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.635 | Episodes = 2641 | Steps = 7923 | Steps Per Second = 329.793\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.973 | Q = -1.517 | Reward = -3.391 | State = 420|-2|47 | Steps = 833270 | Walltime = 2446.312\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.915 | Episodes = 2766 | Steps = 8298 | Steps Per Second = 379.038\n",
            "[Learner] Action = -3.000 | Avg Td Error = -6.330 | Q = -1.646 | Reward = -7.409 | State = 420|4|47 | Steps = 833615 | Walltime = 2447.314\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.271 | Episodes = 2880 | Steps = 8640 | Steps Per Second = 330.208\n",
            "[Learner] Action = 3.000 | Avg Td Error = -26.319 | Q = -0.638 | Reward = -26.957 | State = 390|9|49 | Steps = 833971 | Walltime = 2448.317\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.470 | Episodes = 2999 | Steps = 8997 | Steps Per Second = 336.594\n",
            "[Learner] Action = 0.000 | Avg Td Error = 9.224 | Q = -2.987 | Reward = 6.237 | State = 390|-1|51 | Steps = 834320 | Walltime = 2449.319\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.941 | Episodes = 3116 | Steps = 9348 | Steps Per Second = 325.258\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.178 | Q = -0.712 | Reward = -1.292 | State = 420|2|47 | Steps = 834681 | Walltime = 2450.322\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.005 | Episodes = 3238 | Steps = 9714 | Steps Per Second = 376.835\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.572 | Q = -2.447 | Reward = -0.162 | State = 420|1|49 | Steps = 835055 | Walltime = 2451.322\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.306 | Episodes = 3363 | Steps = 10089 | Steps Per Second = 393.351\n",
            "[Learner] Action = -3.000 | Avg Td Error = -33.558 | Q = -3.684 | Reward = -35.687 | State = 420|3|51 | Steps = 835431 | Walltime = 2452.324\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.906 | Episodes = 3488 | Steps = 10464 | Steps Per Second = 357.266\n",
            "[Learner] Action = 2.000 | Avg Td Error = -2.070 | Q = -1.318 | Reward = -3.388 | State = 390|5|51 | Steps = 835799 | Walltime = 2453.325\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.792 | Episodes = 3611 | Steps = 10833 | Steps Per Second = 392.064\n",
            "[Learner] Action = 4.000 | Avg Td Error = -0.175 | Q = -16.342 | Reward = -16.312 | State = 450|2|50 | Steps = 836174 | Walltime = 2454.326\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.762 | Episodes = 3736 | Steps = 11208 | Steps Per Second = 376.756\n",
            "[Learner] Action = 0.000 | Avg Td Error = 10.052 | Q = -5.114 | Reward = 5.657 | State = 450|2|50 | Steps = 836548 | Walltime = 2455.328\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.931 | Episodes = 3861 | Steps = 11583 | Steps Per Second = 350.499\n",
            "[Learner] Action = -4.000 | Avg Td Error = -0.915 | Q = -4.890 | Reward = -5.805 | State = 390|2|53 | Steps = 836917 | Walltime = 2456.329\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -64.731 | Episodes = 3984 | Steps = 11952 | Steps Per Second = 392.456\n",
            "[Learner] Action = 0.000 | Avg Td Error = -1.255 | Q = -0.182 | Reward = -1.438 | State = 390|3|47 | Steps = 837291 | Walltime = 2457.331\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.108 | Episodes = 4109 | Steps = 12327 | Steps Per Second = 394.572\n",
            "[Learner] Action = 1.000 | Avg Td Error = -11.368 | Q = -4.613 | Reward = -14.759 | State = 420|2|50 | Steps = 837649 | Walltime = 2458.334\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.221 | Episodes = 4229 | Steps = 12687 | Steps Per Second = 379.781\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.305 | Q = -4.995 | Reward = 6.232 | State = 420|-2|49 | Steps = 838013 | Walltime = 2459.334\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.204 | Episodes = 4351 | Steps = 13053 | Steps Per Second = 393.118\n",
            "[Learner] Action = 2.000 | Avg Td Error = -16.658 | Q = -4.798 | Reward = -19.506 | State = 420|0|51 | Steps = 838377 | Walltime = 2460.336\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.551 | Episodes = 4473 | Steps = 13419 | Steps Per Second = 391.467\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.367 | Q = -2.770 | Reward = 0.597 | State = 390|-1|53 | Steps = 838725 | Walltime = 2461.338\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -53.123 | Episodes = 4588 | Steps = 13764 | Steps Per Second = 336.937\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.009 | Q = -3.557 | Reward = -11.987 | State = 420|0|48 | Steps = 839065 | Walltime = 2462.341\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -81.104 | Episodes = 4702 | Steps = 14106 | Steps Per Second = 340.263\n",
            "[Learner] Action = -4.000 | Avg Td Error = 5.191 | Q = -12.967 | Reward = -4.630 | State = 420|2|51 | Steps = 839406 | Walltime = 2463.343\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.128 | Episodes = 4816 | Steps = 14448 | Steps Per Second = 329.387\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.675 | Q = -3.057 | Reward = -8.606 | State = 420|5|51 | Steps = 839739 | Walltime = 2464.345\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.151 | Episodes = 4927 | Steps = 14781 | Steps Per Second = 341.500\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.290 | Episodes = 5258 | Steps = 15774 | Steps Per Second = 1987.508\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.522 | Episodes = 5842 | Steps = 17526 | Steps Per Second = 1797.045\n",
            "[Learner] Action = -2.000 | Avg Td Error = -6.142 | Q = -11.309 | Reward = -12.645 | State = 450|2|50 | Steps = 840000 | Walltime = 2466.746\n",
            "Check Point 56\n",
            "[Learner] Action = -2.000 | Avg Td Error = 11.051 | Q = -11.281 | Reward = 1.670 | State = 450|2|50 | Steps = 840335 | Walltime = 2467.747\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.483 | Episodes = 113 | Steps = 339 | Steps Per Second = 385.329\n",
            "[Learner] Action = 2.000 | Avg Td Error = -3.001 | Q = -0.895 | Reward = -3.664 | State = 420|3|47 | Steps = 840705 | Walltime = 2468.749\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.925 | Episodes = 237 | Steps = 711 | Steps Per Second = 392.627\n",
            "[Learner] Action = 3.000 | Avg Td Error = -4.403 | Q = -3.074 | Reward = -6.629 | State = 420|1|53 | Steps = 841063 | Walltime = 2469.751\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.766 | Episodes = 357 | Steps = 1071 | Steps Per Second = 384.434\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.064 | Q = -0.039 | Reward = 1.069 | State = 420|5|49 | Steps = 841393 | Walltime = 2470.753\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.539 | Episodes = 467 | Steps = 1401 | Steps Per Second = 336.640\n",
            "[Learner] Action = 4.000 | Avg Td Error = -3.383 | Q = -3.097 | Reward = -6.480 | State = 390|4|52 | Steps = 841730 | Walltime = 2471.755\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -9.903 | Episodes = 579 | Steps = 1737 | Steps Per Second = 354.458\n",
            "[Learner] Action = 0.000 | Avg Td Error = 11.031 | Q = -5.277 | Reward = 5.947 | State = 450|2|50 | Steps = 842095 | Walltime = 2472.757\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.586 | Episodes = 701 | Steps = 2103 | Steps Per Second = 383.298\n",
            "[Learner] Action = 0.000 | Avg Td Error = -10.693 | Q = -5.212 | Reward = -11.698 | State = 450|2|50 | Steps = 842461 | Walltime = 2473.757\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -176.281 | Episodes = 823 | Steps = 2469 | Steps Per Second = 342.439\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.419 | Q = -6.601 | Reward = 0.583 | State = 450|2|50 | Steps = 842800 | Walltime = 2474.759\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.713 | Episodes = 937 | Steps = 2811 | Steps Per Second = 387.095\n",
            "[Learner] Action = 4.000 | Avg Td Error = -2.294 | Q = -16.570 | Reward = -18.656 | State = 450|2|50 | Steps = 843142 | Walltime = 2475.760\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.704 | Episodes = 1051 | Steps = 3153 | Steps Per Second = 384.528\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.328 | Q = -0.212 | Reward = -0.022 | State = 420|6|49 | Steps = 843513 | Walltime = 2476.761\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.229 | Episodes = 1175 | Steps = 3525 | Steps Per Second = 393.548\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.330 | Q = -5.207 | Reward = -12.349 | State = 420|2|54 | Steps = 843854 | Walltime = 2477.762\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.873 | Episodes = 1289 | Steps = 3867 | Steps Per Second = 386.655\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.796 | Q = -0.222 | Reward = 3.574 | State = 390|5|54 | Steps = 844230 | Walltime = 2478.765\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.020 | Episodes = 1414 | Steps = 4242 | Steps Per Second = 381.624\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.543 | Q = -6.609 | Reward = 3.199 | State = 450|2|50 | Steps = 844586 | Walltime = 2479.767\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.845 | Episodes = 1532 | Steps = 4596 | Steps Per Second = 326.727\n",
            "[Learner] Action = 1.000 | Avg Td Error = -2.471 | Q = -2.740 | Reward = -5.211 | State = 390|2|51 | Steps = 844924 | Walltime = 2480.770\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.477 | Episodes = 1645 | Steps = 4935 | Steps Per Second = 380.977\n",
            "[Learner] Action = -2.000 | Avg Td Error = -7.908 | Q = -11.262 | Reward = -15.820 | State = 450|2|50 | Steps = 845288 | Walltime = 2481.772\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.951 | Episodes = 1767 | Steps = 5301 | Steps Per Second = 374.402\n",
            "[Learner] Action = 0.000 | Avg Td Error = 6.996 | Q = -0.839 | Reward = 6.158 | State = 390|-2|56 | Steps = 845659 | Walltime = 2482.774\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.471 | Episodes = 1891 | Steps = 5673 | Steps Per Second = 379.988\n",
            "[Learner] Action = 3.000 | Avg Td Error = -9.756 | Q = -1.792 | Reward = -11.303 | State = 420|6|47 | Steps = 846021 | Walltime = 2483.776\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.723 | Episodes = 2012 | Steps = 6036 | Steps Per Second = 375.173\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.307 | Q = -0.881 | Reward = 4.426 | State = 390|-4|47 | Steps = 846393 | Walltime = 2484.776\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.258 | Episodes = 2137 | Steps = 6411 | Steps Per Second = 396.437\n",
            "[Learner] Action = -1.000 | Avg Td Error = -6.462 | Q = -8.043 | Reward = -11.433 | State = 450|2|50 | Steps = 846768 | Walltime = 2485.777\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.170 | Episodes = 2262 | Steps = 6786 | Steps Per Second = 380.447\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.031 | Q = -6.506 | Reward = -4.191 | State = 450|2|50 | Steps = 847120 | Walltime = 2486.779\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.340 | Episodes = 2379 | Steps = 7137 | Steps Per Second = 334.999\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.749 | Q = -5.312 | Reward = 0.271 | State = 450|2|50 | Steps = 847460 | Walltime = 2487.780\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.967 | Episodes = 2494 | Steps = 7482 | Steps Per Second = 359.019\n",
            "[Learner] Action = 3.000 | Avg Td Error = -4.226 | Q = -3.628 | Reward = -7.471 | State = 420|6|50 | Steps = 847828 | Walltime = 2488.781\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.775 | Episodes = 2617 | Steps = 7851 | Steps Per Second = 397.954\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.244 | Q = -1.663 | Reward = -9.908 | State = 390|-5|49 | Steps = 848175 | Walltime = 2489.782\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.814 | Episodes = 2733 | Steps = 8199 | Steps Per Second = 390.374\n",
            "[Learner] Action = -2.000 | Avg Td Error = 5.814 | Q = -11.054 | Reward = -2.117 | State = 450|2|50 | Steps = 848542 | Walltime = 2490.784\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -43.164 | Episodes = 2856 | Steps = 8568 | Steps Per Second = 387.811\n",
            "[Learner] Action = 1.000 | Avg Td Error = 0.853 | Q = -2.865 | Reward = -1.428 | State = 420|2|49 | Steps = 848879 | Walltime = 2491.785\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.761 | Episodes = 2968 | Steps = 8904 | Steps Per Second = 359.132\n",
            "[Learner] Action = -1.000 | Avg Td Error = 2.736 | Q = -7.926 | Reward = -1.770 | State = 450|2|50 | Steps = 849201 | Walltime = 2492.786\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.745 | Episodes = 3076 | Steps = 9228 | Steps Per Second = 362.046\n",
            "[Learner] Action = -2.000 | Avg Td Error = 8.460 | Q = -11.072 | Reward = 0.511 | State = 450|2|50 | Steps = 849551 | Walltime = 2493.787\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.088 | Episodes = 3193 | Steps = 9579 | Steps Per Second = 353.900\n",
            "[Learner] Action = -4.000 | Avg Td Error = 18.587 | Q = -20.804 | Reward = -0.638 | State = 450|2|50 | Steps = 849888 | Walltime = 2494.789\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -126.704 | Episodes = 3306 | Steps = 9918 | Steps Per Second = 353.383\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.448 | Q = -0.147 | Reward = 1.301 | State = 390|5|51 | Steps = 850240 | Walltime = 2495.790\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.916 | Episodes = 3424 | Steps = 10272 | Steps Per Second = 365.145\n",
            "[Learner] Action = -2.000 | Avg Td Error = -5.557 | Q = -11.194 | Reward = -11.970 | State = 450|2|50 | Steps = 850601 | Walltime = 2496.792\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.519 | Episodes = 3544 | Steps = 10632 | Steps Per Second = 365.230\n",
            "[Learner] Action = -3.000 | Avg Td Error = 5.007 | Q = -3.472 | Reward = 1.733 | State = 420|-2|48 | Steps = 850968 | Walltime = 2497.794\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -82.419 | Episodes = 3667 | Steps = 11001 | Steps Per Second = 389.082\n",
            "[Learner] Action = -1.000 | Avg Td Error = -3.593 | Q = -1.070 | Reward = -3.890 | State = 420|5|49 | Steps = 851334 | Walltime = 2498.796\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.427 | Episodes = 3789 | Steps = 11367 | Steps Per Second = 374.893\n",
            "[Learner] Action = 0.000 | Avg Td Error = -16.145 | Q = -3.371 | Reward = -17.529 | State = 420|0|52 | Steps = 851698 | Walltime = 2499.797\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.466 | Episodes = 3911 | Steps = 11733 | Steps Per Second = 384.422\n",
            "[Learner] Action = 0.000 | Avg Td Error = -11.558 | Q = -4.648 | Reward = -11.936 | State = 450|2|50 | Steps = 852046 | Walltime = 2500.798\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.122 | Episodes = 4028 | Steps = 12084 | Steps Per Second = 380.298\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.210 | Q = -3.075 | Reward = -2.876 | State = 420|1|52 | Steps = 852425 | Walltime = 2501.799\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.101 | Episodes = 4155 | Steps = 12465 | Steps Per Second = 404.504\n",
            "[Learner] Action = 2.000 | Avg Td Error = 1.007 | Q = -1.130 | Reward = -0.089 | State = 420|1|47 | Steps = 852790 | Walltime = 2502.800\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.750 | Episodes = 4277 | Steps = 12831 | Steps Per Second = 392.897\n",
            "[Learner] Action = -4.000 | Avg Td Error = -6.843 | Q = -20.644 | Reward = -21.599 | State = 450|2|50 | Steps = 853159 | Walltime = 2503.802\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.227 | Episodes = 4401 | Steps = 13203 | Steps Per Second = 397.376\n",
            "[Learner] Action = 2.000 | Avg Td Error = 10.258 | Q = -8.617 | Reward = 1.809 | State = 450|2|50 | Steps = 853526 | Walltime = 2504.803\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.808 | Episodes = 4522 | Steps = 13566 | Steps Per Second = 338.314\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.444 | Q = -0.878 | Reward = -3.321 | State = 390|2|56 | Steps = 853866 | Walltime = 2505.803\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -32.802 | Episodes = 4636 | Steps = 13908 | Steps Per Second = 332.644\n",
            "[Learner] Action = -3.000 | Avg Td Error = -7.045 | Q = -3.346 | Reward = -10.223 | State = 420|6|49 | Steps = 854207 | Walltime = 2506.805\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.473 | Episodes = 4752 | Steps = 14256 | Steps Per Second = 368.892\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.693 | Q = -1.069 | Reward = -4.762 | State = 390|5|48 | Steps = 854566 | Walltime = 2507.805\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -57.849 | Episodes = 4872 | Steps = 14616 | Steps Per Second = 380.470\n",
            "[Learner] Action = -4.000 | Avg Td Error = -30.471 | Q = -2.636 | Reward = -33.106 | State = 390|-2|48 | Steps = 854941 | Walltime = 2508.806\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.029 | Episodes = 4997 | Steps = 14991 | Steps Per Second = 382.494\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.895 | Episodes = 5615 | Steps = 16845 | Steps Per Second = 1993.806\n",
            "[Learner] Action = 2.000 | Avg Td Error = -1.058 | Q = -0.774 | Reward = -1.832 | State = 390|-1|46 | Steps = 855000 | Walltime = 2510.551\n",
            "Check Point 57\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.414 | Q = -0.237 | Reward = 1.178 | State = 390|5|46 | Steps = 855363 | Walltime = 2511.552\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.735 | Episodes = 123 | Steps = 369 | Steps Per Second = 252.704\n",
            "[Learner] Action = -3.000 | Avg Td Error = -2.981 | Q = -16.482 | Reward = -13.806 | State = 450|2|50 | Steps = 855703 | Walltime = 2512.554\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 7.406 | Episodes = 237 | Steps = 711 | Steps Per Second = 383.848\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.336 | Q = -4.872 | Reward = -0.988 | State = 420|2|52 | Steps = 856065 | Walltime = 2513.554\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.545 | Episodes = 358 | Steps = 1074 | Steps Per Second = 368.795\n",
            "[Learner] Action = -4.000 | Avg Td Error = -12.891 | Q = -3.443 | Reward = -13.776 | State = 420|6|52 | Steps = 856396 | Walltime = 2514.555\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -48.814 | Episodes = 469 | Steps = 1407 | Steps Per Second = 394.473\n",
            "[Learner] Action = 3.000 | Avg Td Error = -31.374 | Q = -12.679 | Reward = -43.803 | State = 450|2|50 | Steps = 856732 | Walltime = 2515.555\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.596 | Episodes = 580 | Steps = 1740 | Steps Per Second = 363.374\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.732 | Q = -5.145 | Reward = -7.972 | State = 450|2|50 | Steps = 857087 | Walltime = 2516.556\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -16.316 | Episodes = 700 | Steps = 2100 | Steps Per Second = 377.593\n",
            "[Learner] Action = -3.000 | Avg Td Error = -3.183 | Q = -3.745 | Reward = -4.302 | State = 420|3|51 | Steps = 857466 | Walltime = 2517.558\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 6.423 | Episodes = 826 | Steps = 2478 | Steps Per Second = 376.700\n",
            "[Learner] Action = -1.000 | Avg Td Error = -5.833 | Q = -1.443 | Reward = -6.707 | State = 420|1|54 | Steps = 857838 | Walltime = 2518.559\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.081 | Episodes = 950 | Steps = 2850 | Steps Per Second = 378.354\n",
            "[Learner] Action = 1.000 | Avg Td Error = 8.688 | Q = -4.628 | Reward = 4.228 | State = 420|2|50 | Steps = 858207 | Walltime = 2519.559\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.619 | Episodes = 1073 | Steps = 3219 | Steps Per Second = 395.130\n",
            "[Learner] Action = 4.000 | Avg Td Error = -7.658 | Q = -3.325 | Reward = -10.696 | State = 420|4|48 | Steps = 858575 | Walltime = 2520.561\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -0.083 | Episodes = 1196 | Steps = 3588 | Steps Per Second = 377.389\n",
            "[Learner] Action = 3.000 | Avg Td Error = 0.004 | Q = -1.357 | Reward = -1.353 | State = 390|1|46 | Steps = 858950 | Walltime = 2521.562\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.224 | Episodes = 1321 | Steps = 3963 | Steps Per Second = 376.599\n",
            "[Learner] Action = 3.000 | Avg Td Error = -41.227 | Q = -3.966 | Reward = -45.193 | State = 390|0|52 | Steps = 859316 | Walltime = 2522.563\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.997 | Episodes = 1444 | Steps = 4332 | Steps Per Second = 373.015\n",
            "[Learner] Action = 4.000 | Avg Td Error = 0.947 | Q = -6.286 | Reward = -4.447 | State = 420|-1|51 | Steps = 859683 | Walltime = 2523.564\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.315 | Episodes = 1567 | Steps = 4701 | Steps Per Second = 337.398\n",
            "[Learner] Action = 1.000 | Avg Td Error = -10.191 | Q = -2.160 | Reward = -12.351 | State = 390|-3|52 | Steps = 860053 | Walltime = 2524.565\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.326 | Episodes = 1691 | Steps = 5073 | Steps Per Second = 394.708\n",
            "[Learner] Action = 0.000 | Avg Td Error = -6.039 | Q = -0.195 | Reward = -6.235 | State = 390|5|55 | Steps = 860420 | Walltime = 2525.567\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.988 | Episodes = 1814 | Steps = 5442 | Steps Per Second = 382.123\n",
            "[Learner] Action = 1.000 | Avg Td Error = 8.919 | Q = -2.840 | Reward = 6.079 | State = 390|0|52 | Steps = 860783 | Walltime = 2526.569\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -265.631 | Episodes = 1935 | Steps = 5805 | Steps Per Second = 339.437\n",
            "[Learner] Action = -3.000 | Avg Td Error = -17.594 | Q = -2.446 | Reward = -20.040 | State = 390|2|47 | Steps = 861151 | Walltime = 2527.571\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -187.345 | Episodes = 2057 | Steps = 6171 | Steps Per Second = 335.607\n",
            "[Learner] Action = 3.000 | Avg Td Error = -12.886 | Q = -0.297 | Reward = -13.183 | State = 390|10|49 | Steps = 861479 | Walltime = 2528.572\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -85.454 | Episodes = 2167 | Steps = 6501 | Steps Per Second = 372.518\n",
            "[Learner] Action = -3.000 | Avg Td Error = 18.506 | Q = -16.222 | Reward = 2.703 | State = 450|2|50 | Steps = 861840 | Walltime = 2529.574\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.107 | Episodes = 2287 | Steps = 6861 | Steps Per Second = 329.284\n",
            "[Learner] Action = -2.000 | Avg Td Error = -1.502 | Q = -2.333 | Reward = -3.441 | State = 420|4|49 | Steps = 862202 | Walltime = 2530.576\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.747 | Episodes = 2409 | Steps = 7227 | Steps Per Second = 390.386\n",
            "[Learner] Action = -2.000 | Avg Td Error = -28.162 | Q = -2.796 | Reward = -30.958 | State = 390|1|51 | Steps = 862573 | Walltime = 2531.576\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 1.370 | Episodes = 2533 | Steps = 7599 | Steps Per Second = 385.730\n",
            "[Learner] Action = -4.000 | Avg Td Error = -16.590 | Q = -21.114 | Reward = -31.691 | State = 450|2|50 | Steps = 862942 | Walltime = 2532.579\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.875 | Episodes = 2656 | Steps = 7968 | Steps Per Second = 371.473\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.101 | Q = -4.432 | Reward = 1.821 | State = 420|2|51 | Steps = 863297 | Walltime = 2533.580\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -38.291 | Episodes = 2775 | Steps = 8325 | Steps Per Second = 333.995\n",
            "[Learner] Action = 3.000 | Avg Td Error = 11.229 | Q = -12.594 | Reward = -1.234 | State = 450|2|50 | Steps = 863670 | Walltime = 2534.580\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.093 | Episodes = 2900 | Steps = 8700 | Steps Per Second = 373.148\n",
            "[Learner] Action = -3.000 | Avg Td Error = 7.234 | Q = -16.436 | Reward = -5.288 | State = 450|2|50 | Steps = 864041 | Walltime = 2535.582\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.057 | Episodes = 3024 | Steps = 9072 | Steps Per Second = 339.583\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.062 | Q = -1.254 | Reward = -5.316 | State = 390|2|50 | Steps = 864385 | Walltime = 2536.584\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.629 | Episodes = 3138 | Steps = 9414 | Steps Per Second = 308.344\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.347 | Q = -3.746 | Reward = 5.245 | State = 420|2|50 | Steps = 864736 | Walltime = 2537.586\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.808 | Episodes = 3256 | Steps = 9768 | Steps Per Second = 366.550\n",
            "[Learner] Action = -4.000 | Avg Td Error = 15.201 | Q = -21.045 | Reward = -2.779 | State = 450|2|50 | Steps = 865109 | Walltime = 2538.587\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.508 | Episodes = 3381 | Steps = 10143 | Steps Per Second = 392.737\n",
            "[Learner] Action = -2.000 | Avg Td Error = 13.779 | Q = -11.243 | Reward = 3.511 | State = 450|2|50 | Steps = 865478 | Walltime = 2539.588\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.752 | Episodes = 3503 | Steps = 10509 | Steps Per Second = 380.608\n",
            "[Learner] Action = -3.000 | Avg Td Error = -1.982 | Q = -3.329 | Reward = -5.312 | State = 390|3|50 | Steps = 865835 | Walltime = 2540.589\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.193 | Episodes = 3623 | Steps = 10869 | Steps Per Second = 311.181\n",
            "[Learner] Action = -3.000 | Avg Td Error = -47.699 | Q = -3.470 | Reward = -51.168 | State = 390|1|49 | Steps = 866200 | Walltime = 2541.591\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -40.092 | Episodes = 3745 | Steps = 11235 | Steps Per Second = 394.560\n",
            "[Learner] Action = 4.000 | Avg Td Error = -6.761 | Q = -6.597 | Reward = -13.359 | State = 390|2|50 | Steps = 866573 | Walltime = 2542.592\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.240 | Episodes = 3869 | Steps = 11607 | Steps Per Second = 394.882\n",
            "[Learner] Action = 2.000 | Avg Td Error = 9.783 | Q = -8.193 | Reward = 1.776 | State = 450|2|50 | Steps = 866941 | Walltime = 2543.594\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -100.453 | Episodes = 3993 | Steps = 11979 | Steps Per Second = 390.713\n",
            "[Learner] Action = 4.000 | Avg Td Error = -12.904 | Q = -4.475 | Reward = -17.378 | State = 390|3|49 | Steps = 867288 | Walltime = 2544.596\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.408 | Episodes = 4109 | Steps = 12327 | Steps Per Second = 382.902\n",
            "[Learner] Action = 0.000 | Avg Td Error = -3.862 | Q = -4.942 | Reward = -4.515 | State = 450|2|50 | Steps = 867661 | Walltime = 2545.598\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.862 | Episodes = 4234 | Steps = 12702 | Steps Per Second = 376.036\n",
            "[Learner] Action = 3.000 | Avg Td Error = 6.135 | Q = -12.680 | Reward = -6.400 | State = 450|2|50 | Steps = 868027 | Walltime = 2546.598\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.898 | Episodes = 4355 | Steps = 13065 | Steps Per Second = 337.289\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.810 | Q = -1.434 | Reward = 2.376 | State = 390|-5|54 | Steps = 868354 | Walltime = 2547.600\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -11.586 | Episodes = 4465 | Steps = 13395 | Steps Per Second = 361.505\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.352 | Q = -2.408 | Reward = -11.761 | State = 390|2|46 | Steps = 868677 | Walltime = 2548.601\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.882 | Episodes = 4573 | Steps = 13719 | Steps Per Second = 382.773\n",
            "[Learner] Action = -3.000 | Avg Td Error = -8.468 | Q = -0.629 | Reward = -9.063 | State = 420|4|46 | Steps = 869038 | Walltime = 2549.604\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.081 | Episodes = 4693 | Steps = 14079 | Steps Per Second = 387.190\n",
            "[Learner] Action = -3.000 | Avg Td Error = -4.275 | Q = -4.520 | Reward = -6.171 | State = 420|1|50 | Steps = 869405 | Walltime = 2550.605\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.609 | Episodes = 4814 | Steps = 14442 | Steps Per Second = 338.414\n",
            "[Learner] Action = -1.000 | Avg Td Error = 4.825 | Q = -6.336 | Reward = 1.018 | State = 420|-2|50 | Steps = 869737 | Walltime = 2551.606\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -29.282 | Episodes = 4926 | Steps = 14778 | Steps Per Second = 385.199\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -67.346 | Episodes = 5208 | Steps = 15624 | Steps Per Second = 1946.915\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.807 | Episodes = 5795 | Steps = 17385 | Steps Per Second = 1914.042\n",
            "[Learner] Action = -4.000 | Avg Td Error = -55.886 | Q = -13.121 | Reward = -67.658 | State = 420|2|51 | Steps = 870000 | Walltime = 2554.050\n",
            "Check Point 58\n",
            "[Learner] Action = -1.000 | Avg Td Error = -0.717 | Q = -0.652 | Reward = -1.369 | State = 390|6|52 | Steps = 870360 | Walltime = 2555.053\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -119.025 | Episodes = 122 | Steps = 366 | Steps Per Second = 377.797\n",
            "[Learner] Action = -2.000 | Avg Td Error = -21.035 | Q = -1.767 | Reward = -22.802 | State = 390|-3|48 | Steps = 870713 | Walltime = 2556.054\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.097 | Episodes = 240 | Steps = 720 | Steps Per Second = 335.885\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.234 | Q = -0.217 | Reward = 0.415 | State = 420|2|46 | Steps = 871050 | Walltime = 2557.056\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.530 | Episodes = 353 | Steps = 1059 | Steps Per Second = 333.128\n",
            "[Learner] Action = -1.000 | Avg Td Error = 6.411 | Q = -1.547 | Reward = 4.863 | State = 390|-6|51 | Steps = 871394 | Walltime = 2558.056\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.233 | Episodes = 469 | Steps = 1407 | Steps Per Second = 395.876\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.709 | Q = -0.365 | Reward = 1.484 | State = 420|5|51 | Steps = 871771 | Walltime = 2559.056\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -45.088 | Episodes = 594 | Steps = 1782 | Steps Per Second = 386.750\n",
            "[Learner] Action = -3.000 | Avg Td Error = 16.740 | Q = -16.337 | Reward = 1.866 | State = 450|2|50 | Steps = 872139 | Walltime = 2560.057\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -24.278 | Episodes = 718 | Steps = 2154 | Steps Per Second = 379.449\n",
            "[Learner] Action = 3.000 | Avg Td Error = -6.453 | Q = -3.493 | Reward = -9.900 | State = 420|5|50 | Steps = 872504 | Walltime = 2561.058\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -8.960 | Episodes = 840 | Steps = 2520 | Steps Per Second = 385.943\n",
            "[Learner] Action = -2.000 | Avg Td Error = 5.917 | Q = -11.172 | Reward = -2.061 | State = 450|2|50 | Steps = 872876 | Walltime = 2562.061\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -99.720 | Episodes = 964 | Steps = 2892 | Steps Per Second = 371.813\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.674 | Q = -1.457 | Reward = 4.252 | State = 420|-1|47 | Steps = 873228 | Walltime = 2563.061\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.773 | Episodes = 1081 | Steps = 3243 | Steps Per Second = 338.187\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.356 | Q = -4.831 | Reward = 5.041 | State = 450|2|50 | Steps = 873593 | Walltime = 2564.061\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.838 | Episodes = 1204 | Steps = 3612 | Steps Per Second = 364.015\n",
            "[Learner] Action = -1.000 | Avg Td Error = -14.690 | Q = -7.594 | Reward = -20.099 | State = 450|2|50 | Steps = 873955 | Walltime = 2565.064\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -37.758 | Episodes = 1324 | Steps = 3972 | Steps Per Second = 345.049\n",
            "[Learner] Action = 1.000 | Avg Td Error = -9.443 | Q = -6.528 | Reward = -14.594 | State = 450|2|50 | Steps = 874307 | Walltime = 2566.066\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -1.791 | Episodes = 1442 | Steps = 4326 | Steps Per Second = 369.412\n",
            "[Learner] Action = 0.000 | Avg Td Error = 8.367 | Q = -4.768 | Reward = 5.101 | State = 450|2|50 | Steps = 874669 | Walltime = 2567.068\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.293 | Episodes = 1563 | Steps = 4689 | Steps Per Second = 365.772\n",
            "[Learner] Action = 0.000 | Avg Td Error = -7.045 | Q = -0.638 | Reward = -7.112 | State = 420|4|53 | Steps = 875039 | Walltime = 2568.070\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.991 | Episodes = 1686 | Steps = 5058 | Steps Per Second = 368.244\n",
            "[Learner] Action = -3.000 | Avg Td Error = -27.991 | Q = -5.365 | Reward = -33.355 | State = 390|-1|50 | Steps = 875392 | Walltime = 2569.071\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.297 | Episodes = 1803 | Steps = 5409 | Steps Per Second = 342.234\n",
            "[Learner] Action = 3.000 | Avg Td Error = 12.513 | Q = -12.425 | Reward = 0.207 | State = 450|2|50 | Steps = 875742 | Walltime = 2570.073\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -124.486 | Episodes = 1921 | Steps = 5763 | Steps Per Second = 380.091\n",
            "[Learner] Action = 1.000 | Avg Td Error = 1.720 | Q = -0.921 | Reward = 0.798 | State = 390|4|54 | Steps = 876116 | Walltime = 2571.075\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -107.873 | Episodes = 2046 | Steps = 6138 | Steps Per Second = 375.924\n",
            "[Learner] Action = -4.000 | Avg Td Error = -9.751 | Q = -3.170 | Reward = -11.052 | State = 420|5|48 | Steps = 876492 | Walltime = 2572.077\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.916 | Episodes = 2172 | Steps = 6516 | Steps Per Second = 365.729\n",
            "[Learner] Action = -4.000 | Avg Td Error = -17.732 | Q = -21.173 | Reward = -35.335 | State = 450|2|50 | Steps = 876854 | Walltime = 2573.078\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -35.415 | Episodes = 2293 | Steps = 6879 | Steps Per Second = 388.637\n",
            "[Learner] Action = -2.000 | Avg Td Error = -17.109 | Q = -11.246 | Reward = -25.833 | State = 450|2|50 | Steps = 877214 | Walltime = 2574.078\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -46.790 | Episodes = 2413 | Steps = 7239 | Steps Per Second = 342.513\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.741 | Q = -5.108 | Reward = 4.914 | State = 420|2|52 | Steps = 877565 | Walltime = 2575.079\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -17.059 | Episodes = 2531 | Steps = 7593 | Steps Per Second = 392.627\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.284 | Q = -0.221 | Reward = 2.080 | State = 420|2|46 | Steps = 877938 | Walltime = 2576.081\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 5.080 | Episodes = 2656 | Steps = 7968 | Steps Per Second = 381.578\n",
            "[Learner] Action = -1.000 | Avg Td Error = 3.209 | Q = -7.487 | Reward = -0.812 | State = 450|2|50 | Steps = 878305 | Walltime = 2577.081\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -70.261 | Episodes = 2779 | Steps = 8337 | Steps Per Second = 374.035\n",
            "[Learner] Action = 0.000 | Avg Td Error = -22.407 | Q = -3.627 | Reward = -24.126 | State = 420|2|50 | Steps = 878660 | Walltime = 2578.082\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -34.189 | Episodes = 2898 | Steps = 8694 | Steps Per Second = 370.631\n",
            "[Learner] Action = -4.000 | Avg Td Error = -25.006 | Q = -0.662 | Reward = -25.668 | State = 390|3|58 | Steps = 879027 | Walltime = 2579.082\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.553 | Episodes = 3021 | Steps = 9063 | Steps Per Second = 390.483\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.988 | Q = -5.255 | Reward = -1.040 | State = 420|2|50 | Steps = 879390 | Walltime = 2580.085\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.447 | Episodes = 3142 | Steps = 9426 | Steps Per Second = 370.653\n",
            "[Learner] Action = -3.000 | Avg Td Error = 4.017 | Q = -2.920 | Reward = 4.640 | State = 420|1|53 | Steps = 879738 | Walltime = 2581.085\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.736 | Episodes = 3258 | Steps = 9774 | Steps Per Second = 333.366\n",
            "[Learner] Action = -4.000 | Avg Td Error = 8.068 | Q = -1.875 | Reward = 6.193 | State = 390|-3|53 | Steps = 880075 | Walltime = 2582.085\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.991 | Episodes = 3370 | Steps = 10110 | Steps Per Second = 330.070\n",
            "[Learner] Action = 1.000 | Avg Td Error = 6.121 | Q = -6.382 | Reward = 0.543 | State = 450|2|50 | Steps = 880414 | Walltime = 2583.086\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.821 | Episodes = 3483 | Steps = 10449 | Steps Per Second = 336.172\n",
            "[Learner] Action = 0.000 | Avg Td Error = -8.299 | Q = -0.357 | Reward = -8.656 | State = 390|1|46 | Steps = 880768 | Walltime = 2584.088\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -22.699 | Episodes = 3603 | Steps = 10809 | Steps Per Second = 390.422\n",
            "[Learner] Action = 1.000 | Avg Td Error = -5.354 | Q = -6.326 | Reward = -10.310 | State = 450|2|50 | Steps = 881142 | Walltime = 2585.089\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -30.247 | Episodes = 3728 | Steps = 11184 | Steps Per Second = 374.915\n",
            "[Learner] Action = 0.000 | Avg Td Error = -2.405 | Q = -0.354 | Reward = -2.759 | State = 390|4|50 | Steps = 881509 | Walltime = 2586.091\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -31.619 | Episodes = 3851 | Steps = 11553 | Steps Per Second = 394.177\n",
            "[Learner] Action = 2.000 | Avg Td Error = 12.541 | Q = -8.317 | Reward = 4.255 | State = 450|2|50 | Steps = 881869 | Walltime = 2587.092\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 8.578 | Episodes = 3971 | Steps = 11913 | Steps Per Second = 390.228\n",
            "[Learner] Action = 3.000 | Avg Td Error = 8.692 | Q = -4.734 | Reward = 3.958 | State = 390|2|53 | Steps = 882227 | Walltime = 2588.092\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.031 | Episodes = 4089 | Steps = 12267 | Steps Per Second = 355.480\n",
            "[Learner] Action = -4.000 | Avg Td Error = 8.914 | Q = -21.154 | Reward = -7.150 | State = 450|2|50 | Steps = 882584 | Walltime = 2589.094\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.757 | Episodes = 4209 | Steps = 12627 | Steps Per Second = 383.345\n",
            "[Learner] Action = 2.000 | Avg Td Error = 11.369 | Q = -8.315 | Reward = 3.110 | State = 450|2|50 | Steps = 882949 | Walltime = 2590.096\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -20.201 | Episodes = 4332 | Steps = 12996 | Steps Per Second = 376.396\n",
            "[Learner] Action = -1.000 | Avg Td Error = 1.113 | Q = -0.034 | Reward = 1.061 | State = 420|5|44 | Steps = 883295 | Walltime = 2591.097\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.069 | Episodes = 4448 | Steps = 13344 | Steps Per Second = 373.979\n",
            "[Learner] Action = -1.000 | Avg Td Error = 9.695 | Q = -7.783 | Reward = 3.566 | State = 450|2|50 | Steps = 883667 | Walltime = 2592.097\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 4.850 | Episodes = 4570 | Steps = 13710 | Steps Per Second = 322.746\n",
            "[Learner] Action = -1.000 | Avg Td Error = -7.238 | Q = -3.953 | Reward = -11.191 | State = 390|-2|52 | Steps = 884029 | Walltime = 2593.100\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -15.807 | Episodes = 4693 | Steps = 14079 | Steps Per Second = 356.517\n",
            "[Learner] Action = 0.000 | Avg Td Error = -0.679 | Q = -2.574 | Reward = -1.336 | State = 420|1|49 | Steps = 884396 | Walltime = 2594.100\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -79.369 | Episodes = 4816 | Steps = 14448 | Steps Per Second = 384.728\n",
            "[Learner] Action = 2.000 | Avg Td Error = -52.966 | Q = -2.357 | Reward = -55.322 | State = 390|1|48 | Steps = 884771 | Walltime = 2595.102\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -71.392 | Episodes = 4942 | Steps = 14826 | Steps Per Second = 371.134\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 10.098 | Episodes = 5327 | Steps = 15981 | Steps Per Second = 1985.626\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -55.660 | Episodes = 5924 | Steps = 17772 | Steps Per Second = 1751.031\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.231 | Q = 0.016 | Reward = 1.247 | State = 390|5|60 | Steps = 885000 | Walltime = 2597.383\n",
            "Check Point 59\n",
            "[Learner] Action = 3.000 | Avg Td Error = 14.275 | Q = -11.835 | Reward = 2.487 | State = 450|2|50 | Steps = 885360 | Walltime = 2598.384\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -14.221 | Episodes = 121 | Steps = 363 | Steps Per Second = 325.847\n",
            "[Learner] Action = 2.000 | Avg Td Error = 1.198 | Q = -2.412 | Reward = -1.157 | State = 420|3|49 | Steps = 885692 | Walltime = 2599.387\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -27.195 | Episodes = 232 | Steps = 696 | Steps Per Second = 338.241\n",
            "[Learner] Action = -2.000 | Avg Td Error = -3.143 | Q = -11.588 | Reward = -9.888 | State = 450|2|50 | Steps = 886037 | Walltime = 2600.387\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -7.975 | Episodes = 347 | Steps = 1041 | Steps Per Second = 350.294\n",
            "[Learner] Action = 0.000 | Avg Td Error = 7.336 | Q = -1.416 | Reward = 5.920 | State = 390|-2|47 | Steps = 886403 | Walltime = 2601.387\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.528 | Episodes = 470 | Steps = 1410 | Steps Per Second = 384.657\n",
            "[Learner] Action = -3.000 | Avg Td Error = -14.824 | Q = -3.863 | Reward = -16.299 | State = 420|4|50 | Steps = 886774 | Walltime = 2602.388\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.024 | Episodes = 593 | Steps = 1779 | Steps Per Second = 318.643\n",
            "[Learner] Action = 3.000 | Avg Td Error = -7.506 | Q = -0.875 | Reward = -8.381 | State = 390|7|52 | Steps = 887145 | Walltime = 2603.390\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.585 | Episodes = 718 | Steps = 2154 | Steps Per Second = 370.445\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.423 | Q = -6.140 | Reward = 4.466 | State = 450|2|50 | Steps = 887514 | Walltime = 2604.392\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.020 | Episodes = 841 | Steps = 2523 | Steps Per Second = 389.636\n",
            "[Learner] Action = 1.000 | Avg Td Error = -7.799 | Q = -6.106 | Reward = -12.537 | State = 450|2|50 | Steps = 887891 | Walltime = 2605.393\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.141 | Episodes = 967 | Steps = 2901 | Steps Per Second = 389.829\n",
            "[Learner] Action = 0.000 | Avg Td Error = 3.090 | Q = -1.078 | Reward = 3.193 | State = 420|3|53 | Steps = 888263 | Walltime = 2606.393\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 3.385 | Episodes = 1091 | Steps = 3273 | Steps Per Second = 391.065\n",
            "[Learner] Action = -4.000 | Avg Td Error = -8.072 | Q = -3.742 | Reward = -11.814 | State = 390|5|49 | Steps = 888637 | Walltime = 2607.396\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.465 | Episodes = 1216 | Steps = 3648 | Steps Per Second = 379.220\n",
            "[Learner] Action = 1.000 | Avg Td Error = -12.215 | Q = -3.616 | Reward = -13.403 | State = 420|0|52 | Steps = 888987 | Walltime = 2608.398\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.181 | Episodes = 1332 | Steps = 3996 | Steps Per Second = 336.154\n",
            "[Learner] Action = 0.000 | Avg Td Error = 2.162 | Q = -0.574 | Reward = 1.780 | State = 420|4|50 | Steps = 889329 | Walltime = 2609.400\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -59.476 | Episodes = 1446 | Steps = 4338 | Steps Per Second = 337.878\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.948 | Q = -0.033 | Reward = 0.915 | State = 390|5|49 | Steps = 889658 | Walltime = 2610.400\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.440 | Episodes = 1556 | Steps = 4668 | Steps Per Second = 315.448\n",
            "[Learner] Action = -4.000 | Avg Td Error = 3.957 | Q = -21.275 | Reward = -11.057 | State = 450|2|50 | Steps = 890024 | Walltime = 2611.400\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -26.296 | Episodes = 1679 | Steps = 5037 | Steps Per Second = 387.286\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.631 | Q = -5.757 | Reward = -0.539 | State = 450|2|50 | Steps = 890400 | Walltime = 2612.401\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -51.085 | Episodes = 1805 | Steps = 5415 | Steps Per Second = 386.964\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.879 | Q = -0.298 | Reward = 0.581 | State = 390|7|48 | Steps = 890761 | Walltime = 2613.402\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -10.383 | Episodes = 1926 | Steps = 5778 | Steps Per Second = 380.240\n",
            "[Learner] Action = 1.000 | Avg Td Error = 9.431 | Q = -6.063 | Reward = 3.612 | State = 450|2|50 | Steps = 891123 | Walltime = 2614.403\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -13.778 | Episodes = 2047 | Steps = 6141 | Steps Per Second = 382.925\n",
            "[Learner] Action = -4.000 | Avg Td Error = -16.326 | Q = -1.570 | Reward = -16.725 | State = 420|6|54 | Steps = 891482 | Walltime = 2615.404\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.678 | Episodes = 2166 | Steps = 6498 | Steps Per Second = 330.625\n",
            "[Learner] Action = 3.000 | Avg Td Error = 7.729 | Q = -6.043 | Reward = 1.686 | State = 390|-2|50 | Steps = 891845 | Walltime = 2616.405\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.310 | Episodes = 2288 | Steps = 6864 | Steps Per Second = 335.625\n",
            "[Learner] Action = -2.000 | Avg Td Error = 8.103 | Q = -11.239 | Reward = 0.199 | State = 450|2|50 | Steps = 892212 | Walltime = 2617.406\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.767 | Episodes = 2411 | Steps = 7233 | Steps Per Second = 395.478\n",
            "[Learner] Action = 0.000 | Avg Td Error = 1.088 | Q = -0.859 | Reward = 0.229 | State = 390|3|51 | Steps = 892547 | Walltime = 2618.408\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -58.027 | Episodes = 2522 | Steps = 7566 | Steps Per Second = 343.570\n",
            "[Learner] Action = 2.000 | Avg Td Error = 6.532 | Q = -0.799 | Reward = 5.733 | State = 390|1|56 | Steps = 892919 | Walltime = 2619.408\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.354 | Episodes = 2647 | Steps = 7941 | Steps Per Second = 388.553\n",
            "[Learner] Action = 3.000 | Avg Td Error = 15.341 | Q = -12.228 | Reward = 3.160 | State = 450|2|50 | Steps = 893287 | Walltime = 2620.410\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -25.333 | Episodes = 2770 | Steps = 8310 | Steps Per Second = 376.734\n",
            "[Learner] Action = 4.000 | Avg Td Error = -9.560 | Q = -3.334 | Reward = -12.694 | State = 420|6|52 | Steps = 893664 | Walltime = 2621.411\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -33.133 | Episodes = 2896 | Steps = 8688 | Steps Per Second = 382.750\n",
            "[Learner] Action = -4.000 | Avg Td Error = 0.879 | Q = -21.620 | Reward = -14.534 | State = 450|2|50 | Steps = 894040 | Walltime = 2622.411\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -36.627 | Episodes = 3022 | Steps = 9066 | Steps Per Second = 385.801\n",
            "[Learner] Action = -3.000 | Avg Td Error = 3.043 | Q = -3.974 | Reward = 0.446 | State = 420|3|51 | Steps = 894413 | Walltime = 2623.411\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -23.293 | Episodes = 3147 | Steps = 9441 | Steps Per Second = 389.311\n",
            "[Learner] Action = 0.000 | Avg Td Error = 5.995 | Q = -6.319 | Reward = 3.066 | State = 420|-2|51 | Steps = 894755 | Walltime = 2624.413\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -4.088 | Episodes = 3259 | Steps = 9777 | Steps Per Second = 307.425\n",
            "[Learner] Action = -2.000 | Avg Td Error = -22.255 | Q = -6.372 | Reward = -26.852 | State = 420|-2|51 | Steps = 895099 | Walltime = 2625.416\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -6.968 | Episodes = 3376 | Steps = 10128 | Steps Per Second = 317.518\n",
            "[Learner] Action = -4.000 | Avg Td Error = -115.418 | Q = -0.370 | Reward = -115.791 | State = 420|3|56 | Steps = 895444 | Walltime = 2626.416\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.028 | Episodes = 3491 | Steps = 10473 | Steps Per Second = 348.297\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.768 | Q = -2.217 | Reward = 4.954 | State = 420|1|53 | Steps = 895768 | Walltime = 2627.419\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 0.956 | Episodes = 3599 | Steps = 10797 | Steps Per Second = 356.851\n",
            "[Learner] Action = -1.000 | Avg Td Error = -1.153 | Q = -0.001 | Reward = -1.154 | State = 390|8|60 | Steps = 896104 | Walltime = 2628.420\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -5.172 | Episodes = 3712 | Steps = 11136 | Steps Per Second = 358.549\n",
            "[Learner] Action = 1.000 | Avg Td Error = 3.647 | Q = -1.485 | Reward = 2.162 | State = 390|3|52 | Steps = 896434 | Walltime = 2629.421\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = 2.242 | Episodes = 3822 | Steps = 11466 | Steps Per Second = 345.002\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.475 | Q = -1.491 | Reward = 5.984 | State = 390|0|48 | Steps = 896757 | Walltime = 2630.422\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.370 | Episodes = 3930 | Steps = 11790 | Steps Per Second = 371.638\n",
            "[Learner] Action = 3.000 | Avg Td Error = 5.788 | Q = -12.161 | Reward = -6.211 | State = 450|2|50 | Steps = 897085 | Walltime = 2631.424\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -67.611 | Episodes = 4039 | Steps = 12117 | Steps Per Second = 365.984\n",
            "[Learner] Action = 0.000 | Avg Td Error = 0.065 | Q = -0.251 | Reward = -0.186 | State = 390|5|46 | Steps = 897422 | Walltime = 2632.426\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -19.934 | Episodes = 4152 | Steps = 12456 | Steps Per Second = 367.674\n",
            "[Learner] Action = 3.000 | Avg Td Error = -10.389 | Q = -2.854 | Reward = -13.243 | State = 390|0|47 | Steps = 897750 | Walltime = 2633.429\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -90.616 | Episodes = 4261 | Steps = 12783 | Steps Per Second = 345.656\n",
            "[Learner] Action = 0.000 | Avg Td Error = -4.737 | Q = -2.314 | Reward = -7.052 | State = 390|-2|48 | Steps = 898106 | Walltime = 2634.430\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -50.172 | Episodes = 4382 | Steps = 13146 | Steps Per Second = 379.472\n",
            "[Learner] Action = 3.000 | Avg Td Error = -5.125 | Q = -4.472 | Reward = -9.007 | State = 420|0|49 | Steps = 898468 | Walltime = 2635.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -2.033 | Episodes = 4503 | Steps = 13509 | Steps Per Second = 381.589\n",
            "[Learner] Action = -1.000 | Avg Td Error = -2.497 | Q = -7.531 | Reward = -6.463 | State = 450|2|50 | Steps = 898844 | Walltime = 2636.433\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -72.694 | Episodes = 4629 | Steps = 13887 | Steps Per Second = 391.820\n",
            "[Learner] Action = 1.000 | Avg Td Error = 7.020 | Q = -5.835 | Reward = 1.969 | State = 450|2|50 | Steps = 899221 | Walltime = 2637.436\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -28.285 | Episodes = 4755 | Steps = 14265 | Steps Per Second = 390.810\n",
            "[Learner] Action = 4.000 | Avg Td Error = -18.256 | Q = -0.570 | Reward = -18.826 | State = 390|5|44 | Steps = 899597 | Walltime = 2638.437\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -39.230 | Episodes = 4880 | Steps = 14640 | Steps Per Second = 388.685\n",
            "[Learner] Action = 0.000 | Avg Td Error = 4.710 | Q = -0.569 | Reward = 4.375 | State = 420|6|47 | Steps = 899961 | Walltime = 2639.440\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -12.898 | Episodes = 5001 | Steps = 15003 | Steps Per Second = 1737.012\n",
            "[Environment Loop] Episode Length = 3 | Episode Return = -3.066 | Episodes = 5635 | Steps = 16905 | Steps Per Second = 1855.887\n",
            "[Learner] Action = 1.000 | Avg Td Error = 10.502 | Q = -5.747 | Reward = 4.922 | State = 450|2|50 | Steps = 900000 | Walltime = 2641.135\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AxT8Rfihje74",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "2b086459-c073-43dc-85c8-cfa6d130e615"
      },
      "source": [
        "firstaction_q = qtable_bot._learner._qtable._qtable['450|2|50']\n",
        "labels = ['Sell 4', 'Sell 3', 'Sell 2', 'Sell 1', 'Hold', 'Buy 1', 'Buy 2', 'Buy 3', 'Buy 4']\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "fig, ax = plt.subplots()\n",
        "fah = ax.bar(x, firstaction_q, width, label='Actions')\n",
        "\n",
        "ax.set_ylabel('Q-Value')\n",
        "ax.set_title('First Action Q-Value')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "ax.legend()\n",
        "plt.savefig(f'{model_path}firstaction.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6cz84jkhCx8",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0b5a18ce-61b4-4873-ce2e-c639af0c469f"
      },
      "source": [
        "#@title Create Delta Hedging Bot and Set Env State Space\n",
        "delta_bot_env_attr = ['remaining_time', 'option_holding', 'option_strike',\n",
        "                      'interest_rate', 'stock_price', 'stock_dividend',\n",
        "                      'stock_sigma', 'stock_holding']\n",
        "environment.set_obs_attr(delta_bot_env_attr)\n",
        "gbm_pred.restart()\n",
        "environment.set_stock_generator(gbm_pred)\n",
        "spec = specs.make_environment_spec(environment)\n",
        "delta_pred_logger = CSVLogger(f'delta_pred/{model_name}',\n",
        "                              label='delta_pred')\n",
        "if os.path.exists(delta_pred_logger.file_path):\n",
        "  os.remove(delta_pred_logger.file_path)\n",
        "delta_bot = DeltaHedgeBot(environment_spec=spec,\n",
        "                          pred_episode=num_prediction_episodes,\n",
        "                          logger = delta_pred_logger)\n",
        "loop = acme.EnvironmentLoop(environment, delta_bot)\n",
        "loop.run(num_episodes=num_prediction_episodes)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Environment Loop] Episode Length = 3 | Episode Return = -21.882 | Episodes = 575 | Steps = 1725 | Steps Per Second = 1769.251\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TVyKngjMpegX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_progress = pd.read_csv(pred_logger.file_path)\n",
        "delta_hedge_status = pd.read_csv(delta_pred_logger.file_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Nuk2Nbh-gdC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 504
        },
        "outputId": "d0984f3a-255d-4202-a542-e39b5fc8da45"
      },
      "source": [
        "fig = plt.figure(figsize=(30,10))\n",
        "ax = plt.subplot(111)\n",
        "ax.plot(train_progress['train_episodes'], train_progress['reward_mean'], label='Reward Mean')\n",
        "ax.hlines(delta_hedge_status['reward_mean'], label='Delta Hedge Reward Mean', xmin=0, \n",
        "          xmax=train_progress['train_episodes'].max(), linestyles='dashed')\n",
        "\n",
        "# Shrink current axis by 20%\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "\n",
        "# Put a legend to the right of the current axis\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "plt.savefig(f'{model_path}reward.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qdWFf9eLqlVr",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 518
        },
        "outputId": "ca42b298-0f8b-4936-cb0a-e7e83d1f1192"
      },
      "source": [
        "fig = plt.figure(figsize=(30,10))\n",
        "ax = plt.subplot(111)\n",
        "\n",
        "pred_count = train_progress['train_episodes']\n",
        "ax.plot(pred_count, train_progress['pnl_mean'], label='Mean', color='g')\n",
        "ax.plot(pred_count, train_progress['pnl_std'], label='Std', color='r')\n",
        "ax.plot(pred_count, train_progress['pnl_quantile_5'], label='Quantile 5%', color='b')\n",
        "ax.plot(pred_count, train_progress['pnl_quantile_95'], label='Quantile 95%', color='y')\n",
        "ax.hlines(delta_hedge_status['pnl_mean'], label='Delta Hedge Mean', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors='g')\n",
        "ax.hlines(delta_hedge_status['pnl_std'], label='Delta Hedge Mean', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors='r')\n",
        "ax.hlines(delta_hedge_status['pnl_quantile_5'], label='Delta Hedge Quantile 5%', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors='b')\n",
        "ax.hlines(delta_hedge_status['pnl_quantile_95'], label='Delta Hedge Quantile 905', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors='y')\n",
        "\n",
        "# Shrink current axis by 20%\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "\n",
        "# Put a legend to the right of the current axis\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.title('Training Progress')\n",
        "plt.savefig(f'{model_path}quantiles.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5_2y6ufd5Y69",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 532
        },
        "outputId": "b460ac1c-5266-4f03-b5d5-da560e15d433"
      },
      "source": [
        "fig = plt.figure(figsize=(30,10))\n",
        "ax = plt.subplot(111)\n",
        "\n",
        "pred_count = train_progress['train_episodes']\n",
        "ax.scatter(pred_count, train_progress['hold'], label='Hold', color=(0.5,0,0))\n",
        "ax.scatter(pred_count, train_progress['buy-1'], label='Buy-1', color=(0.6,0,0))\n",
        "ax.scatter(pred_count, train_progress['buy-2'], label='Buy-2', color=(0.7,0,0))\n",
        "ax.scatter(pred_count, train_progress['buy-3'], label='Buy-3', color=(0.8,0,0))\n",
        "ax.scatter(pred_count, train_progress['buy-4'], label='Buy-4', color=(0.9,0,0))\n",
        "ax.scatter(pred_count, train_progress['sell-1'], label='Sell-1', color=(0.4,0,0))\n",
        "ax.scatter(pred_count, train_progress['sell-2'], label='Sell-2', color=(0.3,0,0))\n",
        "ax.scatter(pred_count, train_progress['sell-3'], label='Sell-3', color=(0.2,0,0))\n",
        "ax.scatter(pred_count, train_progress['sell-4'], label='Sell-4', color=(0.1,0,0))\n",
        "ax.hlines(delta_hedge_status['hold'], label='Delta Hedge Hold', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.5,0,0))\n",
        "ax.hlines(delta_hedge_status['buy-1'], label='Delta Hedge Buy-1', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.6,0,0))\n",
        "ax.hlines(delta_hedge_status['buy-2'], label='Delta Hedge Buy-2', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.7,0,0))\n",
        "ax.hlines(delta_hedge_status['buy-3'], label='Delta Hedge Buy-3', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.8,0,0))\n",
        "ax.hlines(delta_hedge_status['buy-4'], label='Delta Hedge Buy-4', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.9,0,0))\n",
        "ax.hlines(delta_hedge_status['sell-1'], label='Delta Hedge Sell-1', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.4,0,0))\n",
        "ax.hlines(delta_hedge_status['sell-2'], label='Delta Hedge Sell-2', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.3,0,0))\n",
        "ax.hlines(delta_hedge_status['sell-3'], label='Delta Hedge Sell-3', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.2,0,0))\n",
        "ax.hlines(delta_hedge_status['sell-4'], label='Delta Hedge Sell-4', xmin=0, xmax=pred_count.max(), linestyles='dashed', colors=(0.1,0,0))\n",
        "\n",
        "# Shrink current axis by 20%\n",
        "box = ax.get_position()\n",
        "ax.set_position([box.x0, box.y0, box.width * 0.8, box.height])\n",
        "\n",
        "# Put a legend to the right of the current axis\n",
        "ax.legend(loc='center left', bbox_to_anchor=(1, 0.5))\n",
        "\n",
        "plt.title('Training Progress')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 2160x720 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W7RWy8NKGnWo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "8cc790e0-835b-4558-92c1-056bbeff60e9"
      },
      "source": [
        "labels = ['Sell 4', 'Sell 3', 'Sell 2', 'Sell 1', 'Hold', 'Buy 1', 'Buy 2', 'Buy 3', 'Buy 4']\n",
        "delta_hedge_actions = delta_hedge_status[['sell-4','sell-3','sell-2','sell-1','hold', 'buy-1','buy-2','buy-3','buy-4']].iloc[-1]\n",
        "qtable_hedge_actions = train_progress[['sell-4','sell-3','sell-2','sell-1','hold', 'buy-1','buy-2','buy-3','buy-4']].iloc[-1]\n",
        "x = np.arange(len(labels))\n",
        "width = 0.35\n",
        "fig, ax = plt.subplots()\n",
        "deltah = ax.bar(x - width/2, delta_hedge_actions, width, label='Delta hedge')\n",
        "qtableh = ax.bar(x + width/2, qtable_hedge_actions, width, label='QTable hedge')\n",
        "\n",
        "ax.set_ylabel('Percentage')\n",
        "ax.set_title('Action Percentage')\n",
        "ax.set_xticks(x)\n",
        "ax.set_xticklabels(labels)\n",
        "\n",
        "ax.legend()\n",
        "plt.savefig(f'{model_path}action.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEICAYAAABS0fM3AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dfZwXZb3/8de7VW6OIJQuvwywxTIJElBWBD2lVgppaXVUxDTUY2qKaTeadqOIePKUmUfEivKmGxKMssMxSrrTAk3Z1fWGRQQRE0RdUTDEmwU+vz9mdvuy7rLfXXb2yzLv5+Oxj/3OzHVd89nZ3fnMXDNzjSICMzPLr7eVOgAzMystJwIzs5xzIjAzyzknAjOznHMiMDPLOScCM7OccyKwLk/SDyR9s9RxmHVVTgRWUpLulvSypO5Flj9N0oLCeRFxTkRcmUFskyXVS9ogaZ2keyWN6ej1tJekCkkhaZdSx2JdmxOBlYykCuCDQADHljSYls2OiF5AObAA+LUktaUB76htR+dEYKX0WeDvwK3AxMIFkgZK+rWkOklrJd0g6f3AD4AxDUfpadlbJU0tqPs5ScslvSRprqR3FSwLSedIWpYe5U8vZsceEfXAT4B3AntI6iPpJklrJK2WNFVSWbqO0yQtlPQ9SWuByZJ6SvqupKclrZe0QFLPtPzo9GxjnaSHJR1eEO/dkq5M2/unpPmS9kwX/zX9vi7dHmMkvUfSn9Nt9qKkmZL6FrR3oKSH0rZ+KWl2k233cUk1BWdAw1r9LVqX50RgpfRZYGb6NVbS/wNId6h3Ak8DFUB/YFZELAHOAe6LiF4R0bdpg5I+DHwLOBHYK21jVpNiHwcOAoal5ca2FmjadXUa8ExEvEiSvDYB7wUOAI4CziyocjCwAvh/wFXANcBI4BDgHcDFwBZJ/YHfAlPT+V8BfiWpvKCtk4HTgX5At7QMwIfS733T7XEfoPTnfxfwfmAgMDn9GboBd6SxvwO4DfhUwc94AHAzcDawB/BDYG6x3XbWdTkRWElI+nfg3cDtEVENPEmywwMYRbIjuygiXo2I1yNiQQtNNfUZ4OaIeDAi3gAuJTmDqCgoc3VErIuIfwB/AUZso70T0zOPZ0h25J9KE9bRwIVpfC8A3wNOKqj3bERMi4hNwBvAGcAFEbE6IjZHxL1pfKcA8yJiXkRsiYg/AFVp+w1uiYgnIuI14PZtxRsRyyPiDxHxRkTUAdcCh6WLRwO7ANdHRH1E/Bp4oKD6WcAPI+L+NMafpLGP3sb2sZ2A+y6tVCYC89Oja4BfpPO+R3IU+3S6E22rdwEPNkxExIa0e6Y/sDKd/VxB+Y1Ar220d3tEnFI4Q9IoYFdgTUGv0ttIkkWDws97Aj1Ikl1T7wZOkPSJgnm7kiSoBkXHmyap/yG59tI7jevldPG7gNWx9UiThXG+G5go6fyCed3SerYTcyKwTpf2jZ8IlElq2Ml1B/pKGk6yc9pb0i7NJIPWhst9lmSH1rCu3Ui6OVZ3SPCJZ0iOlPfcRrIqjPNF4HXgPcDDzbT1s4j4XDviaG5b/Fc6f/+IeEnSJ4Eb0mVrgP6SVJAMBvKvBPUMcFVEXNWOWKwLc9eQlcIngc3AEJJujhEk/dl/I7lu8ADJTutqSbtJ6iHp0LTu88CAtL+7ObcBp0sakfZt/xdwf0Ss7KjgI2INMB/4rqTdJb0tvUh7WAvlt5D0vV8r6V2SytILu92BnwOfkDQ2nd9D0uGSBhQRSh2wBdinYF5vYAOwPr3+cFHBsvtItvskSbtIOo6kG67Bj4BzJB2sxG6SjpHUu6gNY12WE4GVwkSSfu9/RMRzDV8kR66fIbng+QmSC7H/AFYB49O6fwYWA89JerFpwxHxR+CbwK9Iksl72LrvvqN8lqTbpJak62UOycXplnwFeBRYBLwE/Dfwtoh4BjgO+BrJjv0Zkp13q/+bEbGR5EL0wvQun9HAFcCBwHqSi9C/Lij/JvBp4D+BdSTXJ+4kObshIqqAz5H8Hl4GlpNcILednPxiGrP8knQ/8IOIuKXUsVjp+IzALEckHSbpnWnX0ESSW2h/X+q4rLR8sdgsX/YjuQV1N5LnHI5Pr3lYjrlryMws59w1ZGaWc12ua2jPPfeMioqKUodhZtalVFdXvxgR5c0t63KJoKKigqqqqlKHYWbWpUh6uqVl7hoyM8s5JwIzs5xzIjAzy7kud43AzEqvvr6eVatW8frrr5c6FGuiR48eDBgwgF133bXoOk4EZtZmq1atonfv3lRUVFDEC96sk0QEa9euZdWqVQwaNKjoeu4aMrM2e/3119ljjz2cBHYwkthjjz3afKbmRGBm7eIksGNqz+/FicDMLOd8jcDMtlvFJb/t0PZWXn1Mq2XKysrYf//9qa+vZ5ddduGzn/0sX/ziF3nb21o+vl25ciUf//jHeeyxx6ipqeHZZ5/l6KOPbrF8U3fffTfXXHMNd955Z9F1Wlr/jsSJwKyjTO7TjjrrOz6OnOjZsyc1NTUAvPDCC5x88sm88sorXHHFFUXVr6mpoaqqqk2JYGflriEz6/L69evHjBkzuOGGG4gINm/ezEUXXcRBBx3EsGHD+OEPf7hV+TfffJPLLruM2bNnM2LECGbPns0DDzzAmDFjOOCAAzjkkENYunRps+vasGEDxx9/PIMHD+Yzn/kMDSM4V1dXc9hhhzFy5EjGjh3LmjVrGucPHz6c4cOHM3369MZ2Nm7cyIknnsiQIUP41Kc+xcEHH9w4fM78+fMZM2YMBx54ICeccAIbNmzIYrM1ciIws53CPvvsw+bNm3nhhRe46aab6NOnD4sWLWLRokX86Ec/4qmnnmos261bN6ZMmcL48eOpqalh/PjxDB48mL/97W889NBDTJkyha997WvNruehhx7iuuuuo7a2lhUrVrBw4ULq6+s5//zzmTNnDtXV1Zxxxhl8/etfB+D0009n2rRpPPzww1u1c+ONN/L2t7+d2tparrzySqqrqwF48cUXmTp1Kn/84x958MEHqays5Nprr81oqyXcNWRmO5358+fzyCOPMGfOHADWr1/PsmXLeN/73tdinfXr1zNx4kSWLVuGJOrr65stN2rUKAYMGADAiBEjWLlyJX379uWxxx7jyCOPBGDz5s3stdderFu3jnXr1vGhD30IgFNPPZXf/e53ACxYsIALLrgAgA984AMMGzYMgL///e/U1tZy6KGHAsnZy5gxY7Z3k2yTE4GZ7RRWrFhBWVkZ/fr1IyKYNm0aY8eO3arMypUrW6z/zW9+kyOOOII77riDlStXcvjhhzdbrnv37o2fy8rK2LRpExHB0KFDue+++7Yqu27dujb/HBHBkUceyW233dbmuu3lriEz6/Lq6uo455xzmDRpEpIYO3Ys3//+9xuP6p944gleffXVrer07t2bf/7zn43T69evp3///gDceuutbVr/fvvtR11dXWMiqK+vZ/HixfTt25e+ffuyYMECAGbOnNlY59BDD+X2228HoLa2lkcffRSA0aNHs3DhQpYvXw7Aq6++yhNPPNGmeNoq0zMCSeOA/wHKgB9HxNXNlDkRmAwE8HBEnJxlTGbW8Yq53bOjvfbaa4wYMaLx9tFTTz2VL33pSwCceeaZrFy5kgMPPJCIoLy8nN/85jdb1T/iiCO4+uqrGTFiBJdeeikXX3wxEydOZOrUqRxzTNt+nm7dujFnzhy+8IUvsH79ejZt2sSFF17I0KFDueWWWzjjjDOQxFFHHdVY59xzz2XixIkMGTKEwYMHM3ToUPr06UN5eTm33norEyZM4I033gBg6tSp2+zW2l6ZvbNYUhnwBHAksApYBEyIiNqCMvuSvEj7wxHxsqR+EfHCttqtrKwMv5jGdkg5un10yZIlvP/97y91GF3a5s2bqa+vp0ePHjz55JN89KMfZenSpXTr1m27227u9yOpOiIqmyuf5RnBKGB5RKxIg5gFHAfUFpT5HDA9Il4GaC0JmJntLDZu3MgRRxxBfX09EcGNN97YIUmgPbJMBP2BZwqmVwEHNynzPgBJC0m6jyZHxO+bNiTpLOAsgL333juTYM3MOlPv3r13mNfulvpi8S7AvsDhwATgR5L6Ni0UETMiojIiKsvLm333spmZtVOWiWA1MLBgekA6r9AqYG5E1EfEUyTXFPbNMCYzM2siy0SwCNhX0iBJ3YCTgLlNyvyG5GwASXuSdBWtyDAmMzNrIrNEEBGbgEnAXcAS4PaIWCxpiqRj02J3AWsl1QJ/AS6KiLVZxWRmZm+V6XMEETEPmNdk3mUFnwP4UvplZl1Ve26d3WZ7rd9Wu2rVKs477zxqa2vZvHkzRx99NN/97ne5++67+epXvwrA8uXL6d+/Pz179mTYsGH89Kc/fUs7FRUVVFVVseeee24dwuTJ9OrVi6985SvFhdzG8k3deuutVFVVccMNN7Sr/vYo9cViM7M2iwg+/elP88lPfpJly5axbNkyXnvtNS6++GLGjh1LTU0NNTU1VFZWMnPmTGpqappNApZwIjCzLufPf/4zPXr04PTTTweSMX++973v8dOf/rTFIZs///nPU1lZydChQ7n88su3Wvbtb3+b/fffn1GjRjUO7VDoySefZNy4cYwcOZIPfvCDPP74482uo7a2lsMPP5x99tmH66+/vnH+z3/+c0aNGsWIESM4++yz2bx5MwC33HIL73vf+xg1ahQLFy7can2jR49m//335xvf+Aa9evVqXPad73yncXjtpj9HezkRmFmXs3jxYkaOHLnVvN13352Kiopmd+QAV111FVVVVTzyyCPcc889PPLII43L+vTpw6OPPsqkSZO48MIL31L3rLPOYtq0aVRXV3PNNddw7rnnNruOxx9/nLvuuosHHniAK664gvr6epYsWcLs2bNZuHAhNTU1lJWVMXPmTNasWcPll1/OwoULWbBgAbW1/3rW9oILLuCCCy7g0UcfbRzpFJJRVZctW8YDDzxATU0N1dXV/PWvf23TtmuORx81s1y4/fbbmTFjBps2bWLNmjXU1tY2Dv08YcKExu9f/OIXt6q3YcMG7r33Xk444YTGeQ1jADV1zDHH0L17d7p3706/fv14/vnn+dOf/kR1dTUHHXQQkIyR1K9fP+6//34OP/xwGp6NGj9+fOPgcvfdd1/j2Egnn3xy43WH+fPnM3/+fA444IDG2JYtW9Y4zHV7ORGYWZczZMiQxncNNHjllVd47rnn2G+//d5S/qmnnuKaa65h0aJFvP3tb+e0007j9ddfb1wuqdnPAFu2bKFv376Nr8XclpaGqJ44cSLf+ta3tirbdBC8YkQEl156KWeffXab626Lu4bMrMv5yEc+wsaNGxsvAG/evJkvf/nLTJo0iZ49e76l/CuvvMJuu+1Gnz59eP755xtfDtNg9uzZjd+bvgRm9913Z9CgQfzyl78Ekp1x07eNtRbrnDlzeOGFZCi1l156iaeffpqDDz6Ye+65h7Vr11JfX9/YPiRDUf/qV78CYNasWY3zx44dy80339x4HWT16tWN7W4PnxGY2fbr5FFUJXHHHXdw3nnnceWVV1JXV8f48eMbXw/Z1PDhwznggAMYPHgwAwcObHz7V4OXX36ZYcOG0b1792ZfCDNz5kw+//nPM3XqVOrr6znppJMYPnx4UbEOGTKEqVOnctRRR7FlyxZ23XVXpk+fzujRo5k8eTJjxoyhb9++jBgxorHOddddxymnnMJVV13FuHHj6NMnuT33qKOOYsmSJY3JqlevXvz85z+nX79+RcXSksyGoc6Kh6G2HZaHoS6Ze++9lwkTJnDHHXdw4IEHljqc7bZx40Z69uyJJGbNmsVtt93G//7v/xZdf0cahtrMrFMccsghPP3006UOo8NUV1czadIkIoK+ffty8803Z7o+JwIzsx3MBz/4wTZdh9hevlhsZu3S1bqV86I9vxcnAjNrsx49erB27Vongx1MRLB27Vp69OjRpnruGjKzNhswYACrVq2irq6u1KFYEz169NjqaeRiOBGYWZvtuuuuDBo0qNRhWAdx15CZWc45EZiZ5ZwTgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5ZwTgZlZzjkRmJnlXKaJQNI4SUslLZd0STPLT5NUJ6km/Tozy3jMzOytMhtrSFIZMB04ElgFLJI0NyJqmxSdHRGTsorDzMy2LcszglHA8ohYERFvArOA4zJcn5mZtUOWiaA/8EzB9Kp0XlP/IekRSXMkDWyuIUlnSaqSVOVhb83MOlapLxb/H1AREcOAPwA/aa5QRMyIiMqIqCwvL+/UAM3MdnZZJoLVQOER/oB0XqOIWBsRb6STPwZGZhiPmZk1I8tEsAjYV9IgSd2Ak4C5hQUk7VUweSywJMN4zMysGZndNRQRmyRNAu4CyoCbI2KxpClAVUTMBb4g6VhgE/AScFpW8ZiZWfMyfVVlRMwD5jWZd1nB50uBS7OMwczMtq3UF4vNzKzEnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyLtNEIGmcpKWSlku6ZBvl/kNSSKrMMh4zM3urohKBEqdIuiyd3lvSqFbqlAHTgY8BQ4AJkoY0U643cAFwf1uDNzOz7VfsGcGNwBhgQjr9T5Kd/LaMApZHxIqIeBOYBRzXTLkrgf8GXi8yFjMz60DFJoKDI+I80p11RLwMdGulTn/gmYLpVem8RpIOBAZGxG+31ZCksyRVSaqqq6srMmQzMytGsYmgPu3qCQBJ5cCW7VmxpLcB1wJfbq1sRMyIiMqIqCwvL9+e1ZqZWRPFJoLrgTuAfpKuAhYA/9VKndXAwILpAem8Br2BDwB3S1oJjAbm+oKxmVnn2qWYQhExU1I18BFAwCcjYkkr1RYB+0oaRJIATgJOLmhzPbBnw7Sku4GvRERVm34CMzPbLkUlAknvAF4AbiuYt2tE1LdUJyI2SZoE3AWUATdHxGJJU4CqiJi7faGbmVlHKCoRAA+SdPO8THJG0Bd4TtLzwOciorq5ShExD5jXZN5lLZQ9vMhYzMysAxV7jeAPwNERsWdE7EHybMCdwLkkt5aamVkXVWwiGB0RdzVMRMR8YExE/B3onklkZmbWKYrtGloj6askD4UBjAeeT28p3a7bSM3MrLSKPSM4meT2z9+kX3un88qAE7MJzczMOkOxt4++CJzfwuLlHReOmZl1tmJvHy0HLgaGAj0a5kfEhzOKy8zMOkmxXUMzgceBQcAVwEqSB8bMzKyLKzYR7BERNwH1EXFPRJwB+GzAzGwnUOxdQw1PEK+RdAzwLPCObEIyM7POVGwimCqpD8lIodOA3YELM4vKzMw6TbGJ4OV0kLj1wBEAkg7NLCozM+s0xV4jmFbkPDMz62K2eUYgaQxwCFAu6UsFi3YneZjMzMy6uNa6hroBvdJyvQvmvwIcn1VQZmbWebaZCCLiHuAeSbdGxNOdFJOZWVEqLtnm687fYuXVx2QUSddW7MXi7pJmABWFdfxksZlZ11dsIvgl8APgx8Dm7MIxsw4xuU876qzv+DisSyg2EWyKiO9nGomZmZVEsbeP/p+kcyXtJekdDV+ZRmZmZp2i2DOCien3iwrmBbBPx4ZjZmadrdj3EQzKOhAzMyuNorqGJP2bpG+kdw4haV9JH882NDMz6wzFXiO4BXiT5CljgNXA1EwiMjOzTlVsInhPRHybdDjqiNgIKLOozMys0xSbCN6U1JPkAjGS3gO80VolSeMkLZW0XNIlzSw/R9KjkmokLZA0pE3Rm5nZdis2EVwO/B4YKGkm8CeSdxi3SFIZMB34GDAEmNDMjv4XEbF/RIwAvg1c25bgzcxs+xV719AfJD0IjCbpErogIl5spdooYHlErACQNAs4DqgtaPeVgvK7kZ5xmJlZ5yn2rqFPkTxd/NuIuBPYJOmTrVTrDzxTML0qnde07fMkPUlyRvCFFtZ/lqQqSVV1dXXFhGxmZkUqumsofUMZABGxjqS7aLtFxPSIeA/wVeAbLZSZERGVEVFZXl7eEas1M7NUsYmguXKtdSutBgYWTA9I57VkFtDaWYaZmXWwYhNBlaRrJb0n/boWqG6lziJgX0mDJHUDTgLmFhaQtG/B5DHAsmIDNzOzjlFsIjif5IGy2SRH7q8D522rQkRsAiYBdwFLgNsjYrGkKZKOTYtNkrRYUg3wJf41ppGZmXWSVu8aSm8DvTMijmhr4xExD5jXZN5lBZ8vaGubZmbWsVo9I4iIzcAWSe1404WZme3oih2GegPwqKQ/AK82zIyIZm/3NDOzrqPYRPDr9MvMzHYyxT5Z/JN0rKG9I2JpxjGZmVknKvbJ4k8ANSTjDSFphKS5265lZmZdQbG3j04mGTtoHUBE1ODXVJqZ7RSKTQT1hUNMpLZ0dDBmZtb5ir1YvFjSyUBZ+jTwF4B7swvLzMw6S1ueLB5K8jKaXwDrgQuzCsrMzDrPNs8IJPUAzgHeCzwKjEmHjjAzs51Ea2cEPwEqSZLAx4BrMo/IzMw6VWvXCIZExP4Akm4CHsg+JDMz60ytnRHUN3xwl5CZ2c6ptTOC4ZIa3issoGc6LSAiYvdMozMzs8xtMxFERFlnBWJmZqVR7HMEZrlTcclv21R+ZY+MAjHLWLHPEZiZ2U7KicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznnAjMzHLOicDMLOcyTQSSxklaKmm5pEuaWf4lSbWSHpH0J0nvzjIeMzN7q8wSgaQyYDrJ8NVDgAmShjQp9hBQGRHDgDnAt7OKx8zMmpflGcEoYHlErIiIN4FZwHGFBSLiLxGxMZ38OzAgw3jMzKwZWSaC/sAzBdOr0nkt+U/gd80tkHSWpCpJVXV1dR0YopmZ7RAXiyWdQvImtO80tzwiZkREZURUlpeXd25wZmY7uSxHH10NDCyYHpDO24qkjwJfBw6LiDcyjMfMzJqR5RnBImBfSYMkdQNOAuYWFpB0APBD4NiIeCHDWMzMrAWZJYL01ZaTgLuAJcDtEbFY0hRJx6bFvgP0An4pqUbS3BaaMzOzjGT6YpqImAfMazLvsoLPH81y/WY7C78kx7K0Q1wsNjOz0nEiMDPLOScCM7OccyIwM8s5JwIzs5xzIjAzyzknAjOznHMiMDPLOScCM7OccyIwM8s5JwIzs5xzIjAzyzknAjOznMt09FGztmrzKJtXH5NRJGb54TMCM7OccyIwM8s5JwIzs5xzIjAzyzknAjOznHMiMDPLOScCM7OccyIwM8s5JwIzs5xzIjAzy7lME4GkcZKWSlou6ZJmln9I0oOSNkk6PstYzMyseZklAkllwHTgY8AQYIKkIU2K/QM4DfhFVnGYmdm2ZTno3ChgeUSsAJA0CzgOqG0oEBEr02VbMozDzMy2Icuuof7AMwXTq9J5bSbpLElVkqrq6uo6JDgzM0t0iYvFETEjIiojorK8vLzU4ZiZ7VSyTASrgYEF0wPSeWZmtgPJMhEsAvaVNEhSN+AkYG6G6zMzs3bILBFExCZgEnAXsAS4PSIWS5oi6VgASQdJWgWcAPxQ0uKs4jEzs+Zl+qrKiJgHzGsy77KCz4tIuozMzKxEusTFYjMzy44TgZlZzjkRmJnlnBOBmVnOORGYmeWcE4GZWc45EZiZ5VymzxGY2c6n4pLftqn8yquPySgS6yg+IzAzyzknAjOznHMiMDPLOScCM7OccyIwM8s5JwIzs5zz7aPW5tsBwbcEmu1MnAisa5vcpx111nd8HGZdmLuGzMxyzonAzCznnAjMzHLOicDMLOecCMzMcs6JwMws55wIzMxyzonAzCznMk0EksZJWippuaRLmlneXdLsdPn9kiqyjMfMzN4qsyeLJZUB04EjgVXAIklzI6K2oNh/Ai9HxHslnQT8NzA+q5h2JB7Wwcx2FFkOMTEKWB4RKwAkzQKOAwoTwXHA5PTzHOAGSYqIyDAuM+tMHgZkh6es9rmSjgfGRcSZ6fSpwMERMamgzGNpmVXp9JNpmRebtHUWcFY6uR+wNJOgm7cn8GKrpTqHY3mrHSUOcCwtcSzN6+xY3h0R5c0t6BKDzkXEDGBGKdYtqSoiKkux7qYcy44bBziWljiW5u1IsWR5sXg1MLBgekA6r9kyknYB+gBrM4zJzMyayDIRLAL2lTRIUjfgJGBukzJzgYnp5+OBP/v6gJlZ58qsaygiNkmaBNwFlAE3R8RiSVOAqoiYC9wE/EzScuAlkmSxoylJl1QLHMtb7ShxgGNpiWNp3g4TS2YXi83MrGvwk8VmZjnnRGBmlnO5SgSSvi5psaRHJNVIOriV8remz0Mg6W5JLd7qJel6SRtKGYukmyQ9nLY5R1KvEsYyMx1e5DFJN0vatYSxTEqHMQlJexYTR5P6G5pMnybphlbqTJb0lWbmV6TPz7Rl/ZvTbfGwpAclHdKW+ttot83bJcNY2vz3kmEsbf4/yiqWgvbbtH9pq9wkAkljgI8DB0bEMOCjwDMd1HYl8PYdIJYvRsTwtM1/AJNaq5BhLDOBwcD+QE/gzBLGsjBt6+kOaKsUXouIERExHLgU+FYHtdue7ZJVLG3+e8kwljb/H2UYS5v3L+2Rm0QA7AW8GBFvAETEixHxLICkkZLukVQt6S5JexXbqJIxlb4DXFzqWCLilbQNkfwzFXMnQFaxzIsU8ADJcySliuWhiFhZbPm2SI/w/5wePf5J0t7NlBmZHik+DJy3navcHXg5bfdwSXcWrOeG9Gzlw5J+UzD/SEl3NG2oA7ZLR8bSnr+XrGJpz/9RJrG0c//SZnlKBPOBgZKekHSjpMMA0lPQacDxETESuBm4qg3tTgLmRsSaHSAWJN0CPEdydDWtlLEUtHMq8PtSx7Ideqan/TWSaoApBcumAT9Jjx5nAtc3U/8W4Pz0aHF71v848GPgylbK/wUYLKlhOIHTSbZZR8g0ljb+vWQWSzv+j7KKpT37lzbLTSKIiA3ASJIxi+qA2ZJOIxm76JWDW/AAAAJlSURBVAPAH9J/8m9Q5NGIpHcBJ1DcH0qmsRS0fTrwLmAJRYzkmmUsqRuBv0bE33aAWNqr4bR/RESMAC4rWDYG+EX6+WfAvxdWlNQX6BsRfy0o0971DwbGAT9Nj1ablR5V/ww4JV3/GOB37VhvKWIp+u8ly1ja+n+URSzt3b+0R5cYa6ijRMRm4G7gbkmPkjzVXA0sjogx7WjyAOC9wPL0d/5vkpZHxHtLEMtWbSsZ7fVikqPRksQi6XKgHDi72DpZbpedQUTcp+TCbjmwia0P5noUfL4F+D/gdeCXEbFpR4+lPX8vWcWSttmm/6MMYmn3/qWtcnNGIGk/SfsWzBpBcpFsKVCu5EIlknaVNLSYNiPitxHxzoioiIgKYGMxv6QsYlHivQ2fgWOBx0sRS1r+TGAsMCEithRZJ5NYMnYv/3oi/jPAVkeyEbEOWCfp3wvKtJukwSRP6q8l2TZDlLzgqS/wkYL1Pgs8S3L2VPROrFSxtOfvJYtY2vt/lEUs7d2/tEeezgh6AdPSX8YmYDlwVkS8qeT2w+sl9SHZJtcBi7tYLAJ+Imn39PPDwOdLFAvAD0j+Ce5Lj2Z+HRFTtl0lm1gkfYHkqO6dwCOS5kU6PHoHOB+4RdJFJN1ZpzdT5nTgZklBch2krXqmXWKQ/G4npmdOz0i6HXgMeAp4qEm9mUB5RCxprtF2bpdMYqF9fy9ZxNLe/6Ostkun8BATZjspJc86PBQRNzkWx7LNOJwIzHY+kqqBV4EjG27HdSyOpcVYnAjMzPItNxeLzcyseU4EZmY550RgZpZzTgRmZjnnRGBmlnP/Hw2P60Xcq5kXAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hZtcMjsU2lwy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 265
        },
        "outputId": "7f149726-3a74-487c-febb-365ed1aacee1"
      },
      "source": [
        "qtable_pnl_list = qtable_bot._predictor.get_pred_pnls()\n",
        "delta_pnl_list = delta_bot._predictor.get_pred_pnls()\n",
        "plt.hist(qtable_pnl_list, bins=50, alpha=0.5, label='QTable')\n",
        "plt.hist(delta_pnl_list, bins=50, alpha=0.5, label='Delta')\n",
        "plt.legend(loc='upper right')\n",
        "plt.savefig(f'{model_path}dist.png')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yeQRAHfjs32a",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import shutil \n",
        "shutil.copyfile(pred_logger.file_path,f\"{model_path}qtable_log.csv\")\n",
        "shutil.copyfile(delta_pred_logger.file_path,f\"{model_path}delta_log.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsCKl7wY_mfN",
        "colab_type": "text"
      },
      "source": [
        "# Results\n",
        "\n",
        "In the following setups, P&L $\\kappa=0.08$.\n",
        "\n",
        "  1. Trading cost = 0; Call option holding = -10 shares; Initial stock holding = 5 shares\n",
        "  \n",
        "   <img src=\"https://drive.google.com/thumbnail?id=1-O_6O5j4joKMvdHaeWsDhMN7hDQ0oOil\" width=\"500\" height=\"200\">\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LeyKqWyPjUq3",
        "colab_type": "text"
      },
      "source": [
        "Step by Step Demo of QTable Bot Construction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GnkZHQdppAXZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Reset Environment\n",
        "environment.set_obs_attr(qtable_bot_env_attr)\n",
        "spec = specs.make_environment_spec(environment)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lY6_mv8AoxTz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Build Replay Buffer\n",
        "replay_table_name = 'replay_table'\n",
        "replay_table = reverb.Table(\n",
        "      name=replay_table_name,\n",
        "      sampler=reverb.selectors.Uniform(),\n",
        "      remover=reverb.selectors.Fifo(),\n",
        "      max_size=1,\n",
        "      rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "      signature=adders.NStepTransitionAdder.signature(spec))\n",
        "server = reverb.Server([replay_table], port=None)\n",
        "address = f'localhost:{server.port}'\n",
        "adder = adders.NStepTransitionAdder(\n",
        "        priority_fns={replay_table_name: lambda x: 1.},\n",
        "        client=reverb.Client(address),\n",
        "        n_step=1,\n",
        "        discount=1.0)\n",
        "dataset = datasets.make_reverb_dataset(\n",
        "            table=replay_table_name,\n",
        "            client=reverb.TFClient(address),\n",
        "            batch_size=1,\n",
        "            prefetch_size=None,\n",
        "            environment_spec=spec,\n",
        "            transition_adder=True)\n",
        "\n",
        "dat_iterator = iter(dataset) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ8wx7e7ocXD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Create Actor and Learner\n",
        "step_qtable = QTable(observation_spec=spec.observations, \n",
        "                action_spec=spec.actions)\n",
        "actor = QTableActor(qtable=step_qtable, \n",
        "                    epsilon=0.2, \n",
        "                    adder=adder)\n",
        "learner = QTableLearner(qtable=step_qtable, \n",
        "                        learning_rate=1e-3, \n",
        "                        target_update_period=1, \n",
        "                        dataset=dataset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cvHBwfKS3nNo",
        "colab_type": "code",
        "cellView": "both",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "34ac8d05-6957-4f86-d04d-a2e09f990fc5"
      },
      "source": [
        "#@title Training Loop\n",
        "num_episodes = 100 #@param {type:\"integer\"}\n",
        "\n",
        "actor_steps_taken = 0\n",
        "learner_steps_taken = 0\n",
        "\n",
        "for episode in range(num_episodes):\n",
        "  # actor._adder.reset()\n",
        "  timestep = environment.reset()\n",
        "  actor.observe_first(timestep)\n",
        "  episode_return = 0\n",
        "\n",
        "  while not timestep.last():\n",
        "    #learner Get an action from the agent and step in the environment.\n",
        "    action = actor.select_action(timestep.observation)\n",
        "    next_timestep = environment.step(action)\n",
        "\n",
        "    # Record the transition.\n",
        "    actor.observe(action=action, next_timestep=next_timestep)\n",
        "\n",
        "    # Book-keeping.\n",
        "    episode_return += next_timestep.reward\n",
        "    actor_steps_taken += 1\n",
        "    learner_steps_taken += 1\n",
        "    timestep = next_timestep\n",
        "    learner.step()\n",
        "\n",
        "  # Log quantities.\n",
        "  print('Episode: %d | Return: %f | Learner steps: %d | Actor steps: %d'%(\n",
        "      episode, episode_return, learner_steps_taken, actor_steps_taken))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Learner] Avg Td Error = -1.914 | Steps = 1 | Walltime = 0\n",
            "Episode: 0 | Return: -8.049007 | Learner steps: 3 | Actor steps: 3\n",
            "Episode: 1 | Return: -17.178998 | Learner steps: 6 | Actor steps: 6\n",
            "Episode: 2 | Return: -1.903435 | Learner steps: 9 | Actor steps: 9\n",
            "Episode: 3 | Return: -0.645727 | Learner steps: 12 | Actor steps: 12\n",
            "Episode: 4 | Return: -9.332153 | Learner steps: 15 | Actor steps: 15\n",
            "Episode: 5 | Return: 2.242455 | Learner steps: 18 | Actor steps: 18\n",
            "Episode: 6 | Return: -17.981263 | Learner steps: 21 | Actor steps: 21\n",
            "Episode: 7 | Return: -1.023621 | Learner steps: 24 | Actor steps: 24\n",
            "Episode: 8 | Return: -7.353714 | Learner steps: 27 | Actor steps: 27\n",
            "Episode: 9 | Return: -7.473374 | Learner steps: 30 | Actor steps: 30\n",
            "Episode: 10 | Return: -11.820989 | Learner steps: 33 | Actor steps: 33\n",
            "Episode: 11 | Return: -3.591912 | Learner steps: 36 | Actor steps: 36\n",
            "Episode: 12 | Return: -14.469015 | Learner steps: 39 | Actor steps: 39\n",
            "Episode: 13 | Return: 0.381621 | Learner steps: 42 | Actor steps: 42\n",
            "Episode: 14 | Return: -32.003255 | Learner steps: 45 | Actor steps: 45\n",
            "Episode: 15 | Return: -2.617242 | Learner steps: 48 | Actor steps: 48\n",
            "Episode: 16 | Return: -5.408591 | Learner steps: 51 | Actor steps: 51\n",
            "Episode: 17 | Return: -0.838900 | Learner steps: 54 | Actor steps: 54\n",
            "Episode: 18 | Return: -0.794998 | Learner steps: 57 | Actor steps: 57\n",
            "Episode: 19 | Return: 0.851309 | Learner steps: 60 | Actor steps: 60\n",
            "Episode: 20 | Return: -5.906478 | Learner steps: 63 | Actor steps: 63\n",
            "Episode: 21 | Return: -3.872830 | Learner steps: 66 | Actor steps: 66\n",
            "Episode: 22 | Return: -5.839450 | Learner steps: 69 | Actor steps: 69\n",
            "Episode: 23 | Return: -7.087576 | Learner steps: 72 | Actor steps: 72\n",
            "Episode: 24 | Return: -6.069112 | Learner steps: 75 | Actor steps: 75\n",
            "Episode: 25 | Return: -20.931096 | Learner steps: 78 | Actor steps: 78\n",
            "Episode: 26 | Return: -8.621684 | Learner steps: 81 | Actor steps: 81\n",
            "Episode: 27 | Return: -3.560893 | Learner steps: 84 | Actor steps: 84\n",
            "Episode: 28 | Return: 3.083473 | Learner steps: 87 | Actor steps: 87\n",
            "Episode: 29 | Return: -8.590517 | Learner steps: 90 | Actor steps: 90\n",
            "Episode: 30 | Return: 4.407833 | Learner steps: 93 | Actor steps: 93\n",
            "Episode: 31 | Return: -0.805000 | Learner steps: 96 | Actor steps: 96\n",
            "Episode: 32 | Return: 0.019073 | Learner steps: 99 | Actor steps: 99\n",
            "Episode: 33 | Return: -5.078882 | Learner steps: 102 | Actor steps: 102\n",
            "Episode: 34 | Return: -14.442573 | Learner steps: 105 | Actor steps: 105\n",
            "Episode: 35 | Return: -9.281501 | Learner steps: 108 | Actor steps: 108\n",
            "Episode: 36 | Return: -6.626857 | Learner steps: 111 | Actor steps: 111\n",
            "Episode: 37 | Return: -4.276580 | Learner steps: 114 | Actor steps: 114\n",
            "Episode: 38 | Return: -3.057274 | Learner steps: 117 | Actor steps: 117\n",
            "Episode: 39 | Return: -13.653126 | Learner steps: 120 | Actor steps: 120\n",
            "Episode: 40 | Return: 3.370064 | Learner steps: 123 | Actor steps: 123\n",
            "Episode: 41 | Return: -6.257976 | Learner steps: 126 | Actor steps: 126\n",
            "Episode: 42 | Return: -2.829173 | Learner steps: 129 | Actor steps: 129\n",
            "Episode: 43 | Return: 4.546721 | Learner steps: 132 | Actor steps: 132\n",
            "Episode: 44 | Return: -0.363611 | Learner steps: 135 | Actor steps: 135\n",
            "Episode: 45 | Return: -18.830103 | Learner steps: 138 | Actor steps: 138\n",
            "Episode: 46 | Return: -6.319180 | Learner steps: 141 | Actor steps: 141\n",
            "Episode: 47 | Return: -4.481494 | Learner steps: 144 | Actor steps: 144\n",
            "Episode: 48 | Return: 7.148218 | Learner steps: 147 | Actor steps: 147\n",
            "Episode: 49 | Return: -5.588199 | Learner steps: 150 | Actor steps: 150\n",
            "Episode: 50 | Return: 14.391012 | Learner steps: 153 | Actor steps: 153\n",
            "Episode: 51 | Return: -11.285892 | Learner steps: 156 | Actor steps: 156\n",
            "Episode: 52 | Return: -4.621809 | Learner steps: 159 | Actor steps: 159\n",
            "Episode: 53 | Return: -3.683786 | Learner steps: 162 | Actor steps: 162\n",
            "Episode: 54 | Return: 4.487125 | Learner steps: 165 | Actor steps: 165\n",
            "Episode: 55 | Return: -10.088874 | Learner steps: 168 | Actor steps: 168\n",
            "Episode: 56 | Return: -27.977769 | Learner steps: 171 | Actor steps: 171\n",
            "Episode: 57 | Return: 7.219448 | Learner steps: 174 | Actor steps: 174\n",
            "Episode: 58 | Return: -4.539998 | Learner steps: 177 | Actor steps: 177\n",
            "Episode: 59 | Return: -9.458129 | Learner steps: 180 | Actor steps: 180\n",
            "Episode: 60 | Return: -9.815633 | Learner steps: 183 | Actor steps: 183\n",
            "Episode: 61 | Return: -13.095625 | Learner steps: 186 | Actor steps: 186\n",
            "Episode: 62 | Return: -0.939360 | Learner steps: 189 | Actor steps: 189\n",
            "Episode: 63 | Return: -17.462302 | Learner steps: 192 | Actor steps: 192\n",
            "Episode: 64 | Return: -5.587926 | Learner steps: 195 | Actor steps: 195\n",
            "Episode: 65 | Return: -7.264623 | Learner steps: 198 | Actor steps: 198\n",
            "Episode: 66 | Return: -14.114714 | Learner steps: 201 | Actor steps: 201\n",
            "Episode: 67 | Return: -11.845071 | Learner steps: 204 | Actor steps: 204\n",
            "Episode: 68 | Return: -14.022978 | Learner steps: 207 | Actor steps: 207\n",
            "Episode: 69 | Return: -2.644817 | Learner steps: 210 | Actor steps: 210\n",
            "Episode: 70 | Return: 8.766174 | Learner steps: 213 | Actor steps: 213\n",
            "Episode: 71 | Return: 1.726030 | Learner steps: 216 | Actor steps: 216\n",
            "Episode: 72 | Return: 2.961384 | Learner steps: 219 | Actor steps: 219\n",
            "Episode: 73 | Return: -3.613680 | Learner steps: 222 | Actor steps: 222\n",
            "Episode: 74 | Return: -3.339035 | Learner steps: 225 | Actor steps: 225\n",
            "Episode: 75 | Return: 2.806264 | Learner steps: 228 | Actor steps: 228\n",
            "Episode: 76 | Return: -12.562798 | Learner steps: 231 | Actor steps: 231\n",
            "Episode: 77 | Return: -2.801106 | Learner steps: 234 | Actor steps: 234\n",
            "Episode: 78 | Return: -11.228400 | Learner steps: 237 | Actor steps: 237\n",
            "Episode: 79 | Return: -9.959998 | Learner steps: 240 | Actor steps: 240\n",
            "Episode: 80 | Return: -18.464033 | Learner steps: 243 | Actor steps: 243\n",
            "Episode: 81 | Return: 0.845047 | Learner steps: 246 | Actor steps: 246\n",
            "Episode: 82 | Return: -11.094952 | Learner steps: 249 | Actor steps: 249\n",
            "Episode: 83 | Return: -0.377830 | Learner steps: 252 | Actor steps: 252\n",
            "Episode: 84 | Return: -14.088854 | Learner steps: 255 | Actor steps: 255\n",
            "Episode: 85 | Return: -0.322449 | Learner steps: 258 | Actor steps: 258\n",
            "Episode: 86 | Return: -4.692281 | Learner steps: 261 | Actor steps: 261\n",
            "Episode: 87 | Return: 4.373949 | Learner steps: 264 | Actor steps: 264\n",
            "Episode: 88 | Return: -73.215005 | Learner steps: 267 | Actor steps: 267\n",
            "Episode: 89 | Return: 0.306711 | Learner steps: 270 | Actor steps: 270\n",
            "Episode: 90 | Return: -11.377400 | Learner steps: 273 | Actor steps: 273\n",
            "Episode: 91 | Return: -7.311435 | Learner steps: 276 | Actor steps: 276\n",
            "Episode: 92 | Return: -11.752345 | Learner steps: 279 | Actor steps: 279\n",
            "Episode: 93 | Return: -26.944623 | Learner steps: 282 | Actor steps: 282\n",
            "Episode: 94 | Return: -3.541158 | Learner steps: 285 | Actor steps: 285\n",
            "Episode: 95 | Return: 4.112332 | Learner steps: 288 | Actor steps: 288\n",
            "Episode: 96 | Return: -20.648980 | Learner steps: 291 | Actor steps: 291\n",
            "Episode: 97 | Return: -5.955341 | Learner steps: 294 | Actor steps: 294\n",
            "Episode: 98 | Return: -1.862824 | Learner steps: 297 | Actor steps: 297\n",
            "Episode: 99 | Return: -2.399039 | Learner steps: 300 | Actor steps: 300\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxJaphYrqFJY",
        "colab_type": "text"
      },
      "source": [
        "Dive into Learner step"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VZT_IOHjpV5m",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "160f3df9-01d4-44fa-aa45-e4f3560c4ff2"
      },
      "source": [
        "#@title First step\n",
        "actor._adder.reset()\n",
        "timestep = environment.reset()\n",
        "actor.observe_first(timestep)\n",
        "print(timestep.observation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[455.   0.  50.]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tLFK2QpGpZrv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "a3062ec5-efd9-49d5-bce5-7eebe399e0a4"
      },
      "source": [
        "#@title Mid Actor step\n",
        "action = actor.select_action(timestep.observation)\n",
        "next_timestep = environment.step(action)\n",
        "\n",
        "# Record the transition.\n",
        "actor.observe(action=action, next_timestep=next_timestep)\n",
        "print(action)\n",
        "print(next_timestep.observation)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[2.]\n",
            "[365.        9.       52.36527]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KkSg8zlCpdsk",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "outputId": "9cf9819e-13a1-4789-a65f-43f11fd486a3"
      },
      "source": [
        "#@title Learning Step\n",
        "learning_rate = 0.001 #@param {type:\"number\"}\n",
        "inputs = next(dat_iterator)\n",
        "o_tm1, a_tm1, r_t, d_t, o_t = inputs.data\n",
        "avg_td_error = 0.\n",
        "o_tm1_i = o_tm1[0]\n",
        "a_tm1_i = a_tm1[0]\n",
        "r_t_i = r_t[0]\n",
        "d_t_i = d_t[0]\n",
        "o_t_i = o_t[0]\n",
        "cur_q = step_qtable.getQ(o_tm1_i.numpy(), a_tm1_i.numpy())\n",
        "# bellman eq\n",
        "target_q = r_t_i + d_t_i * \\\n",
        "    step_qtable.select_maxQ(o_t_i.numpy())\n",
        "td_error = target_q - cur_q\n",
        "avg_td_error += td_error\n",
        "inc = learning_rate * td_error\n",
        "print(f\"action = {a_tm1_i.numpy()[0]}; \\nobservation = {o_tm1_i.numpy()}; \\ncur_Q = {cur_q}; \\ntarget_Q = {target_q}; \\ninc = {inc}\")\n",
        "# update qtable\n",
        "step_qtable.update(o_tm1_i.numpy(), a_tm1_i.numpy(), inc)\n",
        "avg_td_error = avg_td_error/o_tm1.shape[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "action = 3.0; \n",
            "observation = [455.   0.  50.]; \n",
            "cur_Q = 0.2827048897743225; \n",
            "target_Q = -1.0892881155014038; \n",
            "inc = -0.0013719931012019515\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}